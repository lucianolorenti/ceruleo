{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CeRULEo","text":""},{"location":"#welcome-tu-ceruleo","title":"Welcome tu CeRULEo","text":"<p>CeRULEo: Comprehensive utilitiEs for Remaining Useful Life Estimation methOds</p>"},{"location":"#predictive-maintenance","title":"Predictive Maintenance","text":"<p>Efficient management of maintenance in modern industrial environments is having a major impact on decreasing costs associated with defective products and equipment inactivity. Therefore, it is critical for companies to develop an efficient and well-implemented maintenance strategy to prevent unexpected outages, improve overall reliability, and reduce operating costs  <sup>1</sup>.</p> <p>The evolution of information systems has transformed traditional manufacturing into factories equipped with intelligent sensors that allow better knowledge about what happens during industrial processes.  All the information collected can be used to optimize decision-making processes. The use of this information in maintenance processes has caused a transition from Preventive Maintenance (PM) techniques to Predictive Maintenance (PdM) methods <sup>2</sup>. PM  is carried out regularly while the asset is still in a  working condition to prevent sudden breakdowns. In contrast, PdM can statistically assess the health status of a piece of equipment, allowing early detection of pending failures, and enabling timely pre-failure interventions, thanks to prediction models based on historical data. The data-driven methods use condition monitoring data acquired from sensors to provide effective solutions in these areas <sup>3</sup>. </p>"},{"location":"#remaining-useful-life-estimation","title":"Remaining useful life estimation","text":"<p>The remaining useful life (RUL) estimation has been considered as a central technology of PdM <sup>4</sup><sup>5</sup>. RUL estimation is a process that uses prediction methods to forecast the future performance of machinery and obtain the time left before machinery loses its operation ability.</p>"},{"location":"#bibliography","title":"Bibliography","text":"<ol> <li> <p>Lei Han, Yisheng Zou, Guofu Ding, Menghao Zhu, Lei Jiang, Shengfeng Qin, and Hongqin Liang. Development of an online tool condition monitoring system for nc machining based on spindle power signals. In 2018 24th International Conference on Automation and Computing (ICAC), volume, 1\u20136. 2018. doi:10.23919/IConAC.2018.8748978.\u00a0\u21a9</p> </li> <li> <p>Liam Damant, Amy Forsyth, Ramona Farcas, Melvin Voigtl\u00e4nder, Sumit Singh, Ip-Shing Fan, and Essam Shehab. Exploring the transition from preventive maintenance to predictive maintenance within erp systems by utilising digital twins. In Transdisciplinary Engineering for Resilience: Responding to System Disruptions, pages 171\u2013180. IOS Press, 2021.\u00a0\u21a9</p> </li> <li> <p>Gian Antonio Susto, Andrea Schirru, Simone Pampuri, Se\u00e1n McLoone, and Alessandro Beghi. Machine learning for predictive maintenance: a multiple classifier approach. IEEE Transactions on Industrial Informatics, 11(3):812\u2013820, 2014.\u00a0\u21a9</p> </li> <li> <p>Felix O Heimes. Recurrent neural networks for remaining useful life estimation. In 2008 international conference on prognostics and health management, 1\u20136. IEEE, 2008.\u00a0\u21a9</p> </li> <li> <p>Xiang Li, Qian Ding, and Jian-Qiao Sun. Remaining useful life estimation in prognostics using deep convolution neural networks. Reliability Engineering &amp; System Safety, 172:1\u201311, 2018.\u00a0\u21a9</p> </li> </ol>"},{"location":"dataset/Example/","title":"Notebook: Dataset creation","text":"<p>Each run-to-failure cycle it's represented as a data frame. Cycles belonging to the same dataset may not have the same number of features</p> <p>Let's start by creating a function for returning a random data frame. Each dataframe will have a random duration and a random number of features. The first feature will be categorical, the remanining ones numeric.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn\n\nseaborn.set_theme()\n\n\ndef random_cycle():\n    Nr = np.random.randint(50, 2000)\n    Nc = np.random.randint(5, 15)\n\n    df = pd.DataFrame(\n        np.random.randn(Nr, Nc), columns=[f\"Feature_{i+1}\" for i in range(Nc)]\n    )\n    df[f\"Feature_{0}\"] = np.random.choice(a=[\"Yes\", \"No\"], size=(Nr,))\n    df[\"RUL\"] = np.linspace(Nr, 0, Nr)\n    return df\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn  seaborn.set_theme()   def random_cycle():     Nr = np.random.randint(50, 2000)     Nc = np.random.randint(5, 15)      df = pd.DataFrame(         np.random.randn(Nr, Nc), columns=[f\"Feature_{i+1}\" for i in range(Nc)]     )     df[f\"Feature_{0}\"] = np.random.choice(a=[\"Yes\", \"No\"], size=(Nr,))     df[\"RUL\"] = np.linspace(Nr, 0, Nr)     return df In\u00a0[4]: Copied! <pre>from ceruleo.dataset.ts_dataset import AbstractPDMDataset\n\n\nclass CustomDataset(AbstractPDMDataset):\n    def __init__(self):\n        super().__init__()\n        self.lives = []\n        for i in range(50):\n            self.lives.append(random_cycle())\n\n    def get_time_series(self, i):\n        return self.lives[i]\n\n    @property\n    def rul_column(self) -&gt; str:\n        return \"RUL\"\n\n    @property\n    def n_time_series(self):\n        return len(self.lives)\n</pre> from ceruleo.dataset.ts_dataset import AbstractPDMDataset   class CustomDataset(AbstractPDMDataset):     def __init__(self):         super().__init__()         self.lives = []         for i in range(50):             self.lives.append(random_cycle())      def get_time_series(self, i):         return self.lives[i]      @property     def rul_column(self) -&gt; str:         return \"RUL\"      @property     def n_time_series(self):         return len(self.lives) In\u00a0[5]: Copied! <pre>dataset = CustomDataset()\n</pre> dataset = CustomDataset() <p>or directly you can instantiate <code>PDMInMemoryDataset</code> passing a list of dataframes with the run-to-failure cycle data</p> In\u00a0[8]: Copied! <pre>from ceruleo.dataset.ts_dataset import PDMInMemoryDataset\n\n\ndataset1 = PDMInMemoryDataset([random_cycle() for i in range(50)], rul_column=\"RUL\")\n</pre> from ceruleo.dataset.ts_dataset import PDMInMemoryDataset   dataset1 = PDMInMemoryDataset([random_cycle() for i in range(50)], rul_column=\"RUL\") <p>For bigget datasets the <code>PDMDataset</code> class can be used to load the run-to-failure cycles from disk</p> In\u00a0[6]: Copied! <pre>dataset.common_features()\n</pre> dataset.common_features() Out[6]: <pre>['Feature_0',\n 'Feature_1',\n 'Feature_2',\n 'Feature_3',\n 'Feature_4',\n 'Feature_5',\n 'RUL']</pre> In\u00a0[20]: Copied! <pre>dataset.numeric_features()\n</pre> dataset.numeric_features() Out[20]: <pre>['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5', 'RUL']</pre> In\u00a0[21]: Copied! <pre>dataset.categorical_features()\n</pre> dataset.categorical_features() Out[21]: <pre>['Feature_0']</pre> In\u00a0[22]: Copied! <pre>len(dataset)\n</pre> len(dataset) Out[22]: <pre>50</pre> In\u00a0[10]: Copied! <pre>fig, ax = plt.subplots(figsize=(17, 5))\nfeature = dataset.numeric_features()[0]\nfor life in dataset:\n    ax.plot(life[feature])\n</pre> fig, ax = plt.subplots(figsize=(17, 5)) feature = dataset.numeric_features()[0] for life in dataset:     ax.plot(life[feature]) In\u00a0[11]: Copied! <pre>fig, ax = plt.subplots()\nax.hist(dataset.durations())\n</pre> fig, ax = plt.subplots() ax.hist(dataset.durations()) Out[11]: <pre>(array([5., 0., 2., 7., 5., 6., 3., 7., 6., 9.]),\n array([  55. ,  249.3,  443.6,  637.9,  832.2, 1026.5, 1220.8, 1415.1,\n        1609.4, 1803.7, 1998. ]),\n &lt;BarContainer object of 10 artists&gt;)</pre> In\u00a0[24]: Copied! <pre>from sklearn.model_selection import train_test_split\n\ntrain_dataset, test_dataset = train_test_split(dataset, train_size=0.8)\n</pre> from sklearn.model_selection import train_test_split  train_dataset, test_dataset = train_test_split(dataset, train_size=0.8) In\u00a0[25]: Copied! <pre>pd.DataFrame(\n    {\n        \"Cycles in the whole dataset\": [len(dataset)],\n        \"Cycles in the training dataset\": [len(train_dataset)],\n        \"Cycles in the test dataset\": [len(test_dataset)],\n    }\n).T\n</pre> pd.DataFrame(     {         \"Cycles in the whole dataset\": [len(dataset)],         \"Cycles in the training dataset\": [len(train_dataset)],         \"Cycles in the test dataset\": [len(test_dataset)],     } ).T Out[25]: 0 Cycles in the whole dataset 50 Cycles in the training dataset 40 Cycles in the test dataset 10 In\u00a0[26]: Copied! <pre>fig, ax = plt.subplots()\nax.hist(train_dataset.durations())\nax.hist(test_dataset.durations())\n</pre> fig, ax = plt.subplots() ax.hist(train_dataset.durations()) ax.hist(test_dataset.durations()) Out[26]: <pre>(array([1., 3., 1., 1., 0., 1., 0., 1., 1., 1.]),\n array([ 274. ,  416.7,  559.4,  702.1,  844.8,  987.5, 1130.2, 1272.9,\n        1415.6, 1558.3, 1701. ]),\n &lt;BarContainer object of 10 artists&gt;)</pre>"},{"location":"dataset/Example/#notebook-dataset-creation","title":"Notebook: Dataset creation\u00b6","text":""},{"location":"dataset/Example/#mock-data","title":"Mock data\u00b6","text":""},{"location":"dataset/Example/#dataset-creation","title":"Dataset creation\u00b6","text":"<p>In order to define a dataset you should subclass <code>AbstractPDMDataset</code> and define three methods:</p> <ul> <li><code>__getitem__(self, i) -&gt; pd.DataFrame</code>: This method should return the i-th life</li> <li><code>n_time_series(self) -&gt; int</code>: The property return the total number of lives present in the dataset</li> <li><code>rul_column(self) -&gt; str</code>: The property should return the name of the RUL column</li> </ul>"},{"location":"dataset/Example/#dataset-instantiation","title":"Dataset instantiation\u00b6","text":""},{"location":"dataset/Example/#dataset-features","title":"Dataset features\u00b6","text":"<p>Since the run-to-failure cycles have differnet feature space. We can obtain the subset that is common to all of the cycles.</p>"},{"location":"dataset/Example/#number-of-run-to-cycle-failures","title":"Number of run-to-cycle failures\u00b6","text":""},{"location":"dataset/Example/#dataset-iteration","title":"Dataset iteration\u00b6","text":"<p>Datasets are iterable. Each element of the iteration is the pandas DataFrame that stores the run-to-failure cycle</p>"},{"location":"dataset/Example/#run-to-failure-cycles-duration","title":"Run-to-failure cycles duration\u00b6","text":""},{"location":"dataset/Example/#train-test-split-based-on-the-cycles","title":"Train-test split based on the cycles\u00b6","text":"<p>When training machine learning model, it's essential to have evaluation sets well defined. In the context of predictive maintenance, the split should be made at the level of cycles. The dataset instance is compatible with all the <code>sklearn.model_selection</code> functions for splitting data.</p>"},{"location":"dataset/builder/","title":"Builder","text":"<p>When building a PdM dataset you need the time series of the sensors of the machine and some indication of when the piece of equipment arrived to its end.</p> <p>For this reason the DatasetBuilder class helps on this. It allow you to specify how your dataset is strucctured and split each run-to-failure cycle for posterior analysis.</p>"},{"location":"dataset/builder/#failure-modes","title":"Failure modes","text":""},{"location":"dataset/builder/#increasing-feature","title":"Increasing feature","text":"<p>In scenarios where the dataset includes an increasing feature denoting the usage time of the item in question, it is possible to detect instances where a value at position 'i' is lower than the value at position 'i+1'. In such instances, we can establish that the item has been replaced. Consequently, we can determine the end of its lifespan as the last point within this increasing sequence.</p>"},{"location":"dataset/builder/#life-identifier-feature","title":"Life identifier feature","text":"<p>In scenarios where the dataset includes a feature denoting each cycle ID, it is possible to detect samples for which the ID remains the same.</p>"},{"location":"dataset/builder/#cycle-end-identifier","title":"Cycle end identifier","text":"<p>In situations where a dataset contains a feature that indicats the end of a cycle, it is possible to segment data points based on this feature. Similar to detecting changes in ascending sequences, this process identifies transitions in the 'life end indicator' feature.</p>"},{"location":"dataset/builder/#data-and-fault-modes","title":"Data and fault modes","text":"<p>In scenearios where the dataset is composed by two separates files: one with the sensor data and another with the fault data, it is possible to use the data and fault modes to split the run to failure cycles by combining both sources.  In that cases a datetime feature is required to align the data and fault modes.</p>"},{"location":"dataset/builder/#examples-of-usage","title":"Examples of usage","text":""},{"location":"dataset/builder/#increasing-feature_1","title":"Increasing feature","text":"<pre><code>df = pd.DataFrame(\n    {\n        \"Cycle\": list(range(0, 12, ))*2,\n        \"feature_a\": list(range(12))*2,\n        \"feature_b\": list(range(12, 24))*2,\n    }\n)\ndataset = (\n    DatasetBuilder()\n    .set_splitting_method(IncreasingFeatureCycleSplitter(\"Cycle\"))\n    .set_rul_column_method(CycleRULColumn(\"Cycle\"))\n    .set_output_mode(InMemoryOutputMode())\n    .build_from_df(df)\n)\n</code></pre>"},{"location":"dataset/builder/#increasing-with-datetime-based-rul-feature","title":"Increasing with datetime based RUL feature","text":"<pre><code>df = pd.DataFrame(\n    {\n        \"Cycle\": list(range(0, 12, ))*2,\n        \"datetime\": pd.date_range(\"2021-01-01\", periods=24, freq=\"min\").tolist(),\n        \"feature_a\": list(range(12))*2,\n        \"feature_b\": list(range(12, 24))*2,\n    }\n)\ndataset = (\n    DatasetBuilder()\n    .set_splitting_method(IncreasingFeatureCycleSplitter(\"Cycle\"))\n    .set_rul_column_method(DatetimeRULColumn(\"datetime\", \"s\"))\n    .set_output_mode(InMemoryOutputMode())\n    .build_from_df(df)\n)\n</code></pre>"},{"location":"dataset/builder/#life-identifier-feature_1","title":"Life identifier feature","text":"<pre><code>df = pd.DataFrame(\n    {\n        \"life_id\": [1]*12 + [2]*12,\n        \"datetime\": pd.date_range(\"2021-01-01\", periods=24, freq=\"min\").tolist(),\n        \"feature_a\": list(range(12))*2,\n        \"feature_b\": list(range(12, 24))*2,\n    }\n)\ndataset = (\n    DatasetBuilder()\n    .set_splitting_method(LifeIdCycleSplitter(\"life_id\"))\n    .set_rul_column_method(DatetimeRULColumn(\"datetime\", \"s\"))\n    .set_output_mode(InMemoryOutputMode())\n    .build_from_df(df)\n)\n</code></pre>"},{"location":"dataset/builder/#life-end-indicator-feature","title":"Life end indicator feature","text":"<pre><code>df = pd.DataFrame(\n    {\n        \"life_end\": [0]*11 + [1] + [0]*11 + [1],\n        \"datetime\": pd.date_range(\"2021-01-01\", periods=24, freq=\"min\").tolist(),\n        \"feature_a\": list(range(12))*2,\n        \"feature_b\": list(range(12, 24))*2,\n    }\n)\ndataset = (\n    DatasetBuilder()\n    .set_splitting_method(LifeEndIndicatorCycleSplitter(\"life_end\"))\n    .set_rul_column_method(DatetimeRULColumn(\"datetime\", \"s\"))\n    .set_output_mode(InMemoryOutputMode())\n    .build_from_df(df)\n)\n</code></pre>"},{"location":"dataset/builder/#data-and-fault-modes_1","title":"Data and fault modes","text":"<p><pre><code>df = pd.DataFrame(\n    {\n\n        \"datetime\": pd.date_range(\"2021-01-01\", periods=24, freq=\"min\").tolist(),\n        \"feature_a\": list(range(12))*2,\n        \"feature_b\": list(range(12, 24))*2,\n    }\n)\nfailures = pd.DataFrame({\n    \"datetime_failure\": [pd.Timestamp(\"2021-01-01 00:11:00\"), pd.Timestamp(\"2021-01-01 00:23:00\")],\n    \"failure_type\": [\"A\", \"B\"]\n})\ndataset = (\n    DatasetBuilder()\n    .set_splitting_method(FailureDataCycleSplitter(\"datetime\", \"datetime_failure\"))\n    .set_rul_column_method(DatetimeRULColumn(\"datetime\", \"s\"))\n    .set_output_mode(InMemoryOutputMode())\n    .build_from_df(df, failures)\n)\n</code></pre> \"\"\"</p>"},{"location":"dataset/builder/#reference","title":"Reference","text":""},{"location":"dataset/builder/#dataset-builder","title":"Dataset Builder","text":""},{"location":"dataset/builder/#cycles-splitter","title":"Cycles Splitter","text":""},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.FailureDataCycleSplitter","title":"<code>FailureDataCycleSplitter</code>","text":"<p>               Bases: <code>CyclesSplitter</code></p> <p>A splitter that divides a DataFrame into cycles based on a separate DataFrame containing failure data.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>class FailureDataCycleSplitter(CyclesSplitter):\n    \"\"\"A splitter that divides a DataFrame into cycles based on a separate DataFrame containing failure data.\"\"\"\n\n    data_time_column: str\n    fault_time_column: str\n\n    def __init__(self, data_time_column: str, fault_time_column: str):\n        self.data_time_column = data_time_column\n        self.fault_time_column = fault_time_column\n\n    def split(self, data: pd.DataFrame, fault: pd.DataFrame):\n        data = self.merge_data_with_faults(data, fault)\n        for life_index, life_data in data.groupby(\"fault_number\"):\n            if life_data.shape[0] == 0:\n                continue\n            yield life_data.copy()\n\n    def merge_data_with_faults(self, data: pd.DataFrame, fault: pd.DataFrame):\n        \"\"\"Merge the raw sensor data with the fault information\n\n        Parameters:\n\n            data_file: Path where the raw sensor data is located\n            fault_data_file: Path where the fault information is located\n\n        Returns:\n\n            df: Dataframe indexed by time with the raw sensors and faults\n                The dataframe contains also a fault_number column\n        \"\"\"\n\n        fault = fault.drop_duplicates(subset=[self.fault_time_column]).copy()\n        fault[\"fault_number\"] = range(fault.shape[0])\n        return pd.merge_asof(\n            data,\n            fault,\n            left_on=self.data_time_column,\n            right_on=self.fault_time_column,\n\n            suffixes=[\"_data\", \"_fault\"],\n            direction=\"forward\",\n        )\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.FailureDataCycleSplitter.merge_data_with_faults","title":"<code>merge_data_with_faults(data, fault)</code>","text":"<p>Merge the raw sensor data with the fault information</p> <p>Parameters:</p> <pre><code>data_file: Path where the raw sensor data is located\nfault_data_file: Path where the fault information is located\n</code></pre> <p>Returns:</p> <pre><code>df: Dataframe indexed by time with the raw sensors and faults\n    The dataframe contains also a fault_number column\n</code></pre> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>def merge_data_with_faults(self, data: pd.DataFrame, fault: pd.DataFrame):\n    \"\"\"Merge the raw sensor data with the fault information\n\n    Parameters:\n\n        data_file: Path where the raw sensor data is located\n        fault_data_file: Path where the fault information is located\n\n    Returns:\n\n        df: Dataframe indexed by time with the raw sensors and faults\n            The dataframe contains also a fault_number column\n    \"\"\"\n\n    fault = fault.drop_duplicates(subset=[self.fault_time_column]).copy()\n    fault[\"fault_number\"] = range(fault.shape[0])\n    return pd.merge_asof(\n        data,\n        fault,\n        left_on=self.data_time_column,\n        right_on=self.fault_time_column,\n\n        suffixes=[\"_data\", \"_fault\"],\n        direction=\"forward\",\n    )\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.IncreasingFeatureCycleSplitter","title":"<code>IncreasingFeatureCycleSplitter</code>","text":"<p>               Bases: <code>CyclesSplitter</code></p> <p>A splitter that divides a DataFrame into cycles based on changes in the value of an increasing feature.</p> <p>When the value of the increasing feature decreases, a new cycle is considered to start.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>class IncreasingFeatureCycleSplitter(CyclesSplitter):\n    \"\"\"\n    A splitter that divides a DataFrame into cycles based on changes in the value of an increasing feature.\n\n    When the value of the increasing feature decreases, a new cycle is considered to start.\n\n    \"\"\"\n\n    def __init__(self, increasing_feature: str):\n        \"\"\"Initializes the splitter with the name of the increasing feature.\n\n        Parameters\n        ----------\n        increasing_feature : str\n            The name of the increasing feature used for splitting.\n        \"\"\"\n        self.increasing_feature = increasing_feature\n\n    def split(self, data: pd.DataFrame) -&gt; Iterator[pd.DataFrame]:\n        \"\"\"Splits the input DataFrame into cycles based on changes in the increasing feature.\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The input DataFrame to be split.\n\n        Yields\n        ------\n        Iterator[pd.DataFrame]\n            An iterator of DataFrames, each containing a cycle of the input data.\n        \"\"\"\n        restart_points = data[data[self.increasing_feature].diff() &lt; 0].index.tolist()\n        start_idx = 0\n        i = 1\n        for restart_idx in restart_points:\n            subset = data.iloc[start_idx:restart_idx]\n            yield subset.copy()\n            start_idx = restart_idx\n            i += 1\n        yield subset.copy()\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.IncreasingFeatureCycleSplitter.__init__","title":"<code>__init__(increasing_feature)</code>","text":"<p>Initializes the splitter with the name of the increasing feature.</p>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.IncreasingFeatureCycleSplitter.__init__--parameters","title":"Parameters","text":"<p>increasing_feature : str     The name of the increasing feature used for splitting.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>def __init__(self, increasing_feature: str):\n    \"\"\"Initializes the splitter with the name of the increasing feature.\n\n    Parameters\n    ----------\n    increasing_feature : str\n        The name of the increasing feature used for splitting.\n    \"\"\"\n    self.increasing_feature = increasing_feature\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.IncreasingFeatureCycleSplitter.split","title":"<code>split(data)</code>","text":"<p>Splits the input DataFrame into cycles based on changes in the increasing feature.</p>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.IncreasingFeatureCycleSplitter.split--parameters","title":"Parameters","text":"<p>data : pd.DataFrame     The input DataFrame to be split.</p>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.IncreasingFeatureCycleSplitter.split--yields","title":"Yields","text":"<p>Iterator[pd.DataFrame]     An iterator of DataFrames, each containing a cycle of the input data.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>def split(self, data: pd.DataFrame) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Splits the input DataFrame into cycles based on changes in the increasing feature.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The input DataFrame to be split.\n\n    Yields\n    ------\n    Iterator[pd.DataFrame]\n        An iterator of DataFrames, each containing a cycle of the input data.\n    \"\"\"\n    restart_points = data[data[self.increasing_feature].diff() &lt; 0].index.tolist()\n    start_idx = 0\n    i = 1\n    for restart_idx in restart_points:\n        subset = data.iloc[start_idx:restart_idx]\n        yield subset.copy()\n        start_idx = restart_idx\n        i += 1\n    yield subset.copy()\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeEndIndicatorCycleSplitter","title":"<code>LifeEndIndicatorCycleSplitter</code>","text":"<p>               Bases: <code>CyclesSplitter</code></p> <p>A splitter that divides a DataFrame into cycles based on a life end indicator feature.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>class LifeEndIndicatorCycleSplitter(CyclesSplitter):\n    \"\"\"A splitter that divides a DataFrame into cycles based on a life end indicator feature.\"\"\"\n\n    def __init__(self, life_end_indicator_feature: str, end_value=1):\n        \"\"\"\n\n        Parameters\n        ----------\n        life_end_indicator_feature : str\n            The name of the column representing the life end indicator.\n        end_value : int, optional\n            The value indicating the end of a life cycle. by default 1\n\n        \"\"\"\n        self.life_end_indicator_feature = life_end_indicator_feature\n        self.end_value = end_value\n\n    def split(self, data: pd.DataFrame) -&gt; Iterator[pd.DataFrame]:\n        \"\"\"Splits the input DataFrame into cycles based on a life end indicator feature.\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The input DataFrame to be split.\n\n        Yields\n        ------\n        Iterator[pd.DataFrame]\n            An iterator of DataFrames, each containing a cycle of the input data.\n        \"\"\"\n        start_idx = 0\n        for idx in data[\n            data[self.life_end_indicator_feature] == self.end_value\n        ].index.tolist():\n            subset = data.iloc[start_idx : idx + 1]\n            yield subset.copy()\n            start_idx = idx + 1\n        if start_idx &lt; data.shape[0]:\n            yield data.iloc[start_idx:].copy()\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeEndIndicatorCycleSplitter.__init__","title":"<code>__init__(life_end_indicator_feature, end_value=1)</code>","text":""},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeEndIndicatorCycleSplitter.__init__--parameters","title":"Parameters","text":"<p>life_end_indicator_feature : str     The name of the column representing the life end indicator. end_value : int, optional     The value indicating the end of a life cycle. by default 1</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>def __init__(self, life_end_indicator_feature: str, end_value=1):\n    \"\"\"\n\n    Parameters\n    ----------\n    life_end_indicator_feature : str\n        The name of the column representing the life end indicator.\n    end_value : int, optional\n        The value indicating the end of a life cycle. by default 1\n\n    \"\"\"\n    self.life_end_indicator_feature = life_end_indicator_feature\n    self.end_value = end_value\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeEndIndicatorCycleSplitter.split","title":"<code>split(data)</code>","text":"<p>Splits the input DataFrame into cycles based on a life end indicator feature.</p>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeEndIndicatorCycleSplitter.split--parameters","title":"Parameters","text":"<p>data : pd.DataFrame     The input DataFrame to be split.</p>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeEndIndicatorCycleSplitter.split--yields","title":"Yields","text":"<p>Iterator[pd.DataFrame]     An iterator of DataFrames, each containing a cycle of the input data.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>def split(self, data: pd.DataFrame) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Splits the input DataFrame into cycles based on a life end indicator feature.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The input DataFrame to be split.\n\n    Yields\n    ------\n    Iterator[pd.DataFrame]\n        An iterator of DataFrames, each containing a cycle of the input data.\n    \"\"\"\n    start_idx = 0\n    for idx in data[\n        data[self.life_end_indicator_feature] == self.end_value\n    ].index.tolist():\n        subset = data.iloc[start_idx : idx + 1]\n        yield subset.copy()\n        start_idx = idx + 1\n    if start_idx &lt; data.shape[0]:\n        yield data.iloc[start_idx:].copy()\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeIdCycleSplitter","title":"<code>LifeIdCycleSplitter</code>","text":"<p>               Bases: <code>CyclesSplitter</code></p> <p>A splitter that divides a DataFrame into cycles based on unique life identifiers.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>class LifeIdCycleSplitter(CyclesSplitter):\n    \"\"\"A splitter that divides a DataFrame into cycles based on unique life identifiers.\"\"\"\n\n    def __init__(self, life_id_feature: str):\n        \"\"\"Initializes the splitter with the name of the life id feature.\n\n        Parameters\n        ----------\n        life_id_feature : str\n            The name of the column representing the life identifier.\n        \"\"\"\n        self.life_id_feature = life_id_feature\n\n    def split(self, data: pd.DataFrame) -&gt; Iterator[pd.DataFrame]:\n        \"\"\"Splits the input DataFrame into cycles based on unique life identifiers.\n\n        Parameters\n        ----------\n        data : pd.DataFrame\n            The input DataFrame to be split.\n\n        Yields\n        ------\n        Iterator[pd.DataFrame]\n            An iterator of DataFrames, each containing a cycle of the input data.\n        \"\"\"\n        for life_id in data[self.life_id_feature].unique():\n            subset = data[data[self.life_id_feature] == life_id]\n            yield subset.copy()\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeIdCycleSplitter.__init__","title":"<code>__init__(life_id_feature)</code>","text":"<p>Initializes the splitter with the name of the life id feature.</p>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeIdCycleSplitter.__init__--parameters","title":"Parameters","text":"<p>life_id_feature : str     The name of the column representing the life identifier.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>def __init__(self, life_id_feature: str):\n    \"\"\"Initializes the splitter with the name of the life id feature.\n\n    Parameters\n    ----------\n    life_id_feature : str\n        The name of the column representing the life identifier.\n    \"\"\"\n    self.life_id_feature = life_id_feature\n</code></pre>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeIdCycleSplitter.split","title":"<code>split(data)</code>","text":"<p>Splits the input DataFrame into cycles based on unique life identifiers.</p>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeIdCycleSplitter.split--parameters","title":"Parameters","text":"<p>data : pd.DataFrame     The input DataFrame to be split.</p>"},{"location":"dataset/builder/#ceruleo.dataset.builder.cycles_splitter.LifeIdCycleSplitter.split--yields","title":"Yields","text":"<p>Iterator[pd.DataFrame]     An iterator of DataFrames, each containing a cycle of the input data.</p> Source code in <code>ceruleo/dataset/builder/cycles_splitter.py</code> <pre><code>def split(self, data: pd.DataFrame) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"Splits the input DataFrame into cycles based on unique life identifiers.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The input DataFrame to be split.\n\n    Yields\n    ------\n    Iterator[pd.DataFrame]\n        An iterator of DataFrames, each containing a cycle of the input data.\n    \"\"\"\n    for life_id in data[self.life_id_feature].unique():\n        subset = data[data[self.life_id_feature] == life_id]\n        yield subset.copy()\n</code></pre>"},{"location":"dataset/catalog/","title":"Catalog","text":"<p>CERULEo dataset catalog is a collection of RUL estimation datasets ready to use. All datasets are exposed as <code>AbstractTimeSeriesDataset</code>, enabling easy-to-use and transformation input pipelines. To get started see the guide and our list of datasets.</p> <ul> <li><code>CMAPSS</code></li> <li><code>CMPSS-2</code></li> <li><code>PHM Dataset 2018</code></li> </ul>"},{"location":"dataset/catalog/#cmapss","title":"CMAPSS","text":""},{"location":"dataset/catalog/#ceruleo.dataset.catalog.CMAPSS.CMAPSSDataset","title":"<code>CMAPSSDataset</code>","text":"<p>               Bases: <code>AbstractPDMDataset</code></p> <p>C-MAPSS Dataset</p> <p>C-MAPSS stands for 'Commercial Modular Aero-Propulsion System Simulation' and it is a tool for the simulation  of realistic large commercial turbofan engine data. Each flight is a combination of a  series of flight conditions with a reasonable linear transition period to allow the  engine to change from one flight condition to the next. The flight conditions are arranged to cover a typical ascent from sea level to 35K ft and descent back down to sea level. </p> <p>The fault was injected at a given time in one of the flights and persists throughout the  remaining flights, effectively increasing the age of the engine. The intent is to identify which  flight and when in the flight the fault occurred.</p> <p>Dataset reference</p> <p>Available models are:</p> <pre><code>- FD001\n- FD002\n- FD003\n- FD004\n</code></pre> Example <pre><code>train_dataset = CMAPSSDataset(train=True, models='FD001')\nvalidation_dataset = CMAPSSDataset(train=False, models='FD001')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>bool</code> <p>Weather to obtain the train data provided, by default True</p> <code>True</code> <code>models</code> <code>Optional[Union[str, List[str]]]</code> <p>Names of the models, by default None (all models)</p> <code>None</code> Source code in <code>ceruleo/dataset/catalog/CMAPSS.py</code> <pre><code>class CMAPSSDataset(AbstractPDMDataset):\n    \"\"\"C-MAPSS Dataset\n\n    C-MAPSS stands for 'Commercial Modular Aero-Propulsion System Simulation' and it is a tool for the simulation \n    of realistic large commercial turbofan engine data. Each flight is a combination of a \n    series of flight conditions with a reasonable linear transition period to allow the \n    engine to change from one flight condition to the next. The flight conditions are arranged\n    to cover a typical ascent from sea level to 35K ft and descent back down to sea level. \n\n    The fault was injected at a given time in one of the flights and persists throughout the \n    remaining flights, effectively increasing the age of the engine. The intent is to identify which \n    flight and when in the flight the fault occurred.\n\n    [Dataset reference](https://data.nasa.gov/dataset/C-MAPSS-Aircraft-Engine-Simulator-Data/xaut-bemq)\n\n    Available models are:\n\n        - FD001\n        - FD002\n        - FD003\n        - FD004\n\n\n    Example:\n        ```\n        train_dataset = CMAPSSDataset(train=True, models='FD001')\n        validation_dataset = CMAPSSDataset(train=False, models='FD001')\n        ```\n\n    Parameters:\n        train: Weather to obtain the train data provided, by default True\n        models: Names of the models, by default None (all models)\n    \"\"\"\n    def __init__(\n        self, train: bool = True, models: Optional[Union[str, List[str]]] = None\n    ):\n        super().__init__()\n        obtain_raw_files(DATASET_PATH)\n        if models is not None and isinstance(models, str):\n            models = [models]\n        self._validate_model_names(models)\n        if train:\n            processing_fun = process_file_train\n        else:\n            processing_fun = process_file_test\n        self.lives = []\n\n        for engine in engines:\n            if models is not None and engine not in models:\n                continue\n            for _, g in processing_fun(engine).groupby(\"UnitNumber\"):\n                g.drop(columns=[\"UnitNumber\"], inplace=True)\n                g[\"Engine\"] = engine\n                self.lives.append(g)\n\n    def _validate_model_names(self, models):\n        if models is not None:\n            for model in models:\n                if model not in operation_mode:\n                    raise ValueError(\n                        f\"Invalid model: valid model are {list(operation_mode.keys())}\"\n                    )\n\n    def get_time_series(self, i):\n        return self.lives[i]\n\n    @property\n    def n_time_series(self):\n        return len(self.lives)\n\n    @property\n    def rul_column(self) -&gt; str:\n        return \"RUL\"\n</code></pre>"},{"location":"dataset/catalog/#ceruleo.dataset.catalog.CMAPSS.obtain_raw_files","title":"<code>obtain_raw_files(raw_data_path=DATASET_PATH)</code>","text":"<p>Download and unzip the raw files</p> <p>Parameters:</p> Name Type Description Default <code>raw_data_path</code> <code>Path</code> <p>Path where to store the dataset</p> <code>DATASET_PATH</code> Source code in <code>ceruleo/dataset/catalog/CMAPSS.py</code> <pre><code>def obtain_raw_files(raw_data_path: Path = DATASET_PATH, ):\n    \"\"\"Download and unzip the raw files\n\n    Parameters:\n        raw_data_path: Path where to store the dataset\n    \"\"\"\n    raw_data_path = raw_data_path / \"files\"\n    logger.info(\"Dataset not processed.\")\n    if not raw_data_path.is_dir():\n        raw_data_path.mkdir(exist_ok=True, parents=True)\n        ZIP_FILE = raw_data_path / \"CMAPSSData.zip\"\n        if not ZIP_FILE.is_file():\n            logger.info('Downloading file')            \n            download(URL, ZIP_FILE)\n        logger.info(\"Unzipping\")\n        with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n            zip_ref.extractall(raw_data_path)\n        logger.info(\"Removing zip file\")\n        ZIP_FILE.unlink()\n</code></pre>"},{"location":"dataset/catalog/#cmapss-2","title":"CMAPSS-2","text":""},{"location":"dataset/catalog/#ceruleo.dataset.catalog.CMAPSS2.CMAPSS2Dataset","title":"<code>CMAPSS2Dataset</code>","text":"<p>               Bases: <code>AbstractPDMDataset</code></p> <p>C-MAPSS-2 Dataset</p> <p>The dataset provides a new realistic dataset of run-to-failure trajectories for a small fleet of aircraft engines under realistic flight conditions.</p> <p>The damage propagation modelling used for the generation of this synthetic dataset builds on the modeling strategy from previous work . The dataset was generated with the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model. The data set is been provided by the Prognostics CoE at NASA Ames in collaboration with ETH Zurich and PARC.</p> <p>Dataset reference</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>Optional[bool]</code> <p>Wether to obtain the train data provided</p> <code>None</code> Source code in <code>ceruleo/dataset/catalog/CMAPSS2.py</code> <pre><code>class CMAPSS2Dataset(AbstractPDMDataset):\n    \"\"\"C-MAPSS-2 Dataset\n\n    The dataset provides a new realistic dataset of run-to-failure trajectories for a small fleet of aircraft\n    engines under realistic flight conditions.\n\n    The damage propagation modelling used for the generation of this synthetic dataset builds on\n    the modeling strategy from previous work .\n    The dataset was generated with the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model.\n    The data set is been provided by the Prognostics CoE at NASA Ames in collaboration with ETH Zurich and PARC.\n\n    [Dataset reference](https://data.phmsociety.org/2021-phm-conference-data-challenge/)\n\n    Parameters:\n        train: Wether to obtain the train data provided\n    \"\"\"\n    def __init__(\n        self,\n        path: Path = DATASET_PATH,\n        train: Optional[bool] = None,\n    ):\n        super().__init__()\n        self.path = path\n        LIVES_TABLE_PATH = path / \"lives_data.pkl\"\n        if not (LIVES_TABLE_PATH).is_file():\n            pr = CMAPSS2PreProcessor()\n            pr.run()\n        with open(LIVES_TABLE_PATH, \"rb\") as file:\n            self.lives = pickle.load(file)\n\n        if train is not None:\n            self.lives = self.lives[self.lives[\"Train\"] == train]\n\n    def get_time_series(self, i):\n        df_path = self.lives.iloc[i][\"Output Dir\"]\n        df = pd.read_parquet(self.path / df_path)\n        return df\n\n    @property\n    def n_time_series(self):\n        return len(self.lives)\n\n    @property\n    def rul_column(self) -&gt; str:\n        return \"RUL\"\n</code></pre>"},{"location":"dataset/catalog/#phmdataset2018","title":"PHMDataset2018","text":""},{"location":"dataset/catalog/#ceruleo.dataset.catalog.PHMDataset2018.FailureType","title":"<code>FailureType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Failure types availables for the dataset.</p> <p>Possible values are: <pre><code>FailureType.FlowCoolPressureDroppedBelowLimit\nFailureType.FlowcoolPressureTooHighCheckFlowcoolPump\nFailureType.FlowcoolLeak\n</code></pre></p> Source code in <code>ceruleo/dataset/catalog/PHMDataset2018.py</code> <pre><code>class FailureType(Enum):\n    \"\"\"Failure types availables for the dataset.\n\n    Possible values are:\n    ```\n    FailureType.FlowCoolPressureDroppedBelowLimit\n    FailureType.FlowcoolPressureTooHighCheckFlowcoolPump\n    FailureType.FlowcoolLeak\n    ```\n    \"\"\"\n\n    FlowCoolPressureDroppedBelowLimit = \"FlowCool Pressure Dropped Below Limit\"\n    FlowcoolPressureTooHighCheckFlowcoolPump = (\n        'Flowcool Pressure Too High Check Flowcool Pump'\n    )\n    FlowcoolLeak = \"Flowcool leak\"\n    FlowcoolPressureTooHighCheckFlowcoolPumpNoWaferID = 'Flowcool Pressure Too High Check Flowcool Pump [NoWaferID]'\n\n\n    @staticmethod\n    def that_starth_with(s: str):\n        for f in FailureType:\n            if s.startswith(f.value):\n                return f\n        return None\n</code></pre>"},{"location":"dataset/catalog/#ceruleo.dataset.catalog.PHMDataset2018.PHMDataset2018","title":"<code>PHMDataset2018</code>","text":"<p>               Bases: <code>PDMDataset</code></p> <p>PHM 2018 Dataset</p> <p>The 2018 PHM dataset is a public dataset released by Seagate which contains the execution of 20 different ion milling machines. They distinguish three different failure causes and provide 22 features, including user-defined variables and sensors.</p> <p>Three faults are present in the dataset</p> <ul> <li>Fault mode 1 occurs when flow-cool pressure drops.</li> <li>Fault mode 2 occurs when flow-cool pressure becomes too high.</li> <li>Fault mode 3 represents flow-cool leakage.</li> </ul> <p>Dataset reference</p> <p>Example:</p> <pre><code>dataset = PHMDataset2018(\n    failure_types=FailureType.FlowCoolPressureDroppedBelowLimit,\n    tools=['01_M02']\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path where the dataset is located</p> <code>DATA_PATH</code> Source code in <code>ceruleo/dataset/catalog/PHMDataset2018.py</code> <pre><code>class PHMDataset2018(PDMDataset):\n    \"\"\"PHM 2018 Dataset\n\n    The 2018 PHM dataset is a public dataset released by Seagate which contains the execution of 20 different\n    ion milling machines. They distinguish three different failure causes and provide 22 features,\n    including user-defined variables and sensors.\n\n    Three faults are present in the dataset\n\n    * Fault mode 1 occurs when flow-cool pressure drops.\n    * Fault mode 2 occurs when flow-cool pressure becomes too high.\n    * Fault mode 3 represents flow-cool leakage.\n\n    [Dataset reference](https://phmsociety.org/conference/annual-conference-of-the-phm-society/annual-conference-of-the-prognostics-and-health-management-society-2018-b/phm-data-challenge-6/)\n\n    Example:\n\n    ```py\n    dataset = PHMDataset2018(\n        failure_types=FailureType.FlowCoolPressureDroppedBelowLimit,\n        tools=['01_M02']\n    )\n    ```\n\n\n\n    Parameters:\n        path: Path where the dataset is located\n    \"\"\"\n\n    failure_types: Optional[List[FailureType]]\n    tools: Optional[List[str]]\n\n    def __init__(\n        self,\n        path: Path = DATA_PATH,\n        url: str = URL,\n        failure_types: Optional[Union[FailureType, List[FailureType]]] = None,\n        tools: Optional[Union[str, List[str]]] = None,\n    ):\n        self.url = url\n        super().__init__(path / \"phm_data_challenge_2018\", \"RUL\")\n        self._prepare_dataset()\n        self.failure_types = failure_types\n        self.tools = tools\n\n        if self.failure_types is not None:\n            if not isinstance(self.failure_types, list):\n                self.failure_types = [failure_types]\n            self.cycles_metadata = self.cycles_metadata[\n                self.cycles_metadata[\"Fault name\"].isin(\n                    [f.value for f in self.failure_types]\n                )\n            ]\n\n\n        if self.tools is not None:\n            if not isinstance(self.tools, list):\n                self.tools = [tools]\n            self.cycles_metadata = self.cycles_metadata[\n                self.cycles_metadata[\"Tool\"].isin(self.tools)\n            ]\n\n    def _prepare_dataset(self):\n        if self.cycles_table_filename.is_file():\n            return\n        if not (self.dataset_path / \"raw\" / \"train\").is_dir():\n            self.prepare_raw_dataset()\n        files = list(Path(self.dataset_path / \"raw\" / \"train\").resolve().glob(\"*.csv\"))\n        faults_files = list(\n            Path(self.dataset_path / \"raw\" / \"train\" / \"train_faults\")\n            .resolve()\n            .glob(\"*.csv\")\n        )\n\n        def get_key_from_filename(filename: str) -&gt; str:\n            return \"_\".join(filename.split(\"_\")[0:2])\n\n        fault_files_map = {get_key_from_filename(f.name): f for f in faults_files}\n        data_fault_pairs = [\n            (file, fault_files_map[get_key_from_filename(file.name)]) for file in files\n        ]\n\n        (\n            DatasetBuilder()\n            .set_splitting_method(\n                FailureDataCycleSplitter(\n                    data_time_column=\"time\", fault_time_column=\"time\"\n                )\n            )\n            .set_rul_column_method(NumberOfRowsRULColumn())\n            .set_output_mode(\n                LocalStorageOutputMode(\n                    self.dataset_path, output_format=DatasetFormat.PARQUET\n                ).set_metadata_columns(\n                    {\"Tool\": \"Tool_data\", \"Fault name\": \"fault_name\"}\n                )\n            )\n            .set_index_column(\"time\")\n            .prepare_from_data_fault_pairs_files(\n                data_fault_pairs,\n            )\n        )\n\n    def prepare_raw_dataset(self):\n        \"\"\"Download and unzip the raw files\n\n        Args:\n            path (Path): Path where to store the raw dataset\n        \"\"\"\n\n        def track_progress(members):\n            for member in tqdm(members, total=70):\n                yield member\n\n        path = self.dataset_path / \"raw\"\n        path.mkdir(parents=True, exist_ok=True)\n        if not (path / OUTPUT).resolve().is_file():\n            download(self.url, path)\n        logger.info(\"Decompressing  dataset...\")\n        with tarfile.open(path / OUTPUT, \"r\") as tarball:\n\n            def is_within_directory(directory, target):\n                abs_directory = os.path.abspath(directory)\n                abs_target = os.path.abspath(target)\n                prefix = os.path.commonprefix([abs_directory, abs_target])\n                return prefix == abs_directory\n\n            def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n                for member in tar.getmembers():\n                    member_path = os.path.join(path, member.name)\n                    if not is_within_directory(path, member_path):\n                        raise Exception(\"Attempted Path Traversal in Tar File\")\n\n                tar.extractall(path, members, numeric_owner=numeric_owner)\n\n            safe_extract(tarball, path=path, members=track_progress(tarball))\n        shutil.move(\n            str(path / \"phm_data_challenge_2018\" / \"train\"), str(path / \"train\")\n        )\n        shutil.move(str(path / \"phm_data_challenge_2018\" / \"test\"), str(path / \"test\"))\n        shutil.rmtree(str(path / \"phm_data_challenge_2018\"))\n        (path / OUTPUT).unlink()\n</code></pre>"},{"location":"dataset/catalog/#ceruleo.dataset.catalog.PHMDataset2018.PHMDataset2018.prepare_raw_dataset","title":"<code>prepare_raw_dataset()</code>","text":"<p>Download and unzip the raw files</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path where to store the raw dataset</p> required Source code in <code>ceruleo/dataset/catalog/PHMDataset2018.py</code> <pre><code>def prepare_raw_dataset(self):\n    \"\"\"Download and unzip the raw files\n\n    Args:\n        path (Path): Path where to store the raw dataset\n    \"\"\"\n\n    def track_progress(members):\n        for member in tqdm(members, total=70):\n            yield member\n\n    path = self.dataset_path / \"raw\"\n    path.mkdir(parents=True, exist_ok=True)\n    if not (path / OUTPUT).resolve().is_file():\n        download(self.url, path)\n    logger.info(\"Decompressing  dataset...\")\n    with tarfile.open(path / OUTPUT, \"r\") as tarball:\n\n        def is_within_directory(directory, target):\n            abs_directory = os.path.abspath(directory)\n            abs_target = os.path.abspath(target)\n            prefix = os.path.commonprefix([abs_directory, abs_target])\n            return prefix == abs_directory\n\n        def safe_extract(tar, path=\".\", members=None, *, numeric_owner=False):\n            for member in tar.getmembers():\n                member_path = os.path.join(path, member.name)\n                if not is_within_directory(path, member_path):\n                    raise Exception(\"Attempted Path Traversal in Tar File\")\n\n            tar.extractall(path, members, numeric_owner=numeric_owner)\n\n        safe_extract(tarball, path=path, members=track_progress(tarball))\n    shutil.move(\n        str(path / \"phm_data_challenge_2018\" / \"train\"), str(path / \"train\")\n    )\n    shutil.move(str(path / \"phm_data_challenge_2018\" / \"test\"), str(path / \"test\"))\n    shutil.rmtree(str(path / \"phm_data_challenge_2018\"))\n    (path / OUTPUT).unlink()\n</code></pre>"},{"location":"dataset/dataset/","title":"Dataset","text":"<p>The dataset package provides an abstract class to handle dataset for predictive maintenance.  The constitutive unit of the dataset are run-to-failure cycles, stored as a DataFrame. The dataset should provides an access to each cycle separately.</p> <p>Models implemented in this library accepts as input method for fitting and predicting, an instance of a Dataset defined in this package.</p>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset","title":"<code>AbstractPDMDataset</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>class AbstractPDMDataset(ABC):\n    def __init__(self):\n        self._common_features = None\n        self._durations = None\n\n    def __iter__(self):\n        return DatasetIterator(self)\n\n    @abstractproperty\n    @property\n    def n_time_series(self) -&gt; int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_time_series(self, i: int) -&gt; pd.DataFrame:\n        \"\"\"\n\n        Returns:\n            df: DataFrame with the data of the life i\n        \"\"\"\n        raise NotImplementedError\n\n    def number_of_samples_of_time_series(self, i: int) -&gt; int:\n        return self[i].shape[0]\n\n    @abstractproperty\n    def rul_column(self) -&gt; str:\n        raise NotImplementedError\n\n    def duration(self, life: pd.DataFrame) -&gt; float:\n        \"\"\"Obtain the duration of the time-series\n\n        Parameters:\n            life: The input life\n\n        Returns:\n            Duration of the life\n        \"\"\"\n        return life[self.rul_column].max()\n\n    def number_of_samples(self) -&gt; List[int]:\n        return [\n            self.number_of_samples_of_time_series(i) for i in tqdm(range(len(self)))\n        ]\n\n\n\n    def durations(self, show_progress: bool = False) -&gt; List[float]:\n        \"\"\"\n        Obtain the length of each life\n\n        Return:\n            List of durations\n        \"\"\"\n        if self._durations is None:\n            if show_progress:\n                iterator = tqdm(self)\n            else:\n                iterator = self\n            self._durations = [self.duration(life) for life in iterator]\n            # [self.rul_column].iloc[0]\n        return self._durations\n\n    def __call__(self, i):\n        return self[i]\n\n    def get_features_of_life(self, i: int) -&gt; pd.DataFrame:\n        return self[i]\n\n\n    def __getitem__(\n        self, i: Union[int, Iterable]\n    ) -&gt; Union[pd.DataFrame, \"FoldedDataset\"]:\n        \"\"\"Obtain a time-series or an splice of the dataset using a FoldedDataset\n\n        Parameters:\n            i: If the parameter is an int it will return a pd.DataFrame with the i-th time-series.\n                If the parameter is a list of int it will return a FoldedDataset with the\n                time-series whose id are present in the list\n\n        Raises:\n            ValueError: When the list does not contain integer parameters\n\n        Returns:\n            The i-th time-series\n            An instance of class FoldedDataset containing the dataset with the lives specified by the list\n        \"\"\"\n        if isinstance(i, slice):\n            i = range(\n                0 if i.start is None else i.start,\n                len(self) if i.stop is None else i.stop,\n                1 if i.step is None else i.step,\n            )\n        if TENSORFLOW_ENABLED and isinstance(i, tf.Tensor):\n            return self.get_time_series(i.ref())\n\n\n\n        if isinstance(i, Iterable):\n            if not all(isinstance(item, (int, np.integer)) for item in i):\n                if len(i) == 2:       \n                    if not isinstance(i[1], EllipsisType):\n                        raise ValueError(\"Invalid iterable index passed\")\n                    i = i[0]        \n\n            return FoldedDataset(self, i)\n        else:\n            df = self.get_time_series(i)\n            return df\n\n    @property\n    def shape(self) -&gt; Tuple[int]:\n        return (self.n_time_series,)\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Compute the number of lifes in the dataset\n\n        Return:\n            Number of time-series in the dataset\n        \"\"\"\n        return self.n_time_series\n\n    def to_pandas(\n        self,\n        proportion_of_lives: float = 1.0,\n        subsample_proportion: float = 1.0,\n        show_progress: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Create a dataset with the time-series concatenated\n\n        Parameters:\n            proportion_of_lives: Proportion of lives to use, by default 1\n            subsample_proportion: Proportion of samples to use, by default 1\n            show_progress: Whether to show progress when concatenating the lives, by default False\n\n        Returns:\n            A DataFrame with all the lives concatenated\n        \"\"\"\n        if show_progress:\n            bar = tqdm\n        else:\n            bar = lambda x: x\n        df = []\n\n        features = list(\n            self._compute_common_features(\n                proportion_of_lives=proportion_of_lives, show_progress=show_progress\n            )\n        )\n\n        for i in bar(range(self.n_time_series)):\n            if proportion_of_lives &lt; 1.0 and np.random.rand() &gt; proportion_of_lives:\n                continue\n\n            current_life = self[i].loc[:, features]\n            if subsample_proportion &lt; 1.0:\n                indices = range(\n                    0,\n                    current_life.shape[0],\n                    int(current_life.shape[0] * subsample_proportion),\n                )\n                current_life = current_life.iloc[indices, :]\n            df.append(current_life)\n        return pd.concat(df)\n\n    def _compute_common_features(\n        self, proportion_of_lives: float = 1.0, show_progress: bool = False\n    ) -&gt; List[str]:\n        common_features = []\n        if show_progress:\n            bar = tqdm\n        else:\n            bar = lambda x: x\n        for i in bar(range(self.n_time_series)):\n            if proportion_of_lives &lt; 1.0 and np.random.rand() &gt; proportion_of_lives:\n                continue\n            life = self.get_features_of_life(i)\n            common_features.append(set(life.columns.values))\n        return sorted(list(common_features[0].intersection(*common_features)))\n\n    def common_features(\n        self, show_progress: bool = False, proportion_of_lives: float = 1.0\n    ) -&gt; List[str]:\n        \"\"\"\n        Compute the common features of the dataset among the different lives\n\n        Parameters:\n            proportion_of_lives: Proportion of lives to use, by default 1\n            show_progress: Whether to show progress when computing the common features, by default False\n\n        Returns:\n            A list with the common features\n        \"\"\"\n        if self._common_features is None:\n            self._common_features = self._compute_common_features(\n                proportion_of_lives, show_progress=show_progress\n            )\n        return self._common_features\n\n    def map(\n        self, transformer: \"TransformedDataset\", cache_size: int = None\n    ) -&gt; \"TransformedDataset\":\n        \"\"\"\n        Apply a transformation to the dataset\n\n        Parameters:\n            transformer: The transformation to apply\n            cache_size: The size of the cache to use, by default None\n\n        Returns:\n            The transformed dataset as an instance of class TransformedDataset\n        \"\"\"\n        from ceruleo.dataset.transformed import TransformedDataset\n\n        return TransformedDataset(self, transformer, cache_size=cache_size)\n\n    def numeric_features(self, show_progress: bool = False) -&gt; List[str]:\n        \"\"\"Obtain the list of the common numeric features in the dataset\n\n        Parameters:\n            show_progress: Whether to show progress when computing the common features, by default False\n\n        Returns:\n            List of columns containing the common numeric features\n        \"\"\"\n\n        features = self.common_features(show_progress=show_progress)\n        df = self.get_features_of_life(0)\n        return list(\n            df.loc[:, features]\n            .select_dtypes(include=[np.number], exclude=[\"datetime\", \"timedelta\"])\n            .columns.values\n        )\n\n    def categorical_features(self, show_progress: bool = False) -&gt; List[str]:\n        \"\"\"Obtain the list of the common categorical features in the dataset\n\n        Parameters:\n            show_progress: Whether to show progress when computing the common features\n\n        Returns:\n            List of columns containing the common numeric features\n        \"\"\"\n        features = self.common_features(show_progress=show_progress)\n        df = self.get_time_series(0)\n        return list(\n            df.loc[:, features]\n            .select_dtypes(exclude=[np.number, \"datetime\", \"timedelta\"])\n            .columns.values\n        )\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.__getitem__","title":"<code>__getitem__(i)</code>","text":"<p>Obtain a time-series or an splice of the dataset using a FoldedDataset</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>Union[int, Iterable]</code> <p>If the parameter is an int it will return a pd.DataFrame with the i-th time-series. If the parameter is a list of int it will return a FoldedDataset with the time-series whose id are present in the list</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>When the list does not contain integer parameters</p> <p>Returns:</p> Type Description <code>Union[DataFrame, FoldedDataset]</code> <p>The i-th time-series</p> <code>Union[DataFrame, FoldedDataset]</code> <p>An instance of class FoldedDataset containing the dataset with the lives specified by the list</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def __getitem__(\n    self, i: Union[int, Iterable]\n) -&gt; Union[pd.DataFrame, \"FoldedDataset\"]:\n    \"\"\"Obtain a time-series or an splice of the dataset using a FoldedDataset\n\n    Parameters:\n        i: If the parameter is an int it will return a pd.DataFrame with the i-th time-series.\n            If the parameter is a list of int it will return a FoldedDataset with the\n            time-series whose id are present in the list\n\n    Raises:\n        ValueError: When the list does not contain integer parameters\n\n    Returns:\n        The i-th time-series\n        An instance of class FoldedDataset containing the dataset with the lives specified by the list\n    \"\"\"\n    if isinstance(i, slice):\n        i = range(\n            0 if i.start is None else i.start,\n            len(self) if i.stop is None else i.stop,\n            1 if i.step is None else i.step,\n        )\n    if TENSORFLOW_ENABLED and isinstance(i, tf.Tensor):\n        return self.get_time_series(i.ref())\n\n\n\n    if isinstance(i, Iterable):\n        if not all(isinstance(item, (int, np.integer)) for item in i):\n            if len(i) == 2:       \n                if not isinstance(i[1], EllipsisType):\n                    raise ValueError(\"Invalid iterable index passed\")\n                i = i[0]        \n\n        return FoldedDataset(self, i)\n    else:\n        df = self.get_time_series(i)\n        return df\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.__len__","title":"<code>__len__()</code>","text":"<p>Compute the number of lifes in the dataset</p> Return <p>Number of time-series in the dataset</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Compute the number of lifes in the dataset\n\n    Return:\n        Number of time-series in the dataset\n    \"\"\"\n    return self.n_time_series\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.categorical_features","title":"<code>categorical_features(show_progress=False)</code>","text":"<p>Obtain the list of the common categorical features in the dataset</p> <p>Parameters:</p> Name Type Description Default <code>show_progress</code> <code>bool</code> <p>Whether to show progress when computing the common features</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of columns containing the common numeric features</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def categorical_features(self, show_progress: bool = False) -&gt; List[str]:\n    \"\"\"Obtain the list of the common categorical features in the dataset\n\n    Parameters:\n        show_progress: Whether to show progress when computing the common features\n\n    Returns:\n        List of columns containing the common numeric features\n    \"\"\"\n    features = self.common_features(show_progress=show_progress)\n    df = self.get_time_series(0)\n    return list(\n        df.loc[:, features]\n        .select_dtypes(exclude=[np.number, \"datetime\", \"timedelta\"])\n        .columns.values\n    )\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.common_features","title":"<code>common_features(show_progress=False, proportion_of_lives=1.0)</code>","text":"<p>Compute the common features of the dataset among the different lives</p> <p>Parameters:</p> Name Type Description Default <code>proportion_of_lives</code> <code>float</code> <p>Proportion of lives to use, by default 1</p> <code>1.0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress when computing the common features, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list with the common features</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def common_features(\n    self, show_progress: bool = False, proportion_of_lives: float = 1.0\n) -&gt; List[str]:\n    \"\"\"\n    Compute the common features of the dataset among the different lives\n\n    Parameters:\n        proportion_of_lives: Proportion of lives to use, by default 1\n        show_progress: Whether to show progress when computing the common features, by default False\n\n    Returns:\n        A list with the common features\n    \"\"\"\n    if self._common_features is None:\n        self._common_features = self._compute_common_features(\n            proportion_of_lives, show_progress=show_progress\n        )\n    return self._common_features\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.duration","title":"<code>duration(life)</code>","text":"<p>Obtain the duration of the time-series</p> <p>Parameters:</p> Name Type Description Default <code>life</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>float</code> <p>Duration of the life</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def duration(self, life: pd.DataFrame) -&gt; float:\n    \"\"\"Obtain the duration of the time-series\n\n    Parameters:\n        life: The input life\n\n    Returns:\n        Duration of the life\n    \"\"\"\n    return life[self.rul_column].max()\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.durations","title":"<code>durations(show_progress=False)</code>","text":"<p>Obtain the length of each life</p> Return <p>List of durations</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def durations(self, show_progress: bool = False) -&gt; List[float]:\n    \"\"\"\n    Obtain the length of each life\n\n    Return:\n        List of durations\n    \"\"\"\n    if self._durations is None:\n        if show_progress:\n            iterator = tqdm(self)\n        else:\n            iterator = self\n        self._durations = [self.duration(life) for life in iterator]\n        # [self.rul_column].iloc[0]\n    return self._durations\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.get_time_series","title":"<code>get_time_series(i)</code>  <code>abstractmethod</code>","text":"<p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>DataFrame with the data of the life i</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>@abstractmethod\ndef get_time_series(self, i: int) -&gt; pd.DataFrame:\n    \"\"\"\n\n    Returns:\n        df: DataFrame with the data of the life i\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.map","title":"<code>map(transformer, cache_size=None)</code>","text":"<p>Apply a transformation to the dataset</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>TransformedDataset</code> <p>The transformation to apply</p> required <code>cache_size</code> <code>int</code> <p>The size of the cache to use, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>TransformedDataset</code> <p>The transformed dataset as an instance of class TransformedDataset</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def map(\n    self, transformer: \"TransformedDataset\", cache_size: int = None\n) -&gt; \"TransformedDataset\":\n    \"\"\"\n    Apply a transformation to the dataset\n\n    Parameters:\n        transformer: The transformation to apply\n        cache_size: The size of the cache to use, by default None\n\n    Returns:\n        The transformed dataset as an instance of class TransformedDataset\n    \"\"\"\n    from ceruleo.dataset.transformed import TransformedDataset\n\n    return TransformedDataset(self, transformer, cache_size=cache_size)\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.numeric_features","title":"<code>numeric_features(show_progress=False)</code>","text":"<p>Obtain the list of the common numeric features in the dataset</p> <p>Parameters:</p> Name Type Description Default <code>show_progress</code> <code>bool</code> <p>Whether to show progress when computing the common features, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of columns containing the common numeric features</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def numeric_features(self, show_progress: bool = False) -&gt; List[str]:\n    \"\"\"Obtain the list of the common numeric features in the dataset\n\n    Parameters:\n        show_progress: Whether to show progress when computing the common features, by default False\n\n    Returns:\n        List of columns containing the common numeric features\n    \"\"\"\n\n    features = self.common_features(show_progress=show_progress)\n    df = self.get_features_of_life(0)\n    return list(\n        df.loc[:, features]\n        .select_dtypes(include=[np.number], exclude=[\"datetime\", \"timedelta\"])\n        .columns.values\n    )\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.AbstractPDMDataset.to_pandas","title":"<code>to_pandas(proportion_of_lives=1.0, subsample_proportion=1.0, show_progress=False)</code>","text":"<p>Create a dataset with the time-series concatenated</p> <p>Parameters:</p> Name Type Description Default <code>proportion_of_lives</code> <code>float</code> <p>Proportion of lives to use, by default 1</p> <code>1.0</code> <code>subsample_proportion</code> <code>float</code> <p>Proportion of samples to use, by default 1</p> <code>1.0</code> <code>show_progress</code> <code>bool</code> <p>Whether to show progress when concatenating the lives, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with all the lives concatenated</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def to_pandas(\n    self,\n    proportion_of_lives: float = 1.0,\n    subsample_proportion: float = 1.0,\n    show_progress: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a dataset with the time-series concatenated\n\n    Parameters:\n        proportion_of_lives: Proportion of lives to use, by default 1\n        subsample_proportion: Proportion of samples to use, by default 1\n        show_progress: Whether to show progress when concatenating the lives, by default False\n\n    Returns:\n        A DataFrame with all the lives concatenated\n    \"\"\"\n    if show_progress:\n        bar = tqdm\n    else:\n        bar = lambda x: x\n    df = []\n\n    features = list(\n        self._compute_common_features(\n            proportion_of_lives=proportion_of_lives, show_progress=show_progress\n        )\n    )\n\n    for i in bar(range(self.n_time_series)):\n        if proportion_of_lives &lt; 1.0 and np.random.rand() &gt; proportion_of_lives:\n            continue\n\n        current_life = self[i].loc[:, features]\n        if subsample_proportion &lt; 1.0:\n            indices = range(\n                0,\n                current_life.shape[0],\n                int(current_life.shape[0] * subsample_proportion),\n            )\n            current_life = current_life.iloc[indices, :]\n        df.append(current_life)\n    return pd.concat(df)\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.FoldedDataset","title":"<code>FoldedDataset</code>","text":"<p>               Bases: <code>AbstractPDMDataset</code></p> <p>Dataset containing a subset of the time-series. An instanc of this class can be obtained by slicing an AbstractTimeSeriesDataset with a list of indexes</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>class FoldedDataset(AbstractPDMDataset):\n    \"\"\"\n    Dataset containing a subset of the time-series. An instanc of this class can be obtained by slicing an AbstractTimeSeriesDataset with a list of indexes\n    \"\"\"\n\n    def __init__(self, dataset: AbstractPDMDataset, indices: list):\n        super().__init__()\n        self.dataset = dataset\n        self.indices = indices\n\n    def __getattribute__(self, __name: str) -&gt; Any:\n        try:\n            return super().__getattribute__(__name)\n        except Exception as e:\n            return self.dataset.__getattribute__(__name)\n\n    @property\n    def n_time_series(self) -&gt; int:\n        \"\"\"\n        Compute the number of lifes in the folded dataset\n\n        Return:\n            Number of time-series in the folded dataset\n\n        \"\"\"\n        return len(self.indices)\n\n    def get_time_series(self, i: int) -&gt; pd.DataFrame:\n        \"\"\"\n        Obtain the i-th time-series in the folded dataset\n\n        Parameters:\n            i: Index of the life\n\n        Returns:\n            The i-th time-series\n        \"\"\"\n        return self.dataset[self.indices[i]]\n\n    def _original_index(self, i: int) -&gt; int:\n        \"\"\"\n        Obtain the index of the i-th time-series in the original dataset\n\n        Parameters:\n            i: Index of the life\n\n        Returns:\n            The index of the i-th time-series in the original dataset\n        \"\"\"\n        if isinstance(self.dataset, FoldedDataset):\n            return self.dataset._original_index(self.indices[i])\n        else:\n            return self.indices[i]\n\n    def original_indices(self) -&gt; List[int]:\n        \"\"\"\n        Obtain the original indices for all the time-series in the FoldedDataset\n\n        Returns:\n            The original indices for all the time-series in the FoldedDataset\n        \"\"\"\n        return [self._original_index(i) for i in range(len(self.indices))]\n\n    def number_of_samples_of_time_series(self, i: int) -&gt; int:\n        \"\"\"\n        Compute the number of samples of the i-th time-series in the FoldedDataset\n\n        Parameters:\n            i: Index of the life\n\n        Returns:\n            Number of samples of the i-th time-series in the FoldedDataset\n        \"\"\"\n        return self[i][0].shape[0]\n\n    def __reduce_ex__(self, __protocol) -&gt; Union[str, Tuple[Any, ...]]:\n        return (self.__class__, (self.dataset, self.indices))\n\n    @property\n    def rul_column(self):\n        return self.dataset.rul_column\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.FoldedDataset.n_time_series","title":"<code>n_time_series: int</code>  <code>property</code>","text":"<p>Compute the number of lifes in the folded dataset</p> Return <p>Number of time-series in the folded dataset</p>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.FoldedDataset.get_time_series","title":"<code>get_time_series(i)</code>","text":"<p>Obtain the i-th time-series in the folded dataset</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The i-th time-series</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def get_time_series(self, i: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Obtain the i-th time-series in the folded dataset\n\n    Parameters:\n        i: Index of the life\n\n    Returns:\n        The i-th time-series\n    \"\"\"\n    return self.dataset[self.indices[i]]\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.FoldedDataset.number_of_samples_of_time_series","title":"<code>number_of_samples_of_time_series(i)</code>","text":"<p>Compute the number of samples of the i-th time-series in the FoldedDataset</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>Index of the life</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of samples of the i-th time-series in the FoldedDataset</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def number_of_samples_of_time_series(self, i: int) -&gt; int:\n    \"\"\"\n    Compute the number of samples of the i-th time-series in the FoldedDataset\n\n    Parameters:\n        i: Index of the life\n\n    Returns:\n        Number of samples of the i-th time-series in the FoldedDataset\n    \"\"\"\n    return self[i][0].shape[0]\n</code></pre>"},{"location":"dataset/dataset/#ceruleo.dataset.ts_dataset.FoldedDataset.original_indices","title":"<code>original_indices()</code>","text":"<p>Obtain the original indices for all the time-series in the FoldedDataset</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>The original indices for all the time-series in the FoldedDataset</p> Source code in <code>ceruleo/dataset/ts_dataset.py</code> <pre><code>def original_indices(self) -&gt; List[int]:\n    \"\"\"\n    Obtain the original indices for all the time-series in the FoldedDataset\n\n    Returns:\n        The original indices for all the time-series in the FoldedDataset\n    \"\"\"\n    return [self._original_index(i) for i in range(len(self.indices))]\n</code></pre>"},{"location":"dataset/visualization/","title":"Visualization","text":""},{"location":"dataset/visualization/#ceruleo.graphics.analysis.plot_correlation_analysis","title":"<code>plot_correlation_analysis(dataset, corr_threshold=0, features=None, ax=None, **kwargs)</code>","text":"<p>Plot the correlated features in a dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>AbstractPDMDataset</code> <p>The dataset</p> required <code>corr_threshold</code> <code>float</code> <p>Minimum threshold to consider that the correlation is high</p> <code>0</code> <code>features</code> <code>Optional[List[str]]</code> <p>List of features</p> <code>None</code> <code>ax</code> <code>Optional[Axes]</code> <p>The axis where to draw</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The plot axis</p> Source code in <code>ceruleo/graphics/analysis.py</code> <pre><code>def plot_correlation_analysis(\n    dataset: AbstractPDMDataset,\n    corr_threshold: float = 0,\n    features: Optional[List[str]] = None,\n    ax: Optional[matplotlib.axes.Axes] = None,\n    **kwargs,\n) -&gt; matplotlib.axes.Axes:\n    \"\"\"Plot the correlated features in a dataset\n\n    Parameters:\n        dataset: The dataset\n        corr_threshold: Minimum threshold to consider that the correlation is high\n        features: List of features\n        ax: The axis where to draw\n\n    Returns:\n        The plot axis\n    \"\"\"\n\n    if features is not None:\n        features = list(set(features) - set([\"relative_time\"]))\n\n    df = correlation_analysis(dataset, features=features).to_pandas()\n    df1 = df[(df.abs_mean_correlation &gt; corr_threshold)]\n\n    df1.reset_index(inplace=True)\n    df1.sort_values(by=\"mean_correlation\", ascending=True, inplace=True)\n    if ax is None:\n        fig, ax = plt.subplots(**kwargs)\n    labels = []\n    for i, (_, r) in enumerate(df1.iterrows()):\n        f1 = r[\"feature_1\"]\n        f2 = r[\"feature_2\"]\n        label = f\"{f1}\\n{f2}\"\n        ax.barh(\n            y=i,\n            width=r[\"mean_correlation\"],\n            label=label,\n            xerr=r[\"std_correlation\"],\n            color=\"#7878FF\",\n        )\n        labels.append(label)\n\n    ax.axvline(x=0.90, linestyle=\"--\")\n    ax.axvline(x=-0.90, linestyle=\"--\")\n\n    ax.set_yticks(list(range(len(labels))))\n    ax.set_yticklabels(labels)\n    xticks = ax.get_xticks()\n\n    ax.set_xticks([-1, -0.90, -0.5, 0, 0.5, 0.90, 1])\n    ax.set_xlabel(\"Correlation\")\n    return ax\n</code></pre>"},{"location":"dataset/visualization/#ceruleo.graphics.duration.durations_boxplot","title":"<code>durations_boxplot(datasets, xlabel, ylabel='Cycle Duration', ax=None, hlines=[], units='m', transform=lambda x: x, maxy=None, **kwargs)</code>","text":"<p>Generate boxplots of the lives duration</p> <p>Example:</p> <pre><code>ax = durations_boxplot(\n    [train_dataset, validation_dataset],\n    xlabel=['Train', 'Validation'],\n    ylabel='Unit Cycles',\n    figsize=(17, 5))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[AbstractPDMDataset, List[AbstractPDMDataset]]</code> <p>Dataset from which take the lives durations</p> required <code>xlabel</code> <code>Union[str, List[str]]</code> <p>Label of each dataset to use as label in the boxplot</p> required <code>ylabel</code> <code>str</code> <p>Label of the y axis</p> <code>'Cycle Duration'</code> <code>ax</code> <code>Optional[Axes]</code> <p>Axis where to draw the plot.If missing a new figure will be created</p> <code>None</code> <code>hlines</code> <code>List[Tuple[float, str]]</code> <p>Horizontal lines to add to the figure in the form [(y_coordinate, label)]</p> <code>[]</code> <code>units</code> <code>str</code> <p>Units of time of the lives. Useful to generate labels</p> <code>'m'</code> <code>transform</code> <code>Callable[[float], float]</code> <p>A function to transform each duration</p> <code>lambda x: x</code> <code>maxy</code> <code>Optional[float]</code> <p>Maximum y value of the plot</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>Axis where plot has been drawn</p> Source code in <code>ceruleo/graphics/duration.py</code> <pre><code>def durations_boxplot(\n    datasets: Union[AbstractPDMDataset, List[AbstractPDMDataset]],\n    xlabel: Union[str, List[str]],\n    ylabel: str = 'Cycle Duration',\n    ax:Optional[matplotlib.axes.Axes]=None,\n    hlines: List[Tuple[float, str]] = [],\n    units: str = \"m\",\n    transform: Callable[[float], float] = lambda x: x,\n    maxy: Optional[float] = None,\n    **kwargs,\n) -&gt;  matplotlib.axes.Axes:\n    \"\"\"Generate boxplots of the lives duration\n\n    Example:\n\n        ax = durations_boxplot(\n            [train_dataset, validation_dataset],\n            xlabel=['Train', 'Validation'],\n            ylabel='Unit Cycles',\n            figsize=(17, 5))\n\n    Parameters:\n        datasets: Dataset from which take the lives durations\n        xlabel:  Label of each dataset to use as label in the boxplot\n        ylabel: Label of the y axis\n        ax: Axis where to draw the plot.If missing a new figure will be created\n        hlines: Horizontal lines to add to the figure in the form [(y_coordinate, label)]\n        units: Units of time of the lives. Useful to generate labels\n        transform: A function to transform each duration\n        maxy: Maximum y value of the plot\n\n    Returns:\n        Axis where plot has been drawn\n    \"\"\"\n    if isinstance(datasets, list):\n        assert isinstance(xlabel, list)\n        assert isinstance(datasets, list)\n        assert len(datasets) == len(xlabel)\n        xlabel_list = xlabel\n        datasets_list = datasets\n    else:\n        assert isinstance(xlabel, str)\n        datasets_list = [datasets]\n        xlabel_list = [xlabel]\n\n    durations = []\n    for ds in datasets_list:\n        durations.append([transform(duration) for duration in ds.durations()])\n\n    return boxplot_from_durations(\n        durations,\n        xlabel=xlabel_list,\n        ylabel=ylabel,\n        ax=ax,\n        hlines=hlines,\n        units=units,\n        maxy=maxy,\n        **kwargs,\n    )\n</code></pre>"},{"location":"dataset/visualization/#ceruleo.graphics.duration.durations_histogram","title":"<code>durations_histogram(datasets, *, label, xlabel='Cycle Duration', bins=15, units='m', vlines=[], ax=None, add_mean=True, add_median=True, transform=lambda x: x, threshold=np.inf, color=None, **kwargs)</code>","text":"<p>Generate an histogram from the lives durations of the dataset</p> <p>Example: '''     durations_histogram(         [train_dataset,validation_dataset],         label=['Train','Validation'],         xlabel='Unit Cycles',         units='cycles',         figsize=(17, 5)); '''</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[AbstractPDMDataset, List[AbstractPDMDataset]]</code> <p>Dataset from which take the lives durations</p> required <code>xlabel</code> <code>str</code> <p>Label of the x axis, by default Cycle Duration</p> <code>'Cycle Duration'</code> <code>label</code> <code>Union[str, List[str]]</code> <p>Label of each dataset to use as label in the boxplot, by default 1</p> required <code>bins</code> <code>int</code> <p>Number of bins to compute in the histogram, by default 15</p> <code>15</code> <code>units</code> <code>str</code> <p>Units of time of the lives. Useful to generate labels, by default m</p> <code>'m'</code> <code>vlines</code> <code>List[Tuple[float, str]]</code> <p>Vertical lines to add to the figure in the form [(x_coordinate, label)]</p> <code>[]</code> <code>ax</code> <code>Optional[Axes]</code> <p>Axis where to draw the plot. If missing a new figure will be created</p> <code>None</code> <code>add_mean</code> <code>bool</code> <p>Whether to add a vertical line with the mean value, by default True</p> <code>True</code> <code>add_median</code> <code>bool</code> <p>whether to add a vertical line with the median value, by default True</p> <code>True</code> <code>transform</code> <code>Callable[[float], float]</code> <p>A function to transform each duration, by default identity transform</p> <code>lambda x: x</code> <code>threshold</code> <code>float</code> <p>Includes duration less than the threshold, by default np.inf</p> <code>inf</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The axis in which the histogram was created</p> Source code in <code>ceruleo/graphics/duration.py</code> <pre><code>def durations_histogram(\n    datasets: Union[AbstractPDMDataset, List[AbstractPDMDataset]],\n    *,\n    label: Union[str, List[str]],\n    xlabel: str = 'Cycle Duration',    \n    bins: int = 15,\n    units: str = \"m\",\n    vlines: List[Tuple[float, str]] = [],\n    ax:Optional[matplotlib.axes.Axes]=None,\n    add_mean: bool = True,\n    add_median: bool = True,\n    transform: Callable[[float], float] = lambda x: x,\n    threshold: float = np.inf,\n    color=None,\n    **kwargs,\n) -&gt;  matplotlib.axes.Axes:\n    \"\"\"Generate an histogram from the lives durations of the dataset\n\n    Example:\n    '''\n        durations_histogram(\n            [train_dataset,validation_dataset],\n            label=['Train','Validation'],\n            xlabel='Unit Cycles',\n            units='cycles',\n            figsize=(17, 5));\n    '''\n\n    Parameters:\n        datasets: Dataset from which take the lives durations\n        xlabel: Label of the x axis, by default Cycle Duration\n        label: Label of each dataset to use as label in the boxplot, by default 1\n        bins:  Number of bins to compute in the histogram, by default 15\n        units: Units of time of the lives. Useful to generate labels, by default m\n        vlines: Vertical lines to add to the figure in the form [(x_coordinate, label)]\n        ax: Axis where to draw the plot. If missing a new figure will be created\n        add_mean: Whether to add a vertical line with the mean value, by default True\n        add_median: whether to add a vertical line with the median value, by default True\n        transform: A function to transform each duration, by default identity transform\n        threshold: Includes duration less than the threshold, by default np.inf\n\n    Returns:\n        The axis in which the histogram was created\n\n    \"\"\"\n    if isinstance(datasets, list):\n        assert isinstance(label,list)\n        assert len(datasets) == len(label)\n        label_list = label\n    else:\n        assert isinstance(label, str)\n        datasets = [datasets]\n        label_list = [label]\n\n    durations = []\n    for ds in datasets:\n        durations.append([transform(duration) for duration in ds.durations()])\n\n    return histogram_from_durations(\n        durations,\n        xlabel=xlabel,\n        label=label_list,\n        bins=bins,\n        units=units,\n        vlines=vlines,\n        ax=ax,\n        add_mean=add_mean,\n        add_median=add_median,\n        threshold=threshold,\n        color=color,\n        **kwargs,\n    )\n</code></pre>"},{"location":"dataset/analysis/Sensor%20Validation/","title":"Notebook: Dataset analysis","text":"In\u00a0[15]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</pre> In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sbn\n\nsbn.set_theme()\n</pre> import matplotlib.pyplot as plt import seaborn as sbn  sbn.set_theme() In\u00a0[17]: Copied! <pre>from ceruleo.dataset.catalog.PHMDataset2018 import PHMDataset2018, FailureType\n</pre> from ceruleo.dataset.catalog.PHMDataset2018 import PHMDataset2018, FailureType In\u00a0[18]: Copied! <pre>dataset = PHMDataset2018(tools=[\"01M01\", \"04M01\"])\n</pre> dataset = PHMDataset2018(tools=[\"01M01\", \"04M01\"]) In\u00a0[19]: Copied! <pre>from ceruleo.dataset.analysis.numerical_features import analyze_as_dataframe, analyze\nfrom ceruleo.transformation.features.selection import (\n    ByNameFeatureSelector,\n)\nfrom ceruleo.transformation.functional.pipeline.pipeline import make_pipeline\nfrom ceruleo.transformation.functional.transformers import Transformer\n</pre> from ceruleo.dataset.analysis.numerical_features import analyze_as_dataframe, analyze from ceruleo.transformation.features.selection import (     ByNameFeatureSelector, ) from ceruleo.transformation.functional.pipeline.pipeline import make_pipeline from ceruleo.transformation.functional.transformers import Transformer In\u00a0[20]: Copied! <pre>from ceruleo.transformation.features.cast import ToDateTime\nfrom ceruleo.transformation.features.scalers import MinMaxScaler\n\n\nFEATURES = [\n    \"IONGAUGEPRESSURE\",\n    \"ETCHBEAMVOLTAGE\",\n    \"ETCHBEAMCURRENT\",\n    \"ETCHSUPPRESSORVOLTAGE\",\n    \"ETCHSUPPRESSORCURRENT\",\n    \"FLOWCOOLFLOWRATE\",\n    \"FLOWCOOLPRESSURE\",\n    \"ETCHGASCHANNEL1READBACK\",\n    \"ETCHPBNGASREADBACK\",\n    \"FIXTURETILTANGLE\",\n    \"ROTATIONSPEED\",\n    \"ACTUALROTATIONANGLE\",\n    \"FIXTURESHUTTERPOSITION\",\n    \"ETCHSOURCEUSAGE\",\n    \"ETCHAUXSOURCETIMER\",\n    \"ETCHAUX2SOURCETIMER\",\n    \"ACTUALSTEPDURATION\",\n]\ntransformer = Transformer(\n    pipelineX=make_pipeline(\n        ToDateTime(index=True),\n        ByNameFeatureSelector(features=FEATURES),\n        MinMaxScaler(range=(-1, 1))\n    ),\n    pipelineY=make_pipeline(\n        ToDateTime(index=True),\n        ByNameFeatureSelector(features=[\"RUL\"]),\n    ),\n)\n\ntransformed_dataset = transformer.fit_map(dataset)\n</pre> from ceruleo.transformation.features.cast import ToDateTime from ceruleo.transformation.features.scalers import MinMaxScaler   FEATURES = [     \"IONGAUGEPRESSURE\",     \"ETCHBEAMVOLTAGE\",     \"ETCHBEAMCURRENT\",     \"ETCHSUPPRESSORVOLTAGE\",     \"ETCHSUPPRESSORCURRENT\",     \"FLOWCOOLFLOWRATE\",     \"FLOWCOOLPRESSURE\",     \"ETCHGASCHANNEL1READBACK\",     \"ETCHPBNGASREADBACK\",     \"FIXTURETILTANGLE\",     \"ROTATIONSPEED\",     \"ACTUALROTATIONANGLE\",     \"FIXTURESHUTTERPOSITION\",     \"ETCHSOURCEUSAGE\",     \"ETCHAUXSOURCETIMER\",     \"ETCHAUX2SOURCETIMER\",     \"ACTUALSTEPDURATION\", ] transformer = Transformer(     pipelineX=make_pipeline(         ToDateTime(index=True),         ByNameFeatureSelector(features=FEATURES),         MinMaxScaler(range=(-1, 1))     ),     pipelineY=make_pipeline(         ToDateTime(index=True),         ByNameFeatureSelector(features=[\"RUL\"]),     ), )  transformed_dataset = transformer.fit_map(dataset) In\u00a0[21]: Copied! <pre>from ceruleo.dataset.analysis.sample_rate import sample_rate, sample_rate_summary\n</pre> from ceruleo.dataset.analysis.sample_rate import sample_rate, sample_rate_summary In\u00a0[22]: Copied! <pre>sample_rates = sample_rate(dataset)\nfig, ax = plt.subplots(1, 2, figsize=(17, 5))\nax[0].boxplot(sample_rates, labels=[\"Sample rate\"])\nax[0].set_ylabel(\"Seconds [s]\")\n\nax[1].boxplot(sample_rates, labels=[\"Sample rate\"])\nax[1].set_ylabel(\"Seconds [s]\")\nax[1].set_ylim(0, 10)\n</pre> sample_rates = sample_rate(dataset) fig, ax = plt.subplots(1, 2, figsize=(17, 5)) ax[0].boxplot(sample_rates, labels=[\"Sample rate\"]) ax[0].set_ylabel(\"Seconds [s]\")  ax[1].boxplot(sample_rates, labels=[\"Sample rate\"]) ax[1].set_ylabel(\"Seconds [s]\") ax[1].set_ylim(0, 10) Out[22]: <pre>(0.0, 10.0)</pre> In\u00a0[23]: Copied! <pre>sample_rate_summary(dataset)\n</pre> sample_rate_summary(dataset) Out[23]: <p>  Median:  4.0 [s]  </p> <p>   Mean +- Std:  3.980 +- 0.219 [s] </p> In\u00a0[24]: Copied! <pre>rr = analyze(transformed_dataset, show_progress=False, what_to_compute=[\"null\", \"std\"])\nrr[\"IONGAUGEPRESSURE\"][\"std\"]\n</pre> rr = analyze(transformed_dataset, show_progress=False, what_to_compute=[\"null\", \"std\"]) rr[\"IONGAUGEPRESSURE\"][\"std\"] Out[24]: <pre>[0.1538784215135119,\n 0.16151160115580462,\n 0.15799453196073904,\n 0.13861749490903919,\n 0.16566794042040264,\n 0.16425937954116393,\n 0.1703799320940931,\n 0.17711007507379947,\n 0.16845221328428622,\n 0.1703432255813474,\n 0.17112279778174258,\n 0.16960183601086146,\n 0.1644573963958884,\n 0.1667180196658114,\n 0.14469551314577048,\n 0.16112822632189672,\n 0.15470216323219949,\n 0.19875805450944148,\n 0.1573624712039302,\n 0.18292432023762176,\n 0.17527184147763705,\n 0.1867674364630246,\n 0.17406487007437757,\n 0.16549787029020377]</pre> <p>We can compute the summary that contains the mean, standard deviation, minimum, maximum, and quantiles of the features for each run-to-failure-cycle. This summary can be used to identify patterns and anomalies in the data. In this case we have the mean value averaged over all the cycles. If a feature was not present in multiple cycles the value will tend to increase.</p> In\u00a0[25]: Copied! <pre>feature_summary = rr[\"IONGAUGEPRESSURE\"].summarize()\nfeature_summary[\"null\"]\n</pre> feature_summary = rr[\"IONGAUGEPRESSURE\"].summarize() feature_summary[\"null\"] Out[25]: <pre>MetricValuesSummary(mean=0.0, std=0.0, max=0.0, min=0.0)</pre> <p>It is possible finally obtain everything the summary as a DataFrame</p> In\u00a0[26]: Copied! <pre>analyze_as_dataframe(transformed_dataset, what_to_compute=[\"null\", \"std\"])\n</pre> analyze_as_dataframe(transformed_dataset, what_to_compute=[\"null\", \"std\"]) Out[26]: null std Mean value across the cycles Standard deviation across the cycles Maximum value found in a cycle Minimum value found in a cycle Mean value across the cycles Standard deviation across the cycles Maximum value found in a cycle Minimum value found in a cycle IONGAUGEPRESSURE 0.000000 0.000000 0.0000 0.0 0.166720 0.012588 0.198758 1.386175e-01 ETCHBEAMVOLTAGE 0.000000 0.000000 0.0000 0.0 0.380300 0.019524 0.409515 3.239813e-01 ETCHBEAMCURRENT 0.000000 0.000000 0.0000 0.0 0.414080 0.020573 0.441704 3.509712e-01 ETCHSUPPRESSORVOLTAGE 0.000000 0.000000 0.0000 0.0 0.557341 0.050995 0.716162 4.599301e-01 ETCHSUPPRESSORCURRENT 0.000000 0.000000 0.0000 0.0 0.077735 0.005504 0.088157 6.658743e-02 FLOWCOOLFLOWRATE 0.000000 0.000000 0.0000 0.0 0.412940 0.035307 0.503905 3.417936e-01 FLOWCOOLPRESSURE 0.000000 0.000000 0.0000 0.0 0.130684 0.078748 0.401497 8.276058e-02 ETCHGASCHANNEL1READBACK 0.000000 0.000000 0.0000 0.0 0.447334 0.027615 0.516869 3.771744e-01 ETCHPBNGASREADBACK 0.000000 0.000000 0.0000 0.0 0.431853 0.030993 0.513432 3.650782e-01 FIXTURETILTANGLE 0.000000 0.000000 0.0000 0.0 0.089086 0.023832 0.148195 5.551115e-16 ROTATIONSPEED 0.000000 0.000000 0.0000 0.0 0.038895 0.027090 0.085676 0.000000e+00 ACTUALROTATIONANGLE 0.000000 0.000000 0.0000 0.0 0.049563 0.020486 0.087564 8.974330e-03 FIXTURESHUTTERPOSITION 0.000083 0.000241 0.0011 0.0 0.019717 0.018796 0.057795 3.994086e-03 ETCHSOURCEUSAGE 0.000000 0.000000 0.0000 0.0 0.154297 0.191935 0.732096 9.668065e-05 ETCHAUXSOURCETIMER 0.000000 0.000000 0.0000 0.0 0.125170 0.164681 0.495712 4.452867e-05 ETCHAUX2SOURCETIMER 0.000000 0.000000 0.0000 0.0 0.191197 0.206324 0.620918 9.676074e-05 ACTUALSTEPDURATION 0.000000 0.000000 0.0000 0.0 0.111877 0.042060 0.192831 4.440892e-16 In\u00a0[27]: Copied! <pre>analyze_as_dataframe(transformed_dataset, what_to_compute=[\"null\"])\n</pre> analyze_as_dataframe(transformed_dataset, what_to_compute=[\"null\"]) Out[27]: null Mean value across the cycles Standard deviation across the cycles Maximum value found in a cycle Minimum value found in a cycle IONGAUGEPRESSURE 0.000000 0.000000 0.0000 0.0 ETCHBEAMVOLTAGE 0.000000 0.000000 0.0000 0.0 ETCHBEAMCURRENT 0.000000 0.000000 0.0000 0.0 ETCHSUPPRESSORVOLTAGE 0.000000 0.000000 0.0000 0.0 ETCHSUPPRESSORCURRENT 0.000000 0.000000 0.0000 0.0 FLOWCOOLFLOWRATE 0.000000 0.000000 0.0000 0.0 FLOWCOOLPRESSURE 0.000000 0.000000 0.0000 0.0 ETCHGASCHANNEL1READBACK 0.000000 0.000000 0.0000 0.0 ETCHPBNGASREADBACK 0.000000 0.000000 0.0000 0.0 FIXTURETILTANGLE 0.000000 0.000000 0.0000 0.0 ROTATIONSPEED 0.000000 0.000000 0.0000 0.0 ACTUALROTATIONANGLE 0.000000 0.000000 0.0000 0.0 FIXTURESHUTTERPOSITION 0.000083 0.000241 0.0011 0.0 ETCHSOURCEUSAGE 0.000000 0.000000 0.0000 0.0 ETCHAUXSOURCETIMER 0.000000 0.000000 0.0000 0.0 ETCHAUX2SOURCETIMER 0.000000 0.000000 0.0000 0.0 ACTUALSTEPDURATION 0.000000 0.000000 0.0000 0.0 <p>We can se that there are 4 cycles with a proportion of missing values for this feature &gt; 0, but with an small proportion.</p> In\u00a0[28]: Copied! <pre>rr = analyze(transformed_dataset, show_progress=False, what_to_compute=[\"null\"])\nrr[\"FIXTURESHUTTERPOSITION\"][\"null\"]\n</pre> rr = analyze(transformed_dataset, show_progress=False, what_to_compute=[\"null\"]) rr[\"FIXTURESHUTTERPOSITION\"][\"null\"] Out[28]: <pre>[0.0,\n 0.0011000495022276003,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0001789318484375671,\n 0.0005336307435343965,\n 0.00017414200235439987,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0]</pre> In\u00a0[29]: Copied! <pre>analyze_as_dataframe(transformed_dataset, what_to_compute=[\"std\"])\n</pre> analyze_as_dataframe(transformed_dataset, what_to_compute=[\"std\"]) Out[29]: std Mean value across the cycles Standard deviation across the cycles Maximum value found in a cycle Minimum value found in a cycle IONGAUGEPRESSURE 0.166720 0.012588 0.198758 1.386175e-01 ETCHBEAMVOLTAGE 0.380300 0.019524 0.409515 3.239813e-01 ETCHBEAMCURRENT 0.414080 0.020573 0.441704 3.509712e-01 ETCHSUPPRESSORVOLTAGE 0.557341 0.050995 0.716162 4.599301e-01 ETCHSUPPRESSORCURRENT 0.077735 0.005504 0.088157 6.658743e-02 FLOWCOOLFLOWRATE 0.412940 0.035307 0.503905 3.417936e-01 FLOWCOOLPRESSURE 0.130684 0.078748 0.401497 8.276058e-02 ETCHGASCHANNEL1READBACK 0.447334 0.027615 0.516869 3.771744e-01 ETCHPBNGASREADBACK 0.431853 0.030993 0.513432 3.650782e-01 FIXTURETILTANGLE 0.089086 0.023832 0.148195 5.551115e-16 ROTATIONSPEED 0.038895 0.027090 0.085676 0.000000e+00 ACTUALROTATIONANGLE 0.049563 0.020486 0.087564 8.974330e-03 FIXTURESHUTTERPOSITION 0.019717 0.018796 0.057795 3.994086e-03 ETCHSOURCEUSAGE 0.154297 0.191935 0.732096 9.668065e-05 ETCHAUXSOURCETIMER 0.125170 0.164681 0.495712 4.452867e-05 ETCHAUX2SOURCETIMER 0.191197 0.206324 0.620918 9.676074e-05 ACTUALSTEPDURATION 0.111877 0.042060 0.192831 4.440892e-16 <p>We can se that there are a lot of cycles with a low variance</p> In\u00a0[30]: Copied! <pre>rr = analyze(transformed_dataset, show_progress=False, what_to_compute=[\"std\"])\nrr[\"ROTATIONSPEED\"][\"std\"]\n</pre> rr = analyze(transformed_dataset, show_progress=False, what_to_compute=[\"std\"]) rr[\"ROTATIONSPEED\"][\"std\"] Out[30]: <pre>[0.0195906016364457,\n 0.04336851470681955,\n 0.05799620898185587,\n 1.1102230246251565e-16,\n 0.0360890061340275,\n 0.038201237893065954,\n 1.1102230246251565e-16,\n 0.06606608560204476,\n 0.04426971331940611,\n 0.06907071252869629,\n 0.08567629006783237,\n 0.05527193794092481,\n 0.048860280663576686,\n 0.048314164539748315,\n 0.0630512429349472,\n 0.038968725642062135,\n 0.06539096888505944,\n 0.07326999721034945,\n 0.01789179320551508,\n 5.551115123125783e-17,\n 1.1102230246251565e-16,\n 0.0,\n 5.551115123125783e-17,\n 0.06212923988601241]</pre> <p>If we inspect these cycles we can se that, in fact, this feature has a constant value.</p> In\u00a0[122]: Copied! <pre>fig, ax = plt.subplots(3, 1, figsize=(17, 5))\nax[0].plot(transformed_dataset[3][0][\"ROTATIONSPEED\"])\nax[1].plot(transformed_dataset[6][0][\"ROTATIONSPEED\"])\nax[2].plot(transformed_dataset[-2][0][\"ROTATIONSPEED\"])\nfig.tight_layout()\n</pre> fig, ax = plt.subplots(3, 1, figsize=(17, 5)) ax[0].plot(transformed_dataset[3][0][\"ROTATIONSPEED\"]) ax[1].plot(transformed_dataset[6][0][\"ROTATIONSPEED\"]) ax[2].plot(transformed_dataset[-2][0][\"ROTATIONSPEED\"]) fig.tight_layout() In\u00a0[32]: Copied! <pre>analyze_as_dataframe(transformed_dataset, what_to_compute=[\"monotonicity\"]).sort_values(\n    by=(\"monotonicity\", \"Mean value across the cycles\"), ascending=False\n)\n</pre> analyze_as_dataframe(transformed_dataset, what_to_compute=[\"monotonicity\"]).sort_values(     by=(\"monotonicity\", \"Mean value across the cycles\"), ascending=False ) Out[32]: monotonicity Mean value across the cycles Standard deviation across the cycles Maximum value found in a cycle Minimum value found in a cycle ETCHAUXSOURCETIMER 0.629001 0.081907 0.767106 0.420611 ETCHAUX2SOURCETIMER 0.628923 0.081898 0.766641 0.420611 ETCHSOURCEUSAGE 0.591408 0.108967 0.781429 0.325093 IONGAUGEPRESSURE 0.131580 0.037162 0.246032 0.081020 ETCHBEAMCURRENT 0.079963 0.022276 0.122318 0.039683 ETCHSUPPRESSORVOLTAGE 0.049960 0.021527 0.119048 0.021107 FLOWCOOLPRESSURE 0.036722 0.060791 0.317460 0.001423 ETCHSUPPRESSORCURRENT 0.027669 0.016906 0.085868 0.000000 ETCHPBNGASREADBACK 0.010919 0.006596 0.025808 0.005138 FLOWCOOLFLOWRATE 0.008001 0.003239 0.014125 0.002467 ETCHGASCHANNEL1READBACK 0.007628 0.004352 0.023810 0.001789 ETCHBEAMVOLTAGE 0.004334 0.005614 0.023810 0.000193 FIXTURETILTANGLE 0.000713 0.000339 0.001484 0.000000 ACTUALROTATIONANGLE 0.000430 0.001567 0.007937 0.000000 ACTUALSTEPDURATION 0.000287 0.000339 0.001789 0.000000 FIXTURESHUTTERPOSITION 0.000087 0.000067 0.000194 0.000000 ROTATIONSPEED 0.000005 0.000021 0.000108 0.000000 In\u00a0[33]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(17, 5))\nax.plot(transformed_dataset[-4][0][\"ETCHSOURCEUSAGE\"].values)\n</pre> fig, ax = plt.subplots(1, 1, figsize=(17, 5)) ax.plot(transformed_dataset[-4][0][\"ETCHSOURCEUSAGE\"].values) Out[33]: <pre>[&lt;matplotlib.lines.Line2D at 0x1bbaee821a0&gt;]</pre> In\u00a0[34]: Copied! <pre>analyze_as_dataframe(transformed_dataset, what_to_compute=[\"correlation\"]).sort_values(\n    by=(\"correlation\", \"Mean value across the cycles\"), ascending=False\n)\n</pre> analyze_as_dataframe(transformed_dataset, what_to_compute=[\"correlation\"]).sort_values(     by=(\"correlation\", \"Mean value across the cycles\"), ascending=False ) Out[34]: correlation Mean value across the cycles Standard deviation across the cycles Maximum value found in a cycle Minimum value found in a cycle ETCHAUXSOURCETIMER 0.844595 0.371078 1.000000 -0.264857 ETCHSOURCEUSAGE 0.783816 0.437367 0.999995 -0.432908 ETCHAUX2SOURCETIMER 0.743158 0.469988 1.000000 -0.432908 FLOWCOOLPRESSURE 0.034503 0.305846 0.603682 -0.916936 ACTUALSTEPDURATION 0.015444 0.183581 0.412032 -0.459476 FIXTURETILTANGLE 0.003311 0.122210 0.318482 -0.223626 ETCHGASCHANNEL1READBACK -0.010918 0.202759 0.307965 -0.688285 ETCHBEAMCURRENT -0.011158 0.209831 0.419246 -0.606570 ROTATIONSPEED -0.013429 0.074998 0.118146 -0.187191 ACTUALROTATIONANGLE -0.024491 0.271434 0.751982 -0.558149 FLOWCOOLFLOWRATE -0.025738 0.209273 0.352479 -0.575904 ETCHSUPPRESSORVOLTAGE -0.028417 0.236531 0.407153 -0.609866 ETCHBEAMVOLTAGE -0.029537 0.226633 0.475763 -0.677671 FIXTURESHUTTERPOSITION -0.032806 0.199978 0.295252 -0.701687 IONGAUGEPRESSURE -0.036029 0.276078 0.562739 -0.771059 ETCHSUPPRESSORCURRENT -0.040677 0.216098 0.275416 -0.845663 ETCHPBNGASREADBACK -0.045505 0.206210 0.268059 -0.593234 In\u00a0[35]: Copied! <pre>from ceruleo.dataset.analysis.correlation import correlation_analysis\n</pre> from ceruleo.dataset.analysis.correlation import correlation_analysis In\u00a0[36]: Copied! <pre>(\n    correlation_analysis(transformed_dataset)\n    .to_pandas()\n    .sort_values(by=\"abs_mean_correlation\", ascending=False)\n    .head(15)\n)\n</pre> (     correlation_analysis(transformed_dataset)     .to_pandas()     .sort_values(by=\"abs_mean_correlation\", ascending=False)     .head(15) ) Out[36]: feature_1 feature_2 mean_correlation std_correlation max_correlation min_correlation abs_mean_correlation std_abs_mean_correlation 81 ETCHGASCHANNEL1READBACK ETCHPBNGASREADBACK 0.974040 0.015899 0.998851 0.909530 0.974040 0.015899 62 ETCHBEAMCURRENT ETCHSUPPRESSORCURRENT 0.969555 0.017441 0.999941 0.926967 0.969555 0.017441 73 ETCHBEAMVOLTAGE ETCHSUPPRESSORCURRENT 0.961725 0.019763 0.999911 0.907479 0.961725 0.019763 89 ETCHGASCHANNEL1READBACK IONGAUGEPRESSURE 0.961469 0.012155 0.983084 0.935980 0.961469 0.012155 58 ETCHBEAMCURRENT ETCHBEAMVOLTAGE 0.961391 0.018625 0.999966 0.896134 0.961391 0.018625 98 ETCHPBNGASREADBACK IONGAUGEPRESSURE 0.951679 0.033223 0.987659 0.807029 0.951679 0.033223 31 ETCHAUX2SOURCETIMER ETCHAUXSOURCETIMER 0.833866 0.481966 1.000000 -0.772497 0.951071 0.118294 96 ETCHPBNGASREADBACK FLOWCOOLFLOWRATE 0.933385 0.020826 0.991879 0.885555 0.933385 0.020826 87 ETCHGASCHANNEL1READBACK FLOWCOOLFLOWRATE 0.914023 0.027393 0.996820 0.867820 0.914023 0.027393 68 ETCHBEAMCURRENT IONGAUGEPRESSURE 0.903116 0.023172 0.970433 0.873936 0.903116 0.023172 113 ETCHSUPPRESSORCURRENT IONGAUGEPRESSURE 0.897708 0.025894 0.970394 0.842836 0.897708 0.025894 36 ETCHAUX2SOURCETIMER ETCHSOURCEUSAGE 0.878960 0.299466 1.000000 -0.172562 0.893340 0.251169 131 FLOWCOOLFLOWRATE IONGAUGEPRESSURE 0.887545 0.038323 0.970444 0.777352 0.887545 0.038323 93 ETCHPBNGASREADBACK ETCHSUPPRESSORVOLTAGE 0.885015 0.035778 0.992222 0.818617 0.885015 0.035778 59 ETCHBEAMCURRENT ETCHGASCHANNEL1READBACK 0.880769 0.033310 0.996809 0.839003 0.880769 0.033310 In\u00a0[37]: Copied! <pre>fig, ax = plt.subplots(figsize=(17, 5))\nax.plot(transformed_dataset.get_features_of_life(12)['ETCHBEAMCURRENT'].values[-1000:],\n        label='ETCHBEAMCURRENT')\nax.plot(transformed_dataset.get_features_of_life(12)['ETCHSUPPRESSORCURRENT'].values[-1000:], \n        label='ETCHSUPPRESSORCURRENT')\nax.legend()\n</pre> fig, ax = plt.subplots(figsize=(17, 5)) ax.plot(transformed_dataset.get_features_of_life(12)['ETCHBEAMCURRENT'].values[-1000:],         label='ETCHBEAMCURRENT') ax.plot(transformed_dataset.get_features_of_life(12)['ETCHSUPPRESSORCURRENT'].values[-1000:],          label='ETCHSUPPRESSORCURRENT') ax.legend() Out[37]: <pre>&lt;matplotlib.legend.Legend at 0x1bba056e530&gt;</pre> In\u00a0[43]: Copied! <pre>from ceruleo.dataset.analysis.distribution import features_divergeces\n</pre> from ceruleo.dataset.analysis.distribution import features_divergeces In\u00a0[100]: Copied! <pre>d = features_divergeces(transformed_dataset, number_of_bins=5)\n# Keep cycles with length &gt; 1000\nd = d[ (d[\"Cycle 1 length\"] &gt; 1000) &amp; (d[\"Cycle 2 length\"] &gt; 1000)]\n# Keep Pairs of cycles with similar length\nd = d[d[\"Abs Length difference\"] &lt; 1000]\n</pre> d = features_divergeces(transformed_dataset, number_of_bins=5) # Keep cycles with length &gt; 1000 d = d[ (d[\"Cycle 1 length\"] &gt; 1000) &amp; (d[\"Cycle 2 length\"] &gt; 1000)] # Keep Pairs of cycles with similar length d = d[d[\"Abs Length difference\"] &lt; 1000] In\u00a0[120]: Copied! <pre>d.sort_values(by=[ \"KL\", \"Abs Length difference\",], ascending=[False, True]).head(15)\n</pre> d.sort_values(by=[ \"KL\", \"Abs Length difference\",], ascending=[False, True]).head(15) Out[120]: Cycle 1 Cycle 2 Cycle 1 length Cycle 2 length Abs Length difference Wasserstein KL feature 615 2 21 22801 22588 213 0.000000 23.025851 ETCHAUX2SOURCETIMER 891 2 21 22801 22588 213 0.000000 23.025851 ETCHAUXSOURCETIMER 689 6 21 21591 22588 997 0.000000 23.025851 ETCHAUX2SOURCETIMER 2345 6 21 21591 22588 997 0.000000 23.025851 ETCHSOURCEUSAGE 965 6 21 21591 22588 997 0.028845 10.919734 ETCHAUXSOURCETIMER 1995 2 21 22801 22588 213 0.037269 0.895635 ETCHPBNGASREADBACK 4479 2 21 22801 22588 213 0.027420 0.771321 ROTATIONSPEED 2069 6 21 21591 22588 997 0.005030 0.686197 ETCHPBNGASREADBACK 63 2 21 22801 22588 213 0.018315 0.513243 ACTUALROTATIONANGLE 339 2 21 22801 22588 213 0.013813 0.386058 ACTUALSTEPDURATION 413 6 21 21591 22588 997 0.013813 0.386058 ACTUALSTEPDURATION 1443 2 21 22801 22588 213 0.157799 0.138403 ETCHBEAMVOLTAGE 1719 2 21 22801 22588 213 0.093787 0.138191 ETCHGASCHANNEL1READBACK 1793 6 21 21591 22588 997 0.082148 0.096925 ETCHGASCHANNEL1READBACK 1517 6 21 21591 22588 997 0.077764 0.072103 ETCHBEAMVOLTAGE In\u00a0[114]: Copied! <pre>def plot_distributions(ax, ds, life1, life2, feature, bins=10):\n    ax.hist(\n        ds.get_features_of_life(life1)[feature],\n        label=feature,\n        density=True,\n        alpha=0.8,\n        bins=bins,\n    )\n    ax.hist(\n        ds.get_features_of_life(life2)[feature],\n        label=feature,\n        density=True,\n        alpha=0.8,\n        bins=bins,\n    )\n    ax.legend()\n\n\ndef plot_timeseries(ax, ds, life, feature):\n    ax.plot(ds.get_features_of_life(life)[feature].values, alpha=0.5, label=feature)\n</pre> def plot_distributions(ax, ds, life1, life2, feature, bins=10):     ax.hist(         ds.get_features_of_life(life1)[feature],         label=feature,         density=True,         alpha=0.8,         bins=bins,     )     ax.hist(         ds.get_features_of_life(life2)[feature],         label=feature,         density=True,         alpha=0.8,         bins=bins,     )     ax.legend()   def plot_timeseries(ax, ds, life, feature):     ax.plot(ds.get_features_of_life(life)[feature].values, alpha=0.5, label=feature) In\u00a0[116]: Copied! <pre>first_row = d.sort_values(by=[ \"KL\", \"Abs Length difference\",], ascending=[False, True]).iloc[5, :]\nfeature = first_row[\"feature\"]\nlife1 = first_row[\"Cycle 1\"]\nlife2 = first_row[\"Cycle 2\"]\nfig, ax = plt.subplots(2, 1, figsize=(17, 9))\nplot_distributions(ax[0], transformed_dataset, life1, life2, feature, bins=25)\n\nplot_timeseries(ax[1], transformed_dataset, life1, feature)\nplot_timeseries(ax[1], transformed_dataset, life2, feature)\nax[1].legend()\n</pre> first_row = d.sort_values(by=[ \"KL\", \"Abs Length difference\",], ascending=[False, True]).iloc[5, :] feature = first_row[\"feature\"] life1 = first_row[\"Cycle 1\"] life2 = first_row[\"Cycle 2\"] fig, ax = plt.subplots(2, 1, figsize=(17, 9)) plot_distributions(ax[0], transformed_dataset, life1, life2, feature, bins=25)  plot_timeseries(ax[1], transformed_dataset, life1, feature) plot_timeseries(ax[1], transformed_dataset, life2, feature) ax[1].legend()    Out[116]: <pre>&lt;matplotlib.legend.Legend at 0x1bbd516ee60&gt;</pre> In\u00a0[119]: Copied! <pre>fig, ax = plt.subplots(2, 1, figsize=(17, 9))\nfirst_row = d.sort_values(by=[ \"Wasserstein\", \"Abs Length difference\",], ascending=[False, True]).iloc[0, :]\nfeature = first_row[\"feature\"]\nlife1 = first_row[\"Cycle 1\"]\nlife2 = first_row[\"Cycle 2\"]\nplot_distributions(ax[0], transformed_dataset, life1, life2, feature, bins=25)\n\n\nplot_timeseries(ax[1], transformed_dataset, life1, feature)\nplot_timeseries(ax[1], transformed_dataset, life2, feature)\nax[1].legend()\n</pre> fig, ax = plt.subplots(2, 1, figsize=(17, 9)) first_row = d.sort_values(by=[ \"Wasserstein\", \"Abs Length difference\",], ascending=[False, True]).iloc[0, :] feature = first_row[\"feature\"] life1 = first_row[\"Cycle 1\"] life2 = first_row[\"Cycle 2\"] plot_distributions(ax[0], transformed_dataset, life1, life2, feature, bins=25)   plot_timeseries(ax[1], transformed_dataset, life1, feature) plot_timeseries(ax[1], transformed_dataset, life2, feature) ax[1].legend() Out[119]: <pre>&lt;matplotlib.legend.Legend at 0x1bbf08305e0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"dataset/analysis/Sensor%20Validation/#notebook-dataset-analysis","title":"Notebook: Dataset analysis\u00b6","text":"<p>Sensor data quality plays a vital role in Internet of Things (IoT) applications, particularly in predictive maintenance and Remaining Useful Life (RUL) estimation. Poor data quality can lead to unreliable predictions and decisions, which can cause significant operational and safety issues.</p> <p>One of the most commonly encountered problems in sensor data is missing data. Missing data can result from various factors, including unstable wireless connections due to network congestion, sensor device outages from limited battery life or environmental interferences. Prolonged periods of missing data can lead to inaccurate RUL predictions.</p> <p>In addition to addressing missing data, understanding the correlation between different sensor readings and the distribution of values in the dataset is crucial for accurate RUL estimation. Correlation analysis helps identify relationships between variables, which can be used to enhance predictive models. Analyzing value distribution provides insights into the data's behavior and highlights any anomalies or biases.</p> <p>In this notebook, we will perform an analysis of an RUL dataset. Our focus will be on:</p> <ul> <li>Missing Data: Identifying and handling missing data to ensure the dataset is complete and reliable for analysis.</li> <li>Monotonicty: Checking the monotonicity of sensor readings to ensure the data follows a logical pattern.</li> <li>Correlation Analysis: Examining the relationships between different sensor readings to improve the accuracy of predictive models.</li> <li>Value Distribution: Analyzing the distribution of sensor values to understand the data's behavior and identify any anomalies.</li> </ul>"},{"location":"dataset/analysis/Sensor%20Validation/#load-the-phmdataset2018-dataset","title":"Load the PHMDataset2018 dataset\u00b6","text":""},{"location":"dataset/analysis/Sensor%20Validation/#create-a-transformer-for-a-dataset","title":"Create a transformer for a dataset\u00b6","text":""},{"location":"dataset/analysis/Sensor%20Validation/#sample-rate","title":"Sample rate\u00b6","text":"<p>We can evalute the sample rate of the dataset without transformation. We can see that there are huge variations of the sample rate, but the vast majority of points are sampled after 4 seconds.</p>"},{"location":"dataset/analysis/Sensor%20Validation/#numeric-feature-analysis","title":"Numeric feature analysis\u00b6","text":"<p>The analysis function of the ceruleo.dataset.analysis.numerical_features module provides an overview of the numeric features in the dataset. The function calculates metrics for each feature and for each run-to-failure-cycle. In the following example the result of the function is a dictionary that contains for each feature a NumericalFeaturesAnalysis object that holds for each run-to-failure-cycle the metrics computed.</p> <p>IN this case we will have for the two metrics: null and std, 24 values for each metric. One metric for each run-to-failure-cycle. The null metric computes the percentage of null values in the feature and the std metric computes the standard deviation of the feature in a cycle.</p>"},{"location":"dataset/analysis/Sensor%20Validation/#missing-values","title":"Missing values\u00b6","text":"<p>Usually, the information of sensors is incomplete and this causes numerous missing values in the features. This library provides functions to analyse the proportion of the missing values for each feature for each life. In some cases, if the feature values are missing in multiple lives, that feature can be discarded. We can see that FIXTURESHUTTERPOSITION has a cycle with 0.0011% of missing values, in this case we can just impute those missingvalues.</p>"},{"location":"dataset/analysis/Sensor%20Validation/#feature-standard-deviation","title":"Feature standard deviation\u00b6","text":"<p>The standard deviation of the features can be used to identify features with low variance, which may not provide useful information for predictive maintenance tasks. Features with low variance can be removed from the dataset to reduce complexity and improve model performance.</p> <p>We can se that ROTATIONSPEED has a cycle with a minimum value of 0.0, this can be a problem for the model, because the feature will not provide any information for the model.</p>"},{"location":"dataset/analysis/Sensor%20Validation/#monotonicity","title":"monotonicity\u00b6","text":"<p>The monotonicity of the features can be used to identify features that exhibit a consistent trend over time. Monotonic features can provide valuable information for predictive maintenance tasks, as they capture the gradual degradation of equipment.</p> <p>We can see that that the timers are monotonic, this is expected, because the time is always increasing. And then the ETCHSOURCEUSAGE it's also monotonic, this is also expected, because the etch source usage is always increasing. In this case the presence of multiple correlated monotonic features can be a sign of multicollinearity, which can affect the performance of predictive models.</p>"},{"location":"dataset/analysis/Sensor%20Validation/#relation-with-the-target","title":"Relation with the target\u00b6","text":"<p>The relation between the features and the target variable can be analyzed using correlation coefficients. The correlation analysis can help identify features that are strongly related to the target variable and can be used to build predictive models.</p> <p>We can see that only the timers and usage are highly correlated with the target variable. This is expected because the target variable is the RUL and the timers and usage are the only features that are related to the time. The othrs features are not strongly correlated with the target variable.</p>"},{"location":"dataset/analysis/Sensor%20Validation/#feature-pairwise-correlation","title":"Feature pairwise correlation\u00b6","text":"<p>The pairwise correlation between features can be used to identify relationships between variables and detect multicollinearity. Multicollinearity occurs when two or more features are highly correlated, which can lead to unstable model coefficients and inaccurate predictions.</p> <p>We can see that ETCHGASCHANNEL1READBACK and ETCHPBNGASREADBACK are highly correlated. This is expected because these features are related to the same process.</p> <p>Another example is ETCHBEAMCURRENT and ETCHSUPPRESSORCURRENT.</p>"},{"location":"dataset/analysis/Sensor%20Validation/#etchbeamcurrent-and-etchsuppressorcurrent-are-highly-correlated","title":"ETCHBEAMCURRENT and ETCHSUPPRESSORCURRENT are highly correlated\u00b6","text":""},{"location":"dataset/analysis/Sensor%20Validation/#feature-distribution","title":"Feature distribution\u00b6","text":"<p>The distribution of feature values can provide insights into the data's behavior and identify any anomalies or biases. Understanding the distribution of features is crucial for building accurate predictive models and making informed decisions. ALso if there are distribution values difference accross cycles may indicate different operating conditions or equipment states that may affect the predictive model's performance.</p> <p>We are going to compare cycles with similar length to avoid problems when computing the distribution values. We can see that the top offenders present different distribution that may indicate a change in the operating conditions.</p>"},{"location":"dataset/analysis/Sensor%20Validation/#kl-divergence","title":"KL Divergence\u00b6","text":"<p>In this case if we see the top offenders, we can see tha tha timers are in the top. Probably because at the start of the cycles the usage or timers were at diffeernt values. The differnece is between the cycles 2 and 21, and 6 and 21 We can also see  that ETCHPBNGASREADBACK present a bias in the values</p>"},{"location":"dataset/analysis/Sensor%20Validation/#wasserstein","title":"Wasserstein\u00b6","text":"<p>We can also see  that ETCHBEAMVoltage present a bias in the values</p>"},{"location":"dataset/analysis/correlation/","title":"Pairwise correlations","text":""},{"location":"dataset/analysis/correlation/#ceruleo.dataset.analysis.correlation.correlation_analysis","title":"<code>correlation_analysis(dataset, features=None)</code>","text":"<p>Correlation Analysis Compute the correlation between all the features given an Iterable of executions.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>AbstractPDMDataset</code> <p>Dataset of time series</p> required <code>features</code> <code>Optional[List[str]]</code> <p>List of features to consider when computing the correlations</p> <code>None</code> <p>Returns:</p> Type Description <code>CorrelationAnalysis</code> <p>A CorrelationAnalysis object with map indexed by two colun names and the following information:s</p> <ul> <li>Mean Correlation</li> <li>Std Correlation</li> <li>Percentage of lives with a high correlation</li> <li>Abs mean correlation</li> <li>Std mean correlation</li> <li>Max correlation</li> <li>Min correlation</li> </ul> Source code in <code>ceruleo/dataset/analysis/correlation.py</code> <pre><code>def correlation_analysis(\n    dataset: AbstractPDMDataset,\n    features: Optional[List[str]] = None,\n) -&gt; CorrelationAnalysis:\n    \"\"\"\n    Correlation Analysis\n    Compute the correlation between all the features given an Iterable of executions.\n\n    Parameters:\n        dataset: Dataset of time series\n        features: List of features to consider when computing the correlations\n\n    Returns:\n        A CorrelationAnalysis object with map indexed by two colun names and the following information:s\n\n            - Mean Correlation\n            - Std Correlation\n            - Percentage of lives with a high correlation\n            - Abs mean correlation\n            - Std mean correlation\n            - Max correlation\n            - Min correlation\n\n    \"\"\"\n    if features is None:\n        features = sorted(list(dataset.common_features()))\n    else:\n        features = sorted(list(set(features).intersection(dataset.common_features())))\n    features = dataset.get_features_of_life(0)[features].corr().columns\n    correlated_features = []\n\n    for ex in iterate_over_features(dataset):\n        ex = ex[features]\n        corr_m = ex.corr().fillna(0)\n\n        correlated_features_for_execution = []\n\n        for f1, f2 in combinations(features, 2):\n            correlated_features_for_execution.append((f1, f2, corr_m.loc[f1, f2]))\n\n        correlated_features.extend(correlated_features_for_execution)\n\n    df = pd.DataFrame(correlated_features, columns=[\"Feature 1\", \"Feature 2\", \"Corr\"])\n    output = df.groupby(by=[\"Feature 1\", \"Feature 2\"]).agg(\n        {\n            \"Corr\": [\n                \"mean\",\n                \"std\",\n                \"max\",\n                \"min\",\n            ]\n        }\n    )\n\n    # Calculate additional statistics\n    output[\"Abs mean correlation\"] = df.groupby(by=[\"Feature 1\", \"Feature 2\"])[\n        \"Corr\"\n    ].apply(lambda x: x.abs().mean())\n    output[\"Std abs mean correlation\"] = df.groupby(by=[\"Feature 1\", \"Feature 2\"])[\n        \"Corr\"\n    ].apply(lambda x: x.abs().std())\n\n    output.columns = [\n        \"mean_correlation\",\n        \"std_correlation\",\n        \"max_correlation\",\n        \"min_correlation\",\n        \"abs_mean_correlation\",\n        \"std_abs_mean_correlation\",\n    ]\n\n    output = output.fillna(0)\n    return CorrelationAnalysis(\n        data={\n            (k[0], k[1]): CorrelationAnalysisElement(\n                mean_correlation=v[\"mean_correlation\"],\n                std_correlation=v[\"std_correlation\"],\n                max_correlation=v[\"max_correlation\"],\n                min_correlation=v[\"min_correlation\"],\n                abs_mean_correlation=v[\"abs_mean_correlation\"],\n                std_abs_mean_correlation=v[\"std_abs_mean_correlation\"],\n            )\n            for k, v in output.iterrows()\n        }\n    )\n</code></pre>"},{"location":"dataset/analysis/distribution/","title":"Feature distribution","text":""},{"location":"dataset/analysis/distribution/#ceruleo.dataset.analysis.distribution.features_divergeces","title":"<code>features_divergeces(ds, number_of_bins=15, columns=None, show_progress=False)</code>","text":"<p>Compute the divergence between features</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>AbstractPDMDataset</code> <p>The dataset</p> required <code>number_of_bins</code> <code>int</code> <p>Number of bins</p> <code>15</code> <code>columns</code> <code>Optional[List[str]]</code> <p>Which columns to use</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame in which each row contains the distances between a feature of two run-to-failure cycle with the following columns:</p> <ul> <li>Cycle 1: Run-to-failure cycle 1</li> <li>Cycle 2: Run-to-failure cycle 2</li> <li>Wasserstein: Wasserstein</li> <li>KL: KL Divergence</li> <li>feature: The feature name</li> </ul> Source code in <code>ceruleo/dataset/analysis/distribution.py</code> <pre><code>def features_divergeces(\n    ds: AbstractPDMDataset,\n    number_of_bins: int = 15,\n    columns: Optional[List[str]] = None,\n    show_progress: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the divergence between features\n\n    Parameters:\n        ds: The dataset\n        number_of_bins: Number of bins\n        columns: Which columns to use\n\n    Returns:\n        A DataFrame in which each row contains the distances between a feature of two run-to-failure cycle with the following columns:\n\n            - Cycle 1: Run-to-failure cycle 1\n            - Cycle 2: Run-to-failure cycle 2\n            - Wasserstein: Wasserstein\n            - KL: KL Divergence\n            - feature: The feature name\n    \"\"\"\n    if columns is None:\n        columns = ds.numeric_features()\n\n    features_bins = {}\n    iterator = tqdm(columns) if show_progress else columns\n\n    for feature in iterator:\n        features_bins[feature] = compute_bins(ds, feature, number_of_bins)\n\n    histograms = {}\n    for life in iterate_over_features(ds):\n        for feature in columns:\n            if feature not in histograms:\n                histograms[feature] = []\n            histograms[feature].append(\n                histogram_per_cycle(life, feature, features_bins[feature])\n            )\n\n    df_data = []\n    for feature in columns:\n        data = {}\n        for (i, h1), (j, h2) in itertools.combinations(\n            enumerate(histograms[feature]), 2\n        ):\n            kl = (np.mean(kl_div(h1, h2)) + np.mean(kl_div(h2, h1))) / 2\n            wd = wasserstein_distance(h1, h2)\n            df_data.append(\n                (\n                    i,\n                    j,\n                    ds.get_features_of_life(i).shape[0],\n                    ds.get_features_of_life(j).shape[0],\n                    abs(ds.get_features_of_life(i).shape[0]-ds.get_features_of_life(j).shape[0]),\n                    wd,\n                    kl,\n                    feature,\n                )\n            )\n    df = pd.DataFrame(\n        df_data,\n        columns=[\n            \"Cycle 1\",\n            \"Cycle 2\",\n            \"Cycle 1 length\",\n            \"Cycle 2 length\",\n            \"Abs Length difference\",           \n            \"Wasserstein\",\n            \"KL\",\n            \"feature\",\n        ],\n    )\n\n    return df\n</code></pre>"},{"location":"dataset/analysis/distribution/#ceruleo.dataset.analysis.distribution.histogram_per_cycle","title":"<code>histogram_per_cycle(cycle, feature, bins_to_use, normalize=True)</code>","text":"<p>Compute the histogram of a feature in a run-to-failure cycle</p> <p>Parameters:</p> Name Type Description Default <code>cycle</code> <code>DataFrame</code> <p>The run-to-failure cycle</p> required <code>feature</code> <code>str</code> <p>The  feature to compute the histogram</p> required <code>bins_to_use</code> <code>ndarray</code> <p>Number of bins to use</p> required <code>normalize</code> <code>bool</code> <p>Wheter to normalize the histogram. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List[np.ndarray]: The histogram of the feature</p> Source code in <code>ceruleo/dataset/analysis/distribution.py</code> <pre><code>def histogram_per_cycle(\n    cycle: pd.DataFrame,\n    feature: str,\n    bins_to_use: np.ndarray,\n    normalize: bool = True,\n) -&gt; List[np.ndarray]:\n    \"\"\"Compute the histogram of a feature in a run-to-failure cycle\n\n    Args:\n        cycle (pd.DataFrame): The run-to-failure cycle\n        feature (str): The  feature to compute the histogram\n        bins_to_use (np.ndarray): Number of bins to use\n        normalize (bool, optional): Wheter to normalize the histogram. Defaults to True.\n\n    Returns:\n        List[np.ndarray]: The histogram of the feature\n    \"\"\"\n    try:\n        d = cycle[feature]\n        h, _ = np.histogram(d, bins=bins_to_use)\n\n        if normalize:\n            h = h / np.sum(h)\n            h += 1e-50\n        return h\n    except Exception as e:\n        logger.info(f\"Error {e} when computing the distribution for feature {feature}\")\n</code></pre>"},{"location":"dataset/analysis/sample_rate/","title":"Sample rate analysis","text":""},{"location":"dataset/analysis/sample_rate/#ceruleo.dataset.analysis.sample_rate.sample_rate","title":"<code>sample_rate(ds, unit='s')</code>","text":"<p>Obtain an array of time difference between two consecutive samples</p> <p>If the index it's a timestamp, the time difference will be converted to the provided unit</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>AbstractPDMDataset</code> <p>The dataset</p> required <code>unit</code> <code>str</code> <p>Unit to convert the timestamps differences</p> <code>'s'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of time differences</p> Source code in <code>ceruleo/dataset/analysis/sample_rate.py</code> <pre><code>def sample_rate(ds: AbstractPDMDataset, unit: str = \"s\") -&gt; np.ndarray:\n    \"\"\"Obtain an array of time difference between two consecutive samples\n\n    If the index it's a timestamp, the time difference will be converted to the provided unit\n\n    Parameters:\n        ds: The dataset\n        unit: Unit to convert the timestamps differences\n\n    Returns:\n        Array of time differences\n\n    \"\"\"\n    time_diff : List[float ]= []\n    for life in ds:\n        diff = np.diff(life.index.values)\n        diff = diff[diff &lt;= np.median(diff)]\n        if pd.api.types.is_timedelta64_ns_dtype(diff.dtype):\n            diff = diff / np.timedelta64(1, unit)\n        time_diff.extend(diff)\n    return np.array(time_diff)\n</code></pre>"},{"location":"dataset/analysis/sample_rate/#ceruleo.dataset.analysis.sample_rate.sample_rate_summary","title":"<code>sample_rate_summary(ds, unit='s')</code>","text":"<p>Obtain the mean, median and standard deviation of the sample rate of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>AbstractPDMDataset</code> <p>The dataset</p> required <code>unit</code> <code>str</code> <p>Unit to convert the time differences</p> <code>'s'</code> <p>Returns:</p> Type Description <code>SampleRateAnalysis</code> <p>A SampleRateAnalysis with the following information: Mean sample rate, Std sample rate, Mode sample rate</p> Source code in <code>ceruleo/dataset/analysis/sample_rate.py</code> <pre><code>def sample_rate_summary(\n    ds: AbstractPDMDataset, unit: str = \"s\"\n) -&gt; SampleRateAnalysis:\n    \"\"\"\n    Obtain the mean, median and standard deviation of the sample rate of the dataset\n\n    Parameters:\n        ds: The dataset\n        unit: Unit to convert the time differences\n\n    Returns:\n        A SampleRateAnalysis with the following information: Mean sample rate, Std sample rate, Mode sample rate\n    \"\"\"\n    sr = sample_rate(ds, unit)\n    return SampleRateAnalysis(\n        mean=np.mean(sr),\n        std=np.std(sr),\n        median=np.median(sr),\n        unit=unit\n    )\n</code></pre>"},{"location":"dataset/analysis/sensor_validation/","title":"Sensor validation","text":"<p>Sensor data quality plays a vital role in Internet of Things (IoT) applications as they are rendered useless if the data quality is bad.</p>"},{"location":"dataset/analysis/sensor_validation/#analysis-elements","title":"Analysis elements","text":""},{"location":"dataset/analysis/sensor_validation/#ceruleo.dataset.analysis.numerical_features.entropy","title":"<code>entropy(s)</code>","text":"<p>Approximate entropy</p> <p>The approximate entropy quantifies the amount of regularity and the unpredictability of fluctuations over time-series data.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>ndarray</code> <p>A single feature</p> required <p>Returns:</p> Type Description <code>float</code> <p>Approximate entropy of feature s</p> Source code in <code>ceruleo/dataset/analysis/numerical_features.py</code> <pre><code>def entropy(s: np.ndarray) -&gt; float:\n    \"\"\"\n    Approximate entropy\n\n    The approximate entropy quantifies the amount of regularity and the unpredictability of fluctuations over time-series data.\n\n    Parameters:\n        s: A single feature\n\n    Returns:\n        Approximate entropy of feature s\n    \"\"\"\n    return ant.app_entropy(s)\n</code></pre>"},{"location":"dataset/analysis/sensor_validation/#ceruleo.dataset.analysis.numerical_features.n_unique","title":"<code>n_unique(s)</code>","text":"<p>Number of unique values in the array</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>ndarray</code> <p>A single feature</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of unique values</p> Source code in <code>ceruleo/dataset/analysis/numerical_features.py</code> <pre><code>def n_unique(s: np.ndarray) -&gt; int:\n    \"\"\"\n    Number of unique values in the array\n\n    Parameters:\n        s: A single feature\n\n    Returns:\n        Number of unique values\n    \"\"\"\n    return len(np.unique(s))\n</code></pre>"},{"location":"dataset/analysis/sensor_validation/#ceruleo.dataset.analysis.numerical_features.monotonicity","title":"<code>monotonicity(s)</code>","text":"<p>Monotonicity of a feature, the two extreme values are 0 if the feature is constant and 1 if it is strictly monotonic.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>ndarray</code> <p>A single feature</p> required <p>Returns:</p> Type Description <code>float</code> <p>Monotonicity of the feature</p> Source code in <code>ceruleo/dataset/analysis/numerical_features.py</code> <pre><code>def monotonicity(s: np.ndarray) -&gt; float:\n    \"\"\"\n    Monotonicity of a feature, the two extreme values are 0 if the feature is constant and 1 if it is strictly monotonic.\n\n    Parameters:\n        s: A single feature\n\n    Returns:\n        Monotonicity of the feature\n    \"\"\"\n    N = s.shape[0]\n    diff = np.diff(s)\n    return 1 / (N - 1) * np.abs(np.sum(diff &gt; 0) - np.sum(diff &lt; 0))\n</code></pre>"},{"location":"dataset/analysis/sensor_validation/#ceruleo.dataset.analysis.numerical_features.autocorrelation","title":"<code>autocorrelation(s)</code>","text":"<p>Autocorrelation of a feature</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>ndarray</code> <p>A single feature</p> required <p>Returns:</p> Type Description <code>float</code> <p>Autocorrelation of the feature</p> Source code in <code>ceruleo/dataset/analysis/numerical_features.py</code> <pre><code>def autocorrelation(s: np.ndarray) -&gt; float:\n    \"\"\"\n    Autocorrelation of a feature\n\n    Parameters:\n        s: A single feature\n\n    Returns:\n        Autocorrelation of the feature\n    \"\"\"\n    diff = np.diff(s)\n    return np.sum(diff**2) / s.shape[0]\n</code></pre>"},{"location":"dataset/analysis/sensor_validation/#ceruleo.dataset.analysis.numerical_features.correlation","title":"<code>correlation(s, y=None)</code>","text":"<p>Correlation of the feature with the target</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>ndarray</code> <p>A single feature</p> required <code>y</code> <code>Optional[ndarray]</code> <p>The RUL target</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Correlation between the feature and the RUL target</p> Source code in <code>ceruleo/dataset/analysis/numerical_features.py</code> <pre><code>def correlation(s: np.ndarray, y: Optional[np.ndarray] = None) -&gt; float:\n    \"\"\"\n    Correlation of the feature with the target\n\n    Parameters:\n        s: A single feature\n        y: The RUL target\n\n    Returns:\n        Correlation between the feature and the RUL target\n    \"\"\"\n    N = s.shape[0]\n    if not (s[0] == s).all():\n        corr = spearmanr(s, np.arange(N), nan_policy=\"omit\")\n        corr = corr.correlation\n    else:\n        corr = np.nan\n    return corr\n</code></pre>"},{"location":"dataset/analysis/sensor_validation/#ceruleo.dataset.analysis.numerical_features.mutual_information","title":"<code>mutual_information(x, y)</code>","text":"<p>Mutual information between a feature and the target</p> <p>Reference</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>A single feature</p> required <code>y</code> <code>ndarray</code> <p>RUL Target</p> required <p>Returns:</p> Type Description <code>float</code> <p>Mutual information between x and y</p> Source code in <code>ceruleo/dataset/analysis/numerical_features.py</code> <pre><code>def mutual_information(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Mutual information between a feature and the target\n\n    [Reference](Remaining Useful Life Prediction Using Ranking Mutual Information Based Monotonic Health Indicator)\n\n    Parameters:\n        x: A single feature\n        y: RUL Target\n\n    Returns:\n        Mutual information between x and y\n\n    \"\"\"\n    x = x.reshape(-1, 1)\n    x = np.nan_to_num(x)\n    return mutual_info_regression(x, y)\n</code></pre>"},{"location":"iterators/Iterators/","title":"Notebook: Iterators","text":"In\u00a0[9]: Copied! <pre>from ceruleo.dataset.catalog.PHMDataset2018 import PHMDataset2018, FailureType\n</pre> from ceruleo.dataset.catalog.PHMDataset2018 import PHMDataset2018, FailureType In\u00a0[10]: Copied! <pre>dataset = PHMDataset2018(\n    tools=['01M01', '04M01']\n)\n</pre> dataset = PHMDataset2018(     tools=['01M01', '04M01'] ) In\u00a0[11]: Copied! <pre>from ceruleo.iterators.iterators import RelativeToEnd\nfrom ceruleo.transformation.features.cast import ToDateTime\nfrom ceruleo.transformation.features.resamplers import IndexMeanResampler\nfrom ceruleo.transformation.features.selection import (\n    ByNameFeatureSelector,\n)\nfrom ceruleo.transformation.features.slicing import SliceRows\nfrom ceruleo.transformation.features.transformation import Clip\nfrom ceruleo.transformation.functional.pipeline.pipeline import make_pipeline\nfrom ceruleo.transformation.functional.transformers import Transformer\n</pre> from ceruleo.iterators.iterators import RelativeToEnd from ceruleo.transformation.features.cast import ToDateTime from ceruleo.transformation.features.resamplers import IndexMeanResampler from ceruleo.transformation.features.selection import (     ByNameFeatureSelector, ) from ceruleo.transformation.features.slicing import SliceRows from ceruleo.transformation.features.transformation import Clip from ceruleo.transformation.functional.pipeline.pipeline import make_pipeline from ceruleo.transformation.functional.transformers import Transformer In\u00a0[12]: Copied! <pre>FEATURES = [\n   'IONGAUGEPRESSURE', 'ETCHBEAMVOLTAGE', 'ETCHBEAMCURRENT',\n   'ETCHSUPPRESSORVOLTAGE', 'ETCHSUPPRESSORCURRENT', 'FLOWCOOLFLOWRATE',\n   'FLOWCOOLPRESSURE', 'ETCHGASCHANNEL1READBACK', 'ETCHPBNGASREADBACK',\n]\ntransformer = Transformer(\n    pipelineX=make_pipeline(\n        ToDateTime(index=True),\n        ByNameFeatureSelector(features=FEATURES), \n        Clip(lower=-6, upper=6),\n        IndexMeanResampler(rule='120s'),\n        SliceRows(initial=RelativeToEnd(1500))\n    ), \n    pipelineY=make_pipeline(\n        ToDateTime(index=True),\n        ByNameFeatureSelector(features=['RUL']),  \n        IndexMeanResampler(rule='120s'),\n        SliceRows(initial=RelativeToEnd(1500))\n    )\n)\n\ntransformed_dataset = transformer.fit_map(dataset)\n</pre>    FEATURES = [    'IONGAUGEPRESSURE', 'ETCHBEAMVOLTAGE', 'ETCHBEAMCURRENT',    'ETCHSUPPRESSORVOLTAGE', 'ETCHSUPPRESSORCURRENT', 'FLOWCOOLFLOWRATE',    'FLOWCOOLPRESSURE', 'ETCHGASCHANNEL1READBACK', 'ETCHPBNGASREADBACK', ] transformer = Transformer(     pipelineX=make_pipeline(         ToDateTime(index=True),         ByNameFeatureSelector(features=FEATURES),          Clip(lower=-6, upper=6),         IndexMeanResampler(rule='120s'),         SliceRows(initial=RelativeToEnd(1500))     ),      pipelineY=make_pipeline(         ToDateTime(index=True),         ByNameFeatureSelector(features=['RUL']),           IndexMeanResampler(rule='120s'),         SliceRows(initial=RelativeToEnd(1500))     ) )  transformed_dataset = transformer.fit_map(dataset) In\u00a0[13]: Copied! <pre>from ceruleo.iterators.iterators import WindowedDatasetIterator, IterationType\n</pre> from ceruleo.iterators.iterators import WindowedDatasetIterator, IterationType In\u00a0[14]: Copied! <pre>iterator = WindowedDatasetIterator(\n    transformed_dataset,\n    window_size=150,\n    step=15,\n    horizon=5,\n    iteration_type=IterationType.FORECAST # The default value\n)\n</pre> iterator = WindowedDatasetIterator(     transformed_dataset,     window_size=150,     step=15,     horizon=5,     iteration_type=IterationType.FORECAST # The default value ) In\u00a0[15]: Copied! <pre>X, y, sw = next(iterator)\n(X.shape, y.shape)\n</pre> X, y, sw = next(iterator) (X.shape, y.shape) Out[15]: <pre>((150, 9), (5, 1))</pre> <p>It is possible to obtain all the data following the order of the shuffler in an numpy matrix. By default all the data is flattented</p> In\u00a0[16]: Copied! <pre>X, y, sw = iterator.get_data()\n(X.shape, y.shape, sw.shape)\n</pre> X, y, sw = iterator.get_data() (X.shape, y.shape, sw.shape) Out[16]: <pre>((1678, 1350), (1678, 5), (1678,))</pre> <p>If flatten is False, we can see the shape of the data. X has 1679 samples, of a window size of 150 and 9 features.</p> In\u00a0[17]: Copied! <pre>X, y, sw = iterator.get_data(flatten=False)\n(X.shape, y.shape, sw.shape)\n</pre> X, y, sw = iterator.get_data(flatten=False) (X.shape, y.shape, sw.shape) Out[17]: <pre>((1678, 150, 9), (1678, 5), (1678,))</pre> In\u00a0[18]: Copied! <pre>iterator = WindowedDatasetIterator(\n    transformed_dataset,\n    window_size=150,\n    step=15,\n    iteration_type=IterationType.SEQ_TO_SEQ \n)\n</pre> iterator = WindowedDatasetIterator(     transformed_dataset,     window_size=150,     step=15,     iteration_type=IterationType.SEQ_TO_SEQ  ) In\u00a0[19]: Copied! <pre>X, y, sw = next(iterator)\n(X.shape, y.shape)\n</pre> X, y, sw = next(iterator) (X.shape, y.shape) Out[19]: <pre>((150, 9), (150, 1))</pre> In\u00a0[20]: Copied! <pre>from ceruleo.iterators.batcher import Batcher\n</pre> from ceruleo.iterators.batcher import Batcher In\u00a0[21]: Copied! <pre>batcher = Batcher.new(\n    transformed_dataset,\n    batch_size=64,\n    window=150,\n    step=15,\n    horizon=5\n)\nX, y, sw = next(batcher)\n(X.shape, y.shape, sw.shape)\n</pre> batcher = Batcher.new(     transformed_dataset,     batch_size=64,     window=150,     step=15,     horizon=5 ) X, y, sw = next(batcher) (X.shape, y.shape, sw.shape) Out[21]: <pre>((64, 150, 9), (64, 5, 1), (64, 1))</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"iterators/Iterators/#notebook-iterators","title":"Notebook: Iterators\u00b6","text":""},{"location":"iterators/Iterators/#load-dataset","title":"Load dataset\u00b6","text":""},{"location":"iterators/Iterators/#create-a-transformer-for-a-dataset","title":"Create a transformer for a dataset\u00b6","text":""},{"location":"iterators/Iterators/#iterator","title":"Iterator\u00b6","text":""},{"location":"iterators/Iterators/#forecast-iterator","title":"Forecast iterator\u00b6","text":"<p>The forecast iterator produces as target the values of the Y transformers that start where the X data ends.</p>"},{"location":"iterators/Iterators/#seq-to-seq-iterator","title":"Seq to Seq Iterator\u00b6","text":"<p>The seq to seq iterator will return as a target a window of a same size as the input aligned with it</p>"},{"location":"iterators/Iterators/#batcher","title":"Batcher\u00b6","text":""},{"location":"iterators/batcher/","title":"Batcher","text":""},{"location":"iterators/batcher/#batcher","title":"Batcher","text":"<p>Batching capabilities</p> <p>Many machine learning models require data to be provided in the form of mini batches. This module interacts with iterators to generate batches. In particular, this module was created to be used with pytorch models.</p> <p>Take a look at the pytorch example to see its usage.</p>"},{"location":"iterators/batcher/#ceruleo.iterators.batcher.Batcher","title":"<code>Batcher</code>","text":"<p>WindowedIterator Batcher   </p> <p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>WindowedDatasetIterator</code> <p>Dataset iterator</p> required <code>batch_size</code> <code>int</code> <p>int</p> required Source code in <code>ceruleo/iterators/batcher.py</code> <pre><code>class Batcher:\n    \"\"\"\n    WindowedIterator Batcher   \n\n    Parameters:\n        iterator: Dataset iterator\n        batch_size: int\n\n    \"\"\"\n\n    def __init__(\n        self,\n        iterator: WindowedDatasetIterator,\n        batch_size: int,\n    ):\n        self.iterator = iterator\n        self.batch_size = batch_size\n        self.stop = False\n        self.batch_data = None\n\n    @staticmethod\n    def new(\n        dataset: AbstractPDMDataset,\n        window: int,\n        batch_size: int,\n        step: int,\n        horizon: int = 1,\n        shuffler: AbstractShuffler = NotShuffled(),\n        sample_weight: SampleWeight = NotWeighted(),\n        right_closed: bool = True,\n        padding: bool = False,\n    ) -&gt; \"Batcher\":\n        \"\"\"\n        Batcher constructor from a dataset\n\n        The method constructs a WindowedDatasetIterator from the dataset and\n        then a Batcher from the iterator.\n        Most of the parameters come from the WindowedDatasetIterator,\n\n\n        Example:\n            ```\n            batcher = Batcher.new(transformed_dataset,\n                                window=150,\n                                batch_size=64,\n                                step=1,\n                                horizon=1)\n            X, y, data = next(batcher)     \n            X.shape\n\n            (64, 150, n_features)       \n            ```                         \n\n        Parameters:\n            dataset: Dataset from which the batcher will be created\n            batch_size: Batch size\n            step: strides\n            horizon: Size of the horizon to predict.\n            shuffle: AbstractShuffler\n            sample_weight: SampleWeight\n            right_closed: bool\n            padding: wheter to pad data if there are not enough points to fill the window\n\n        Returns:\n            A new constructed batcher\n        \"\"\"\n        iterator = WindowedDatasetIterator(\n            dataset,\n            window,\n            step=step,\n            horizon=horizon,\n            shuffler=shuffler,\n            sample_weight=sample_weight,\n            right_closed=right_closed,\n            padding=padding,\n        )\n        b = Batcher(iterator, batch_size)\n        return b\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Number of batches\n\n        Returns:\n            Number of batches in the iterator\n        \"\"\"\n        if len(self.iterator) is None:\n            return None\n        q = math.ceil(len(self.iterator) / self.batch_size)\n        return q\n\n    def __iter__(self):\n        self.stop = False\n        self.iterator.__iter__()\n        return self\n\n    @property\n    def n_features(self) -&gt; int:\n        \"\"\"\n        Number of features of the transformed dataset\n\n        This is a helper method to obtain the transformed dataset information from the WindowedDatasetIterator\n\n        Returns:\n           Number of features of the transformed dataset\n        \"\"\"\n        return self.iterator.n_features\n\n    @property\n    def window_size(self) -&gt; int:\n        \"\"\"\n        Lookback window size\n\n        This is a helper method to obtain the WindowedDatasetIterator information\n\n        Returns:\n            Lookback window size\n        \"\"\"\n        return self.iterator.window_size\n\n    @property\n    def output_shape(self) -&gt; int:\n        \"\"\"\n        Number of values returned as target by each sample\n\n        Returns:\n            Number of values returned as target by each sample\n        \"\"\"\n        return self.iterator.output_size\n\n    @property\n    def input_shape(self) -&gt; Tuple[int, int]:\n        \"\"\"\n        Tuple containing (window_size, n_features)\n\n        Returns:\n            (window_size, n_features)\n        \"\"\"\n        return self.iterator.input_shape\n\n    @property\n    def computed_step(self):\n        if isinstance(self.step, int):\n            return self.step\n        elif isinstance(self.step, tuple):\n            if self.step[0] == \"auto\":\n                return int(self.window / self.step[1])\n        raise ValueError(\"Invalid step parameter\")\n\n    def initialize_batch(self):\n        def initialize_batch_element(elem):\n            if isinstance(elem, tuple):\n                for e in elem:\n                    initialize_batch_element(e)\n            else:\n                elem.fill(0)\n\n        if self.batch_data is None:\n            return\n        for i in range(len(self.batch_data)):\n            initialize_batch_element(self.batch_data[i])\n\n    def allocate_batch_data(self, d):\n        def allocate_batch_data_element(d):\n            if isinstance(d, tuple):\n                return tuple(allocate_batch_data_element(q) for q in d)\n            else:\n\n                if isinstance(d, np.ndarray) or isinstance(d, pd.Series):\n                    shape = d.shape\n                elif isinstance(d, list):\n                    shape = (len(d),)\n                return np.zeros((self.batch_size, *shape))\n\n        if self.batch_data is not None:\n            return\n        self.batch_data = []\n        for i in range(len(d)):\n            self.batch_data.append(allocate_batch_data_element(d[i]))\n\n    def _assign_data(self, d, j):\n        for i, elem in enumerate(d):\n            if isinstance(elem, tuple):\n                for k in range(len(elem)):\n                    self.batch_data[i][k][j, :] = elem[k]\n            else:\n                self.batch_data[i][j, :] = elem\n\n    def _slice_data(self, actual_batch_size):\n        def slice_batch_data_element(d, actual_batch_size):\n            if isinstance(d, tuple):\n                return tuple(slice_batch_data_element(q, actual_batch_size) for q in d)\n            else:\n                return d[: actual_batch_size - 1, :]\n\n        if actual_batch_size == self.batch_size:\n            return self.batch_data\n        sliced_data = []\n        for i in range(len(self.batch_data)):\n            sliced_data.append(\n                slice_batch_data_element(self.batch_data[i], actual_batch_size)\n            )\n        return sliced_data\n\n    def __next__(self):\n        if self.stop:\n            raise StopIteration\n        try:\n            actual_batch_size = 0\n            for j in range(self.batch_size):\n                actual_batch_size += 1\n                d = next(self.iterator)\n                self.allocate_batch_data(d)\n                self._assign_data(d, j)\n        except StopIteration:\n            self.stop = True\n\n        return self._slice_data(actual_batch_size)\n</code></pre>"},{"location":"iterators/batcher/#ceruleo.iterators.batcher.Batcher.input_shape","title":"<code>input_shape: Tuple[int, int]</code>  <code>property</code>","text":"<p>Tuple containing (window_size, n_features)</p> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>(window_size, n_features)</p>"},{"location":"iterators/batcher/#ceruleo.iterators.batcher.Batcher.n_features","title":"<code>n_features: int</code>  <code>property</code>","text":"<p>Number of features of the transformed dataset</p> <p>This is a helper method to obtain the transformed dataset information from the WindowedDatasetIterator</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of features of the transformed dataset</p>"},{"location":"iterators/batcher/#ceruleo.iterators.batcher.Batcher.output_shape","title":"<code>output_shape: int</code>  <code>property</code>","text":"<p>Number of values returned as target by each sample</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of values returned as target by each sample</p>"},{"location":"iterators/batcher/#ceruleo.iterators.batcher.Batcher.window_size","title":"<code>window_size: int</code>  <code>property</code>","text":"<p>Lookback window size</p> <p>This is a helper method to obtain the WindowedDatasetIterator information</p> <p>Returns:</p> Type Description <code>int</code> <p>Lookback window size</p>"},{"location":"iterators/batcher/#ceruleo.iterators.batcher.Batcher.__len__","title":"<code>__len__()</code>","text":"<p>Number of batches</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of batches in the iterator</p> Source code in <code>ceruleo/iterators/batcher.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Number of batches\n\n    Returns:\n        Number of batches in the iterator\n    \"\"\"\n    if len(self.iterator) is None:\n        return None\n    q = math.ceil(len(self.iterator) / self.batch_size)\n    return q\n</code></pre>"},{"location":"iterators/batcher/#ceruleo.iterators.batcher.Batcher.new","title":"<code>new(dataset, window, batch_size, step, horizon=1, shuffler=NotShuffled(), sample_weight=NotWeighted(), right_closed=True, padding=False)</code>  <code>staticmethod</code>","text":"<p>Batcher constructor from a dataset</p> <p>The method constructs a WindowedDatasetIterator from the dataset and then a Batcher from the iterator. Most of the parameters come from the WindowedDatasetIterator,</p> Example <pre><code>batcher = Batcher.new(transformed_dataset,\n                    window=150,\n                    batch_size=64,\n                    step=1,\n                    horizon=1)\nX, y, data = next(batcher)     \nX.shape\n\n(64, 150, n_features)       \n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>AbstractPDMDataset</code> <p>Dataset from which the batcher will be created</p> required <code>batch_size</code> <code>int</code> <p>Batch size</p> required <code>step</code> <code>int</code> <p>strides</p> required <code>horizon</code> <code>int</code> <p>Size of the horizon to predict.</p> <code>1</code> <code>shuffle</code> <p>AbstractShuffler</p> required <code>sample_weight</code> <code>SampleWeight</code> <p>SampleWeight</p> <code>NotWeighted()</code> <code>right_closed</code> <code>bool</code> <p>bool</p> <code>True</code> <code>padding</code> <code>bool</code> <p>wheter to pad data if there are not enough points to fill the window</p> <code>False</code> <p>Returns:</p> Type Description <code>Batcher</code> <p>A new constructed batcher</p> Source code in <code>ceruleo/iterators/batcher.py</code> <pre><code>@staticmethod\ndef new(\n    dataset: AbstractPDMDataset,\n    window: int,\n    batch_size: int,\n    step: int,\n    horizon: int = 1,\n    shuffler: AbstractShuffler = NotShuffled(),\n    sample_weight: SampleWeight = NotWeighted(),\n    right_closed: bool = True,\n    padding: bool = False,\n) -&gt; \"Batcher\":\n    \"\"\"\n    Batcher constructor from a dataset\n\n    The method constructs a WindowedDatasetIterator from the dataset and\n    then a Batcher from the iterator.\n    Most of the parameters come from the WindowedDatasetIterator,\n\n\n    Example:\n        ```\n        batcher = Batcher.new(transformed_dataset,\n                            window=150,\n                            batch_size=64,\n                            step=1,\n                            horizon=1)\n        X, y, data = next(batcher)     \n        X.shape\n\n        (64, 150, n_features)       \n        ```                         \n\n    Parameters:\n        dataset: Dataset from which the batcher will be created\n        batch_size: Batch size\n        step: strides\n        horizon: Size of the horizon to predict.\n        shuffle: AbstractShuffler\n        sample_weight: SampleWeight\n        right_closed: bool\n        padding: wheter to pad data if there are not enough points to fill the window\n\n    Returns:\n        A new constructed batcher\n    \"\"\"\n    iterator = WindowedDatasetIterator(\n        dataset,\n        window,\n        step=step,\n        horizon=horizon,\n        shuffler=shuffler,\n        sample_weight=sample_weight,\n        right_closed=right_closed,\n        padding=padding,\n    )\n    b = Batcher(iterator, batch_size)\n    return b\n</code></pre>"},{"location":"iterators/iterators/","title":"Iterators","text":"<p>Windowed iteration capabilites</p> <p>Usually time series data is divided in a contigous periods of a determined window size, with some arbitray overlapping. How the time series is iterated constitutes a very important aspect when building a PdM model.</p> <p>This module provides utilities to iterate the dataset</p>"},{"location":"iterators/iterators/#ceruleo.iterators.iterators.WindowedDatasetIterator","title":"<code>WindowedDatasetIterator</code>","text":"<p>Iterate a dataset using windows</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>TransformedDataset</code> <p>The transformed dataset</p> required <code>window_size</code> <code>int</code> <p>Size of the lookback window</p> required <code>step</code> <code>int</code> <p>Separation between two consecutive size. If step == window_size there is no overlapping between two consecutive windows</p> <code>1</code> <code>horizon</code> <code>int</code> <p>Horizon to be predicted. If this value is 3, for each window, 3 elements of the target are expected to be predicted</p> <code>1</code> <code>shuffler</code> <code>AbstractShuffler</code> <p>How the data should be shuffled</p> <code>NotShuffled()</code> <code>sample_weight</code> <code>SampleWeight</code> <p>Which are the sample weight for each sample</p> <code>NotWeighted()</code> <code>right_closed</code> <code>bool</code> <p>Weather the last point of the window should be included or not</p> <code>True</code> <code>padding</code> <code>bool</code> <p>Wether to pad elements if the samples are not enough to fill the window. Usually this happens at the beginning of the window</p> <code>False</code> <code>iteration_type</code> <code>IterationType</code> <p>Specify if the underlying model it's a forecasting model in which a scalar is predicted, or a sequence to sequence model similar to an autoencoder</p> <code>FORECAST</code> <code>start_index</code> <code>Union[int, RelativePosition]</code> <p>Initial index of each run-tu-failure cycle</p> <code>0</code> <code>end_index</code> <code>Optional[Union[int, RelativePosition]]</code> <p>Final index of each run-to-failure cycle</p> <code>None</code> <code>valid_sample</code> <code>Callable[[int, int, int, int, int], bool]</code> <p>A callable that returns weather a sample is valid or not</p> <code>valid_sample</code> <code>last_point</code> <code>bool</code> <p>Weather to add the last point</p> <code>True</code> Source code in <code>ceruleo/iterators/iterators.py</code> <pre><code>class WindowedDatasetIterator:\n    \"\"\"\n    Iterate a dataset using windows\n\n    Parameters:\n        dataset: The transformed dataset\n        window_size: Size of the lookback window\n        step: Separation between two consecutive size. If step == window_size there is no overlapping between two consecutive windows\n        horizon: Horizon to be predicted. If this value is 3, for each window, 3 elements of the target are expected to be predicted\n        shuffler: How the data should be shuffled\n        sample_weight: Which are the sample weight for each sample\n        right_closed: Weather the last point of the window should be included or not\n        padding: Wether to pad elements if the samples are not enough to fill the window. Usually this happens at the beginning of the window\n        iteration_type: Specify if the underlying model it's a forecasting model in which a scalar is predicted, or a sequence to sequence model similar to an autoencoder\n        start_index: Initial index of each run-tu-failure cycle\n        end_index: Final index of each run-to-failure cycle\n        valid_sample: A callable that returns weather a sample is valid or not\n        last_point: Weather to add the last point\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: TransformedDataset,\n        window_size: int,\n        step: int = 1,\n        horizon: int = 1,\n        shuffler: AbstractShuffler = NotShuffled(),\n        sample_weight: SampleWeight = NotWeighted(),\n        right_closed: bool = True,\n        padding: bool = False,\n        iteration_type: IterationType = IterationType.FORECAST,\n        start_index: Union[int, RelativePosition] = 0,\n        end_index: Optional[Union[int, RelativePosition]] = None,\n        valid_sample: Callable[[int, int, int, int, int], bool] = valid_sample,\n        last_point: bool = True,\n    ):\n        self.last_point = last_point\n        if isinstance(start_index, int):\n            start_index = RelativeToStart(start_index)\n        self.start_index = start_index\n        if end_index is None:\n            end_index = RelativeToEnd(0)\n        elif isinstance(end_index, int):\n            end_index = RelativeToStart(end_index)\n        self.end_index = end_index\n        self.dataset = dataset\n        self.shuffler = shuffler\n        self.window_size = window_size\n        self.step = step\n        self.shuffler.initialize(self)\n        self.iteration_type = iteration_type\n\n        if self.iteration_type == IterationType.FORECAST:\n            self.slicing_function = windowed_signal_generator\n        else:\n            self.slicing_function = seq_to_seq_signal_generator\n\n        if not isinstance(sample_weight, AbstractSampleWeights) or not callable(\n            sample_weight\n        ):\n            raise ValueError(\n                \"sample_weight should be an AbstractSampleWeights or a callable\"\n            )\n\n        self.sample_weight = sample_weight\n\n        self.i = 0\n        self.horizon = horizon\n        self.right_closed = right_closed\n        self.length = None\n        self.padding = padding\n        self.valid_sample = functools.partial(\n            valid_sample, self.padding, self.window_size\n        )\n\n    def __len__(self):\n        \"\"\"\n        Return the length of the iterator\n\n        If it was not iterated once, it will compute the length by iterating from the entire dataset\n        \"\"\"\n        if self.length is None:\n            self.length = sum(1 for _ in self)\n            self.__iter__()\n        return self.length\n\n    def __iter__(self):\n        self.i = 0\n        self.shuffler.start(self)\n        return self\n\n    def __next__(self):\n        life, timestamp = self.shuffler.next_element()\n        X, y, metadata = self.dataset[life]\n        is_df = isinstance(y, pd.DataFrame)\n        if is_df:\n            valid = self.valid_sample(timestamp, y.iloc[timestamp])\n        else:\n            valid = self.valid_sample(timestamp, y[timestamp])\n        while not valid:\n            life, timestamp = self.shuffler.next_element()\n            X, y, metadata = self.dataset[life]\n            if is_df:\n                valid = self.valid_sample(timestamp, y.iloc[timestamp])\n            else:\n                valid = self.valid_sample(timestamp, y[timestamp])\n\n        curr_X, curr_y = self.slicing_function(\n            X, y, timestamp, self.window_size, self.horizon, self.right_closed\n        )\n        return curr_X, curr_y, [self.sample_weight(y, timestamp, metadata)]\n\n    def get_data(\n        self, flatten: bool = True, show_progress: bool = False\n    ) -&gt; Tuple[np.ndarray, np.array, np.array]:\n        \"\"\"\n        Obtain data, target and sample weights as numpy arrays.\n\n        Parameters:\n            flatten: Wether to flatten data\n            show_progress: Wether to show progress\n\n        Returns:\n            Data, target and sample weights\n        \"\"\"\n        N_points = len(self)\n\n        if flatten:\n            dimension = self.window_size * self.n_features\n            X = np.zeros((N_points, dimension), dtype=np.float32)\n        else:\n            X = np.zeros(\n                (N_points, self.window_size, self.n_features), dtype=np.float32\n            )\n        if self.iteration_type == IterationType.FORECAST:\n            y = np.zeros((N_points, self.horizon), dtype=np.float32)\n        else:\n            y = np.zeros((N_points, self.window_size, self.horizon), dtype=np.float32)\n        sample_weight = np.zeros(N_points, dtype=np.float32)\n\n        iterator = enumerate(self)\n        if show_progress:\n            iterator = tqdm(iterator, total=len(self))\n\n        for i, (X_, y_, sample_weight_) in iterator:\n            if flatten:\n                X[i, :] = X_.flatten()\n            else:\n                X[i, :, :] = X_\n            if self.iteration_type == IterationType.FORECAST:\n                y[i, :] = y_.flatten()\n            else:\n                y[i, :, :] = y_\n            sample_weight[i] = sample_weight_[0]\n        return X, y, sample_weight\n\n    @property\n    def n_features(self) -&gt; int:\n        \"\"\"Number of features of the transformed dataset. This is a helper method to obtain the transformed dataset information from the WindowedDatasetIterator\n\n        Returns:\n            Number of features of the transformed dataset\n        \"\"\"\n        return self.dataset.transformer.n_features\n\n    @property\n    def shape(self) -&gt; Tuple[int, int]:\n        \"\"\"\n        Tuple containing (window_size, n_features)\n\n        Returns:\n            Tuple containing (window_size, n_features)\n        \"\"\"\n        return (self.window_size, self.n_features)\n</code></pre>"},{"location":"iterators/iterators/#ceruleo.iterators.iterators.WindowedDatasetIterator.n_features","title":"<code>n_features: int</code>  <code>property</code>","text":"<p>Number of features of the transformed dataset. This is a helper method to obtain the transformed dataset information from the WindowedDatasetIterator</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of features of the transformed dataset</p>"},{"location":"iterators/iterators/#ceruleo.iterators.iterators.WindowedDatasetIterator.shape","title":"<code>shape: Tuple[int, int]</code>  <code>property</code>","text":"<p>Tuple containing (window_size, n_features)</p> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Tuple containing (window_size, n_features)</p>"},{"location":"iterators/iterators/#ceruleo.iterators.iterators.WindowedDatasetIterator.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of the iterator</p> <p>If it was not iterated once, it will compute the length by iterating from the entire dataset</p> Source code in <code>ceruleo/iterators/iterators.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Return the length of the iterator\n\n    If it was not iterated once, it will compute the length by iterating from the entire dataset\n    \"\"\"\n    if self.length is None:\n        self.length = sum(1 for _ in self)\n        self.__iter__()\n    return self.length\n</code></pre>"},{"location":"iterators/iterators/#ceruleo.iterators.iterators.WindowedDatasetIterator.get_data","title":"<code>get_data(flatten=True, show_progress=False)</code>","text":"<p>Obtain data, target and sample weights as numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>flatten</code> <code>bool</code> <p>Wether to flatten data</p> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Wether to show progress</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, array, array]</code> <p>Data, target and sample weights</p> Source code in <code>ceruleo/iterators/iterators.py</code> <pre><code>def get_data(\n    self, flatten: bool = True, show_progress: bool = False\n) -&gt; Tuple[np.ndarray, np.array, np.array]:\n    \"\"\"\n    Obtain data, target and sample weights as numpy arrays.\n\n    Parameters:\n        flatten: Wether to flatten data\n        show_progress: Wether to show progress\n\n    Returns:\n        Data, target and sample weights\n    \"\"\"\n    N_points = len(self)\n\n    if flatten:\n        dimension = self.window_size * self.n_features\n        X = np.zeros((N_points, dimension), dtype=np.float32)\n    else:\n        X = np.zeros(\n            (N_points, self.window_size, self.n_features), dtype=np.float32\n        )\n    if self.iteration_type == IterationType.FORECAST:\n        y = np.zeros((N_points, self.horizon), dtype=np.float32)\n    else:\n        y = np.zeros((N_points, self.window_size, self.horizon), dtype=np.float32)\n    sample_weight = np.zeros(N_points, dtype=np.float32)\n\n    iterator = enumerate(self)\n    if show_progress:\n        iterator = tqdm(iterator, total=len(self))\n\n    for i, (X_, y_, sample_weight_) in iterator:\n        if flatten:\n            X[i, :] = X_.flatten()\n        else:\n            X[i, :, :] = X_\n        if self.iteration_type == IterationType.FORECAST:\n            y[i, :] = y_.flatten()\n        else:\n            y[i, :, :] = y_\n        sample_weight[i] = sample_weight_[0]\n    return X, y, sample_weight\n</code></pre>"},{"location":"iterators/iterators/#iteration-type","title":"Iteration Type","text":"<p>               Bases: <code>Enum</code></p> <p>Iteration type</p> <p>Possible values are:</p> <ul> <li>SEQ_TO_SEQ = 1: The seq to seq iterator will return as a target a window of a same size as the input aligned with it</li> <li>FORECAST = 2: The forecast iterator produces as target the values of the Y transformers     that start where the X data ends.</li> </ul> Source code in <code>ceruleo/iterators/iterators.py</code> <pre><code>class IterationType(Enum):\n    \"\"\"\n    Iteration type\n\n    Possible values are:\n\n    - SEQ_TO_SEQ = 1: The seq to seq iterator will return as a target a window of a same size as the input aligned with it\n    - FORECAST = 2: The forecast iterator produces as target the values of the Y transformers\n        that start where the X data ends.\n    \"\"\"\n\n    SEQ_TO_SEQ = 1\n    FORECAST = 2\n</code></pre>"},{"location":"iterators/iterators/#sample-weights","title":"Sample weights","text":"<p>The Sample Weight type is a callable with the following signature</p> <pre><code>fun(y, i:int, metadata)\n</code></pre> <p>Given the target and the sample index <code>i</code> it returns the sample weight for sample <code>i</code>. There area few callable classes already made with standard sample weight schemes in PdM.</p>"},{"location":"iterators/iterators/#ceruleo.iterators.sample_weight.SampleWeight","title":"<code>ceruleo.iterators.sample_weight.SampleWeight = Union[AbstractSampleWeights, Callable[[np.ndarray, int, Any], float]]</code>  <code>module-attribute</code>","text":""},{"location":"iterators/iterators/#ceruleo.iterators.sample_weight.AbstractSampleWeights","title":"<code>ceruleo.iterators.sample_weight.AbstractSampleWeights</code>","text":"<p>The base class for the sample weight provider</p> Source code in <code>ceruleo/iterators/sample_weight.py</code> <pre><code>class AbstractSampleWeights:\n    \"\"\"\n    The base class for the sample weight provider\n    \"\"\"\n\n    def __call__(self, y: Union[np.ndarray, pd.DataFrame], i: int, metadata):\n        raise NotImplementedError\n</code></pre>"},{"location":"iterators/iterators/#ceruleo.iterators.sample_weight.NotWeighted","title":"<code>ceruleo.iterators.sample_weight.NotWeighted</code>","text":"<p>               Bases: <code>AbstractSampleWeights</code></p> <p>Simplest sample weight provider</p> <p>Provide 1 as a sample weight for every sample</p> Source code in <code>ceruleo/iterators/sample_weight.py</code> <pre><code>class NotWeighted(AbstractSampleWeights):\n    \"\"\"\n    Simplest sample weight provider\n\n    Provide 1 as a sample weight for every sample\n    \"\"\"\n\n    def __call__(self, y: Union[np.ndarray, pd.DataFrame], i: int, metadata):\n        return 1\n</code></pre>"},{"location":"iterators/iterators/#ceruleo.iterators.sample_weight.RULInverseWeighted","title":"<code>ceruleo.iterators.sample_weight.RULInverseWeighted</code>","text":"<p>               Bases: <code>AbstractSampleWeights</code></p> <p>Weight each sample by the inverse of the RUL</p> Source code in <code>ceruleo/iterators/sample_weight.py</code> <pre><code>class RULInverseWeighted(AbstractSampleWeights):\n    \"\"\"\n    Weight each sample by the inverse of the RUL\n    \"\"\"\n\n    def __call__(self, y : Union[np.ndarray, pd.DataFrame], i: int, metadata):\n        return 1 / (get_value(y, i) + 1)\n</code></pre>"},{"location":"iterators/iterators/#ceruleo.iterators.sample_weight.InverseToLengthWeighted","title":"<code>ceruleo.iterators.sample_weight.InverseToLengthWeighted</code>","text":"<p>               Bases: <code>AbstractSampleWeights</code></p> <p>Weights samples according to the duration of the run-to-failure cycle they belong to.</p> <p>All points in the run-to-cycle are weighted equally inverse to the cycle duration</p> Source code in <code>ceruleo/iterators/sample_weight.py</code> <pre><code>class InverseToLengthWeighted(AbstractSampleWeights):\n    \"\"\"\n    Weights samples according to the duration of the run-to-failure cycle they belong to.\n\n    All points in the run-to-cycle are weighted equally inverse to the cycle duration\n\n    \"\"\"\n\n    def __call__(self, y:Union[np.ndarray, pd.DataFrame], i: int, metadata):\n        return 1 / get_value(y, 0)\n</code></pre>"},{"location":"iterators/iterators/#relative-positioning","title":"Relative positioning","text":"<p>Sometimes is useful to iterate the run-to-failure cycle starting or ending at some specifig indices. These classes allow to specify relative positions to start or end the iteration.</p>"},{"location":"iterators/iterators/#ceruleo.iterators.iterators.RelativePosition","title":"<code>ceruleo.iterators.iterators.RelativePosition</code>","text":"<p>Relative position selector base class</p> <p>The relative position selectors allow specifying the iteration starts and end relative to the beginning or the end of the run-to-cycle failure</p> Source code in <code>ceruleo/iterators/iterators.py</code> <pre><code>class RelativePosition:\n    \"\"\"\n    Relative position selector base class\n\n    The relative position selectors allow specifying the iteration starts and end relative to the beginning or the end of the run-to-cycle failure\n    \"\"\"\n\n    def __init__(self, i: int):\n        self.i = i\n\n    @abstractmethod\n    def get(self, time_series_length: int):\n        raise NotImplementedError\n</code></pre>"},{"location":"iterators/iterators/#ceruleo.iterators.iterators.RelativeToStart","title":"<code>ceruleo.iterators.iterators.RelativeToStart</code>","text":"<p>               Bases: <code>RelativePosition</code></p> <p>Specify positions relative to the start of the run-to-failure cycle</p> Example <p>An iterator that iterate each run-to-failure cycle skipping the first 200 samples of each cycle. <pre><code>iterator = WindowedDatasetIterator(\n        transformed_ds,\n        window_size=3,\n        step=1,\n        start_index=RelativeToStart(25),\n        horizon=1)\n</code></pre></p> Source code in <code>ceruleo/iterators/iterators.py</code> <pre><code>class RelativeToStart(RelativePosition):\n    \"\"\"\n    Specify positions relative to the start of the run-to-failure cycle\n\n    Example:\n        An iterator that iterate each run-to-failure cycle skipping the first 200 samples of each cycle.\n        ```\n        iterator = WindowedDatasetIterator(\n                transformed_ds,\n                window_size=3,\n                step=1,\n                start_index=RelativeToStart(25),\n                horizon=1)\n        ```\n    \"\"\"\n\n    def get(self, time_series_length: int):\n        return self.i\n</code></pre>"},{"location":"iterators/iterators/#ceruleo.iterators.iterators.RelativeToEnd","title":"<code>ceruleo.iterators.iterators.RelativeToEnd</code>","text":"<p>               Bases: <code>RelativePosition</code></p> <p>Specify positions relative to the end of the run-to-failure cycle</p> Example <p>An iterator that iterates each run-to-failure cycle starting in the last 500 samples of each cycle. <pre><code>iterator = WindowedDatasetIterator(\n        transformed_ds,\n        window_size=3,\n        step=1,\n        start_index=RelativeToEnd(500),\n        horizon=1)\n</code></pre></p> Source code in <code>ceruleo/iterators/iterators.py</code> <pre><code>class RelativeToEnd(RelativePosition):\n    \"\"\"\n    Specify positions relative to the end of the run-to-failure cycle\n\n    Example:\n        An iterator that iterates each run-to-failure cycle starting in the last 500 samples of each cycle.\n        ```\n        iterator = WindowedDatasetIterator(\n                transformed_ds,\n                window_size=3,\n                step=1,\n                start_index=RelativeToEnd(500),\n                horizon=1)\n        ```\n    \"\"\"\n\n    def get(self, time_series_length: int):\n        return max(time_series_length - self.i, 0)\n</code></pre>"},{"location":"iterators/shufflers/","title":"Shufflers","text":""},{"location":"iterators/shufflers/#shufflers","title":"Shufflers","text":"<p>Shufflers are helpers classes used by the iterator to change the order of the run-to-failure cycles and the timestamps inside each cycle</p> <p>There are six types of shuffles</p> <ul> <li>NotShuffled: All the cycles are processed in order</li> <li>AllShuffled: Everything is shuffled</li> <li>IntraTimeSeriesShuffler: Each point of the time series is shuffled, but the TS are kept in order</li> <li>InverseOrder: The data points will be fed in RUL decreasing order</li> <li>TimeSeriesOrderIntraSignalShuffling: Each point in the ts is shuffled, and the ts order are shuffled also.</li> <li>TimeSeriesOrderShuffling: Time series are shuffled, but each point inside the time series kept  its order</li> </ul>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler","title":"<code>AbstractShuffler</code>","text":"<p>A Shuffler is used by the iterator to interleave samples of different run-to-fail cycles</p> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>class AbstractShuffler:\n    \"\"\"\n    A Shuffler is used by the iterator to interleave samples of different run-to-fail cycles\n    \"\"\"\n\n    class Iterator:\n        def __init__(self, shuffler, iterator):\n            self.iterator = iterator\n            self.shuffler = shuffler\n\n        def __iter__(self):\n            self.shuffler.initialize(self.iterator)\n            return self\n\n        def __next__(self):\n            return self.shuffler.next_element()\n\n    def iterator(self, iterator: \"WindowedDatasetIterator\"):\n        return AbstractShuffler.Iterator(self, iterator)\n\n    def at_end(self) -&gt; bool:\n        \"\"\"\n        Determines weather the iterator reached to its end\n\n        Returns:\n            bool: Wether the iterator is at its end\n        \"\"\"\n        return self.current_time_series == self.wditerator.dataset.n_time_series\n\n    def next_element(self) -&gt; Tuple[int, int]:\n        \"\"\"\n        Iterating function\n\n        The method takes the current time serie, and the current time stamp\n        and calls the advance method\n\n        Returns:\n            Index of the current run-to-failure cycle andcurrent timestamp of that cycle\n\n        Raises:\n           When the iteration reaches the end\n        \"\"\"\n\n        if self.at_end():\n            raise StopIteration\n        ts_index = self.time_series()\n        timestamp = self.timestamp()\n        self.advance()\n\n        return ts_index, timestamp\n\n    def start(self, iterator: \"WindowedDatasetIterator\"):\n        \"\"\"\n        Start the shuffler given an iterator\n\n        Parameters:\n            iterator: An Windowed Iterator\n        \"\"\"\n        self.initialize(iterator)\n\n    def time_series(self) -&gt; int:\n        \"\"\"\n        Current time series\n\n        Returns:\n            Current Time Series\n        \"\"\"\n        return self.current_time_series\n\n    def initialize(self, iterator: \"WindowedDatasetIterator\"):\n        \"\"\"\n        Initialize the current shuffler\n\n        This method is in charge of initializing everything to allow the correct iteration\n\n        Parameters:\n            iterator: WindowedDatasetIterator\n\n        \"\"\"\n        self.wditerator = iterator\n        self._samples_per_time_series = (\n            np.ones(self.wditerator.dataset.n_time_series, dtype=np.int64)\n            * np.iinfo(np.int64).max\n        )\n        self._time_series_sizes = (\n            np.ones(self.wditerator.dataset.n_time_series, dtype=np.int64)\n            * np.iinfo(np.int64).max\n        )\n        self.current_time_series = 0\n\n    def load_time_series(self, time_series_index: int):\n        total_length = self.wditerator.dataset.number_of_samples_of_time_series(\n            time_series_index\n        )\n        start_index = self.wditerator.start_index.get(total_length)\n        end_index = self.wditerator.end_index.get(total_length)\n\n        N = end_index - start_index\n        samples_per_step = N / self.wditerator.step\n        self._samples_per_time_series[time_series_index] = math.ceil(samples_per_step)\n        last_part_missing = (\n            math.ceil((N - 1) / self.wditerator.step) - ((N - 1) / self.wditerator.step)\n            &gt; 0\n        )\n        if self.wditerator.last_point and last_part_missing:\n            self._samples_per_time_series[time_series_index] += 1\n\n        self._time_series_sizes[time_series_index] = total_length\n\n    def number_samples_of_time_series(self, time_series_index: int) -&gt; int:\n        if self._samples_per_time_series[time_series_index] == np.iinfo(np.int64).max:\n            self.load_time_series(time_series_index)\n        return self._samples_per_time_series[time_series_index]\n\n    def number_of_samples_of_current_time_series(self) -&gt; int:\n        \"\"\"\n        Obtain the number of samples for the current time series\n\n        Returns:\n            Number of samples\n\n        \"\"\"\n\n        return self.number_samples_of_time_series(self.current_time_series)\n\n    def timestamp(self) -&gt; int:\n        \"\"\"Obtain a timestamp for the current run-to-failure cycle\"\"\"\n        raise NotImplementedError\n\n    def time_series_size(self, time_series_index: int):\n        if self._time_series_sizes[time_series_index] == np.iinfo(np.int64).max:\n            self.load_time_series(time_series_index)\n        return self._time_series_sizes[time_series_index]\n\n    def current_time_series_size(self):\n        return self.time_series_size(self.current_time_series)\n\n    def advance(self):\n        \"\"\"Abstract method for obtain the new pair (time series, timestamp)\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler.advance","title":"<code>advance()</code>","text":"<p>Abstract method for obtain the new pair (time series, timestamp)</p> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>def advance(self):\n    \"\"\"Abstract method for obtain the new pair (time series, timestamp)\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler.at_end","title":"<code>at_end()</code>","text":"<p>Determines weather the iterator reached to its end</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Wether the iterator is at its end</p> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>def at_end(self) -&gt; bool:\n    \"\"\"\n    Determines weather the iterator reached to its end\n\n    Returns:\n        bool: Wether the iterator is at its end\n    \"\"\"\n    return self.current_time_series == self.wditerator.dataset.n_time_series\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler.initialize","title":"<code>initialize(iterator)</code>","text":"<p>Initialize the current shuffler</p> <p>This method is in charge of initializing everything to allow the correct iteration</p> <p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>WindowedDatasetIterator</code> <p>WindowedDatasetIterator</p> required Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>def initialize(self, iterator: \"WindowedDatasetIterator\"):\n    \"\"\"\n    Initialize the current shuffler\n\n    This method is in charge of initializing everything to allow the correct iteration\n\n    Parameters:\n        iterator: WindowedDatasetIterator\n\n    \"\"\"\n    self.wditerator = iterator\n    self._samples_per_time_series = (\n        np.ones(self.wditerator.dataset.n_time_series, dtype=np.int64)\n        * np.iinfo(np.int64).max\n    )\n    self._time_series_sizes = (\n        np.ones(self.wditerator.dataset.n_time_series, dtype=np.int64)\n        * np.iinfo(np.int64).max\n    )\n    self.current_time_series = 0\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler.next_element","title":"<code>next_element()</code>","text":"<p>Iterating function</p> <p>The method takes the current time serie, and the current time stamp and calls the advance method</p> <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Index of the current run-to-failure cycle andcurrent timestamp of that cycle</p> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>def next_element(self) -&gt; Tuple[int, int]:\n    \"\"\"\n    Iterating function\n\n    The method takes the current time serie, and the current time stamp\n    and calls the advance method\n\n    Returns:\n        Index of the current run-to-failure cycle andcurrent timestamp of that cycle\n\n    Raises:\n       When the iteration reaches the end\n    \"\"\"\n\n    if self.at_end():\n        raise StopIteration\n    ts_index = self.time_series()\n    timestamp = self.timestamp()\n    self.advance()\n\n    return ts_index, timestamp\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler.number_of_samples_of_current_time_series","title":"<code>number_of_samples_of_current_time_series()</code>","text":"<p>Obtain the number of samples for the current time series</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of samples</p> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>def number_of_samples_of_current_time_series(self) -&gt; int:\n    \"\"\"\n    Obtain the number of samples for the current time series\n\n    Returns:\n        Number of samples\n\n    \"\"\"\n\n    return self.number_samples_of_time_series(self.current_time_series)\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler.start","title":"<code>start(iterator)</code>","text":"<p>Start the shuffler given an iterator</p> <p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>WindowedDatasetIterator</code> <p>An Windowed Iterator</p> required Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>def start(self, iterator: \"WindowedDatasetIterator\"):\n    \"\"\"\n    Start the shuffler given an iterator\n\n    Parameters:\n        iterator: An Windowed Iterator\n    \"\"\"\n    self.initialize(iterator)\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler.time_series","title":"<code>time_series()</code>","text":"<p>Current time series</p> <p>Returns:</p> Type Description <code>int</code> <p>Current Time Series</p> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>def time_series(self) -&gt; int:\n    \"\"\"\n    Current time series\n\n    Returns:\n        Current Time Series\n    \"\"\"\n    return self.current_time_series\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AbstractShuffler.timestamp","title":"<code>timestamp()</code>","text":"<p>Obtain a timestamp for the current run-to-failure cycle</p> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>def timestamp(self) -&gt; int:\n    \"\"\"Obtain a timestamp for the current run-to-failure cycle\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.AllShuffled","title":"<code>AllShuffled</code>","text":"<p>               Bases: <code>AbstractShuffler</code></p> <p>Everything is shuffled</p> <pre><code>Iteration 1: | TS 1 | TS 2 | TS 2 | TS 1 | TS 1 | TS 2\n             |   3  | 2    |  1   |   1  |   2  |   3\n</code></pre> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>class AllShuffled(AbstractShuffler):\n    \"\"\"\n    Everything is shuffled\n\n        Iteration 1: | TS 1 | TS 2 | TS 2 | TS 1 | TS 1 | TS 2\n                     |   3  | 2    |  1   |   1  |   2  |   3\n    \"\"\"\n\n    def initialize(\n        self, iterator: \"WindowedDatasetIterator\", evenly_sampled: bool = True\n    ):\n        super().initialize(iterator)\n        self.evenly_sampled = evenly_sampled\n        self.timestamps_per_ts = {\n            i: None for i in range(self.wditerator.dataset.n_time_series)\n        }\n        self.timestamps_per_ts_indices = np.array(\n            [0 for i in range(self.wditerator.dataset.n_time_series)], dtype=np.int64\n        )\n\n    def time_series(self) -&gt; int:\n        l = np.random.randint(self.wditerator.dataset.n_time_series)\n        while (\n            self.number_samples_of_time_series(l) == self.timestamps_per_ts_indices[l]\n        ):\n            l = np.random.randint(self.wditerator.dataset.n_time_series)\n        self.current_time_series = l\n\n        return self.current_time_series\n\n    def at_end(self) -&gt; bool:\n        return (self._samples_per_time_series == self.timestamps_per_ts_indices).all()\n\n    def timestamp(self) -&gt; int:\n        return self.timestamps_per_ts[self.current_time_series][\n            self.timestamps_per_ts_indices[self.current_time_series]\n        ]\n\n    def advance(self):\n        self.timestamps_per_ts_indices[self.current_time_series] += 1\n\n    def load_time_series(self, time_series_index: int):\n        super().load_time_series(time_series_index)\n        if self.evenly_sampled:\n            total_length = self.time_series_size(time_series_index)\n            final_index = self.wditerator.end_index.get(total_length)\n\n            self.timestamps_per_ts[time_series_index] = np.arange(\n                start=self.wditerator.start_index.get(total_length),\n                stop=final_index,\n                step=self.wditerator.step,\n                dtype=np.int64,\n            )\n            if self.timestamps_per_ts[time_series_index][-1] != final_index - 1:\n                self.timestamps_per_ts[time_series_index] = np.append(\n                    self.timestamps_per_ts[time_series_index], final_index - 1\n                )\n\n        else:\n            N = self.time_series_size(time_series_index)\n            step = self.wditerator.step\n            self.timestamps_per_ts[time_series_index] = N - np.logspace(\n                0, 1, num=math.ceil(N / step), base=N\n            ).astype(\"int\")\n        np.random.shuffle(self.timestamps_per_ts[time_series_index])\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.IntraTimeSeriesShuffler","title":"<code>IntraTimeSeriesShuffler</code>","text":"<p>               Bases: <code>AbstractShuffler</code></p> <p>Each point of the time series is shuffled, but the TS are kept in order</p> <pre><code>Iteration 1: | TS 1 | TS 1 | TS 1 | TS 2 | TS 2 | TS 2\n             |   3  |  1   |  2   |   2  |   3  |   1\nIteration 2: | TS 1 | TS 1 | TS 1 | TS 2 | TS 2 | TS 2\n             |   1  |  3   |  2   |   3  |   2  |   1\n</code></pre> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>class IntraTimeSeriesShuffler(AbstractShuffler):\n    \"\"\"\n    Each point of the time series is shuffled, but the TS are kept in order\n\n        Iteration 1: | TS 1 | TS 1 | TS 1 | TS 2 | TS 2 | TS 2\n                     |   3  |  1   |  2   |   2  |   3  |   1\n        Iteration 2: | TS 1 | TS 1 | TS 1 | TS 2 | TS 2 | TS 2\n                     |   1  |  3   |  2   |   3  |   2  |   1\n    \"\"\"\n\n    def time_series_changed(self):\n        self.current_timestamp_index = 0\n        if self.current_time_series == self.wditerator.dataset.n_time_series:\n            return\n        self.timestamps = np.arange(\n            start=0,\n            stop=self.current_time_series_size(),\n            step=self.wditerator.step,\n            dtype=np.int64,\n        )\n        if (\n            self.wditerator.last_point\n            and self.timestamps[-1] != self.current_time_series_size() - 1\n        ):\n            self.timestamps = np.append(\n                self.timestamps, self.current_time_series_size() - 1\n            )\n        np.random.shuffle(self.timestamps)\n\n    def initialize(self, iterator: \"WindowedDatasetIterator\"):\n        super().initialize(iterator)\n        self.current_time_series = 0\n        self.time_series_changed()\n        self.n_time_series = iterator.dataset.n_time_series\n\n    def timestamp(self) -&gt; int:\n        ret = self.timestamps[self.current_timestamp_index]\n        return ret\n\n    def advance(self):\n        self.current_timestamp_index += 1\n\n        if (\n            self.current_timestamp_index\n            &gt;= self.number_of_samples_of_current_time_series()\n        ):\n            self.current_time_series += 1\n            self.time_series_changed()\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.InverseOrder","title":"<code>InverseOrder</code>","text":"<p>               Bases: <code>AbstractShuffler</code></p> <p>The data points will be fed in RUL decreasing order</p> <pre><code>Iteration 1: | TS 2 | TS 1 | TS 2 | TS 1 | TS 2 | TS 1\n             |   4  | 3    |  3   |   2  |   2  |   1\n</code></pre> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>class InverseOrder(AbstractShuffler):\n    \"\"\"\n    The data points will be fed in RUL decreasing order\n\n        Iteration 1: | TS 2 | TS 1 | TS 2 | TS 1 | TS 2 | TS 1\n                     |   4  | 3    |  3   |   2  |   2  |   1\n    \"\"\"\n\n    def initialize(self, iterator: \"WindowedDatasetIterator\"):\n        super().initialize(iterator)\n        self.sizes = np.array(\n            [self.time_series_size(i) for i in range(iterator.dataset.n_time_series)],\n            dtype=np.int64,\n        )\n\n    def time_series(self) -&gt; int:\n        self.current_time_series = np.argmax(self.sizes)\n        return self.current_time_series\n\n    def timestamp(self):\n        ret = self.sizes[self.current_time_series] - 1\n\n        return ret\n\n    def advance(self):\n        self.sizes[self.current_time_series] = np.clip(\n            np.int64(self.sizes[self.current_time_series]) - self.wditerator.step,\n            0,\n            np.inf,\n        )\n        if np.sum(self.sizes) == 0:\n            self.current_time_series = len(self.sizes)\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.NotShuffled","title":"<code>NotShuffled</code>","text":"<p>               Bases: <code>AbstractShuffler</code></p> <p>Nothing is shuffled. Each sample of each run-to-failure cycle are iterated in order</p> <pre><code>Iteration 1: | Life 1 | Life 1  | Life 1 | Life 2 | Life 3 | Life 3\n             |   1    | 2       |  3     |   1    |   2    |   1\nIteration 2: | Life 1 | Life 1  | Life 1 | Life 2 | Life 3 | Life 3\n             |   1    | 2       |  3     |   1    |   2    |   1\n</code></pre> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>class NotShuffled(AbstractShuffler):\n    \"\"\"\n    Nothing is shuffled. Each sample of each run-to-failure cycle are iterated in order\n\n        Iteration 1: | Life 1 | Life 1  | Life 1 | Life 2 | Life 3 | Life 3\n                     |   1    | 2       |  3     |   1    |   2    |   1\n        Iteration 2: | Life 1 | Life 1  | Life 1 | Life 2 | Life 3 | Life 3\n                     |   1    | 2       |  3     |   1    |   2    |   1\n    \"\"\"\n\n    def initialize(self, iterator: \"WindowedDatasetIterator\"):\n        super().initialize(iterator)\n        self.current_time_series = 0\n\n        self.current_timestamp = self.wditerator.start_index.get(\n            self.current_time_series_size()\n        )\n\n    def time_series_changed(self):\n        self.current_time_series += 1\n        if not self.at_end():\n            self.current_timestamp = self.wditerator.start_index.get(\n                self.current_time_series_size()\n            )\n\n    def timestamp(self) -&gt; int:\n        return self.current_timestamp\n\n    def advance(self):\n        final_timestamp = self.wditerator.end_index.get(self.current_time_series_size())\n        if self.current_timestamp == final_timestamp - 1:\n            self.time_series_changed()\n        else:\n            self.current_timestamp += self.wditerator.step\n            self.current_timestamp = min(self.current_timestamp, final_timestamp - 1)\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.TimeSeriesOrderIntraSignalShuffling","title":"<code>TimeSeriesOrderIntraSignalShuffling</code>","text":"<p>               Bases: <code>AbstractShuffler</code></p> <p>Each point in the ts is shuffled, and the ts order are shuffled also.</p> <p>!!! note</p> <pre><code>Iteration 1: | TS 1 | TS 1 | TS 1 | TS 2 | TS 2 | TS 2\n             |   3  | 2    |  1   |   1  |   3  |   2\nIteration 2: | TS 2 | TS 2 | TS 2 | TS 1 | TS 1 | TS 1\n             |   3 |  1   |  2   |   3  |   1  |   2\n</code></pre> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>class TimeSeriesOrderIntraSignalShuffling(AbstractShuffler):\n    \"\"\"\n     Each point in the ts is shuffled, and the ts order are shuffled also.\n\n    !!! note\n\n        Iteration 1: | TS 1 | TS 1 | TS 1 | TS 2 | TS 2 | TS 2\n                     |   3  | 2    |  1   |   1  |   3  |   2\n        Iteration 2: | TS 2 | TS 2 | TS 2 | TS 1 | TS 1 | TS 1\n                     |   3 |  1   |  2   |   3  |   1  |   2\n\n    \"\"\"\n\n    def initialize(self, iterator: \"WindowedDatasetIterator\"):\n        super().initialize(iterator)\n        self.available_time_series = np.arange(\n            iterator.dataset.n_time_series, dtype=np.int64\n        )\n        np.random.shuffle(self.available_time_series)\n        self.available_time_series_index = 0\n        self.current_time_series = self.available_time_series[\n            self.available_time_series_index\n        ]\n\n        self.available_time_stamps = np.arange(\n            start=0,\n            stop=self.number_of_samples_of_current_time_series(),\n            step=self.wditerator.step,\n            dtype=np.int64,\n        )\n        self.available_time_stamps_index = 0\n\n    def timestamp(self) -&gt; int:\n        ret = self.available_time_stamps[self.available_time_stamps_index]\n\n        return ret\n\n    def advance(self):\n        self.available_time_stamps_index += 1\n        if self.available_time_stamps_index == len(self.available_time_stamps):\n            self.time_series_changed()\n\n    def time_series_changed(self):\n        self.available_time_stamps_index = 0\n        self.available_time_series_index += 1\n        if self.available_time_series_index &lt; len(self.available_time_series):\n            self.current_time_series = self.available_time_series[\n                self.available_time_series_index\n            ]\n            self.available_time_stamps = np.arange(\n                start=0,\n                stop=self.current_time_series_size(),\n                step=self.wditerator.step,\n                dtype=np.int64,\n            )\n\n        else:\n            self.current_time_series = len(self.available_time_series)\n</code></pre>"},{"location":"iterators/shufflers/#ceruleo.iterators.shufflers.TimeSeriesOrderShuffling","title":"<code>TimeSeriesOrderShuffling</code>","text":"<p>               Bases: <code>AbstractShuffler</code></p> <p>Time series are shuffled, but each point inside the time series kept  its order</p> <pre><code>Iteration 1: | TS 1 | TS 1 | TS 1 | TS 2 | TS 2 | tS 2 |\n             |   1  | 2    |  3   |   1  |   2  |   3  |\nIteration 2: | TS 2 | TS 2 | TS 2 | TS 1 | TS 1 | TS 1 |\n             |   1  |  2   |  3   |   1  |   2  |   3  |\n</code></pre> Source code in <code>ceruleo/iterators/shufflers.py</code> <pre><code>class TimeSeriesOrderShuffling(AbstractShuffler):\n    \"\"\"\n    Time series are shuffled, but each point inside the time series kept  its order\n\n        Iteration 1: | TS 1 | TS 1 | TS 1 | TS 2 | TS 2 | tS 2 |\n                     |   1  | 2    |  3   |   1  |   2  |   3  |\n        Iteration 2: | TS 2 | TS 2 | TS 2 | TS 1 | TS 1 | TS 1 |\n                     |   1  |  2   |  3   |   1  |   2  |   3  |\n    \"\"\"\n\n    def initialize(self, iterator: \"WindowedDatasetIterator\"):\n        super().initialize(iterator)\n        self.available_time_series = np.arange(\n            iterator.dataset.n_time_series, dtype=np.int64\n        )\n        np.random.shuffle(self.available_time_series)\n        self.available_time_series_index = 0\n        self.current_timestamp = 0\n        self.current_time_series = self.available_time_series[\n            self.available_time_series_index\n        ]\n\n    def timestamp(self) -&gt; int:\n        ret = self.current_timestamp\n\n        return ret\n\n    def advance(self):\n        if self.current_timestamp == self.current_time_series_size() - 1:\n            self.time_series_changed()\n        else:\n            self.current_timestamp += self.wditerator.step\n            self.current_timestamp = min(\n                self.current_timestamp, self.current_time_series_size() - 1\n            )\n\n    def time_series_changed(self):\n        self.current_timestamp = 0\n        self.available_time_series_index += 1\n        if self.available_time_series_index &lt; len(self.available_time_series):\n            self.current_time_series = self.available_time_series[\n                self.available_time_series_index\n            ]\n        else:\n            self.current_time_series = len(self.available_time_series)\n</code></pre>"},{"location":"models/Models/","title":"Notebook: Tensorflow Models","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sbn\nfrom ceruleo.graphics.results import plot_predictions\n\nsbn.set()\n</pre> import matplotlib.pyplot as plt import seaborn as sbn from ceruleo.graphics.results import plot_predictions  sbn.set() <pre>2022-09-09 12:26:50.508592: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n/home/luciano/venvs/ceruleo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[3]: Copied! <pre>from ceruleo.dataset.catalog.CMAPSS import CMAPSSDataset\n</pre> from ceruleo.dataset.catalog.CMAPSS import CMAPSSDataset In\u00a0[4]: Copied! <pre>train_dataset = CMAPSSDataset(train=True, models='FD001')\n\ntest_dataset = CMAPSSDataset(train=False, models='FD001')[15:30]\n</pre> train_dataset = CMAPSSDataset(train=True, models='FD001')  test_dataset = CMAPSSDataset(train=False, models='FD001')[15:30] In\u00a0[5]: Copied! <pre>from ceruleo.transformation.functional.transformers import Transformer\nfrom ceruleo.transformation.features.selection import ByNameFeatureSelector\nfrom ceruleo.transformation.functional.pipeline.pipeline import make_pipeline\nfrom ceruleo.transformation.features.scalers import MinMaxScaler\nfrom ceruleo.dataset.catalog.CMAPSS import sensor_indices\n</pre> from ceruleo.transformation.functional.transformers import Transformer from ceruleo.transformation.features.selection import ByNameFeatureSelector from ceruleo.transformation.functional.pipeline.pipeline import make_pipeline from ceruleo.transformation.features.scalers import MinMaxScaler from ceruleo.dataset.catalog.CMAPSS import sensor_indices   In\u00a0[6]: Copied! <pre>FEATURES = [train_dataset[0].columns[i] for i in sensor_indices]\n</pre> FEATURES = [train_dataset[0].columns[i] for i in sensor_indices] In\u00a0[7]: Copied! <pre>transformer = Transformer(\n    pipelineX=make_pipeline(\n        ByNameFeatureSelector(features=FEATURES), \n        MinMaxScaler(range=(-1, 1))\n\n    ), \n    pipelineY=make_pipeline(\n        ByNameFeatureSelector(features=['RUL']),  \n    )\n)\n</pre>  transformer = Transformer(     pipelineX=make_pipeline(         ByNameFeatureSelector(features=FEATURES),          MinMaxScaler(range=(-1, 1))      ),      pipelineY=make_pipeline(         ByNameFeatureSelector(features=['RUL']),       ) )   In\u00a0[8]: Copied! <pre>from sklearn.model_selection import train_test_split\n</pre> from sklearn.model_selection import train_test_split In\u00a0[9]: Copied! <pre>train_dataset, val_dataset = train_test_split(train_dataset, train_size=0.9)\n</pre> train_dataset, val_dataset = train_test_split(train_dataset, train_size=0.9) In\u00a0[10]: Copied! <pre>len(train_dataset), len(val_dataset), len(test_dataset)\n</pre> len(train_dataset), len(val_dataset), len(test_dataset) Out[10]: <pre>(90, 10, 15)</pre> In\u00a0[11]: Copied! <pre>transformer.fit(train_dataset)\n</pre> transformer.fit(train_dataset) Out[11]: <pre>&lt;ceruleo.transformation.functional.transformers.Transformer at 0x7f86af5850c0&gt;</pre> In\u00a0[12]: Copied! <pre>from ceruleo.iterators.utils import true_values\nfrom ceruleo.iterators.shufflers import AllShuffled\nfrom ceruleo.iterators.iterators import WindowedDatasetIterator\nfrom ceruleo.models.keras.dataset import tf_regression_dataset\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\n</pre> from ceruleo.iterators.utils import true_values from ceruleo.iterators.shufflers import AllShuffled from ceruleo.iterators.iterators import WindowedDatasetIterator from ceruleo.models.keras.dataset import tf_regression_dataset import tensorflow as tf from tensorflow.keras.callbacks import EarlyStopping In\u00a0[13]: Copied! <pre>train_iterator = WindowedDatasetIterator(\n    train_dataset.map(transformer),\n    window_size=32,\n    step=1,\n    horizon=1,\n    shuffler=AllShuffled())\n\nval_iterator = WindowedDatasetIterator(\n    val_dataset.map(transformer),\n    window_size=32,\n    step=1,\n    horizon=1)\n\ntest_iterator = WindowedDatasetIterator(\n    test_dataset.map(transformer),\n    window_size=32,\n    step=1,\n    horizon=1)\n</pre> train_iterator = WindowedDatasetIterator(     train_dataset.map(transformer),     window_size=32,     step=1,     horizon=1,     shuffler=AllShuffled())  val_iterator = WindowedDatasetIterator(     val_dataset.map(transformer),     window_size=32,     step=1,     horizon=1)  test_iterator = WindowedDatasetIterator(     test_dataset.map(transformer),     window_size=32,     step=1,     horizon=1) In\u00a0[20]: Copied! <pre>from tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import (Conv1D, \n                                     SeparableConv1D, \n                                     GlobalMaxPooling1D,\n                                     SpatialDropout1D,\n                                     LayerNormalization,\n                                     Dropout,\n                                     Flatten, Dense, Lambda)\n\ninput = Input(train_iterator.shape)\nx = Conv1D(64, 3, padding='same', activation='relu')(input)\nx = Conv1D(64, 3, padding='same', activation='relu')(x)\nx = Conv1D(64, 3, padding='same', activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(64, activation='relu')(x)\nx = Dense(1, activation='linear')(x)\n\nmodel = Model(input, x)\nmodel.compile(loss='mae', \n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n              weighted_metrics=[])\n</pre>  from tensorflow.keras import Model, Input from tensorflow.keras.layers import (Conv1D,                                       SeparableConv1D,                                       GlobalMaxPooling1D,                                      SpatialDropout1D,                                      LayerNormalization,                                      Dropout,                                      Flatten, Dense, Lambda)  input = Input(train_iterator.shape) x = Conv1D(64, 3, padding='same', activation='relu')(input) x = Conv1D(64, 3, padding='same', activation='relu')(x) x = Conv1D(64, 3, padding='same', activation='relu')(x) x = GlobalMaxPooling1D()(x) x = Dense(64, activation='relu')(x) x = Dense(1, activation='linear')(x)  model = Model(input, x) model.compile(loss='mae',                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),               weighted_metrics=[]) In\u00a0[21]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 32, 14)]          0         \n                                                                 \n conv1d_3 (Conv1D)           (None, 32, 64)            2752      \n                                                                 \n conv1d_4 (Conv1D)           (None, 32, 64)            12352     \n                                                                 \n conv1d_5 (Conv1D)           (None, 32, 64)            12352     \n                                                                 \n global_max_pooling1d_1 (Glo  (None, 64)               0         \n balMaxPooling1D)                                                \n                                                                 \n dense_2 (Dense)             (None, 64)                4160      \n                                                                 \n dense_3 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 31,681\nTrainable params: 31,681\nNon-trainable params: 0\n_________________________________________________________________\n</pre> <p>Before calling fit the iterators are converted to tf.Dataset instances with <code>tf_regression_dataset</code></p> In\u00a0[22]: Copied! <pre>model.fit(tf_regression_dataset(train_iterator).batch(32),\n         validation_data=tf_regression_dataset(val_iterator).batch(32),\n         epochs=25,\n         callbacks=[EarlyStopping(patience=5)])\n</pre>  model.fit(tf_regression_dataset(train_iterator).batch(32),          validation_data=tf_regression_dataset(val_iterator).batch(32),          epochs=25,          callbacks=[EarlyStopping(patience=5)]) <pre>Epoch 1/25\n497/497 [==============================] - 6s 12ms/step - loss: 30.9233 - val_loss: 54.3612\nEpoch 2/25\n497/497 [==============================] - 6s 12ms/step - loss: 24.3385 - val_loss: 61.9812\nEpoch 3/25\n497/497 [==============================] - 6s 12ms/step - loss: 23.7552 - val_loss: 53.1126\nEpoch 4/25\n497/497 [==============================] - 6s 12ms/step - loss: 22.5025 - val_loss: 73.2846\nEpoch 5/25\n497/497 [==============================] - 6s 12ms/step - loss: 22.2755 - val_loss: 57.3249\nEpoch 6/25\n497/497 [==============================] - 6s 11ms/step - loss: 21.4041 - val_loss: 53.7682\nEpoch 7/25\n497/497 [==============================] - 6s 12ms/step - loss: 20.2960 - val_loss: 48.7970\nEpoch 8/25\n497/497 [==============================] - 6s 12ms/step - loss: 20.0461 - val_loss: 57.0885\nEpoch 9/25\n497/497 [==============================] - 6s 12ms/step - loss: 19.7204 - val_loss: 43.7519\nEpoch 10/25\n497/497 [==============================] - 6s 12ms/step - loss: 19.4036 - val_loss: 44.2331\nEpoch 11/25\n497/497 [==============================] - 6s 12ms/step - loss: 18.9765 - val_loss: 40.1705\nEpoch 12/25\n497/497 [==============================] - 6s 11ms/step - loss: 18.4863 - val_loss: 51.0144\nEpoch 13/25\n497/497 [==============================] - 6s 11ms/step - loss: 17.9724 - val_loss: 33.2148\nEpoch 14/25\n497/497 [==============================] - 6s 12ms/step - loss: 17.5956 - val_loss: 59.0072\nEpoch 15/25\n497/497 [==============================] - 6s 12ms/step - loss: 17.4037 - val_loss: 42.0109\nEpoch 16/25\n497/497 [==============================] - 6s 12ms/step - loss: 16.1480 - val_loss: 51.3150\nEpoch 17/25\n497/497 [==============================] - 6s 12ms/step - loss: 15.8136 - val_loss: 34.9649\nEpoch 18/25\n497/497 [==============================] - 6s 12ms/step - loss: 14.9915 - val_loss: 38.7736\n</pre> Out[22]: <pre>&lt;keras.callbacks.History at 0x7f3ff44f0b20&gt;</pre> In\u00a0[25]: Copied! <pre>plot_predictions( \n    (\n        true_values(val_iterator),\n        model.predict(tf_regression_dataset(val_iterator).batch(32))\n    ),\n    figsize=(17, 5)\n)\n</pre> plot_predictions(      (         true_values(val_iterator),         model.predict(tf_regression_dataset(val_iterator).batch(32))     ),     figsize=(17, 5) ) <pre>51/51 [==============================] - 0s 9ms/step\n</pre> Out[25]: <pre>&lt;AxesSubplot:xlabel='Hours [h]', ylabel='Hours [h]'&gt;</pre> In\u00a0[26]: Copied! <pre>plot_predictions( \n    (\n        true_values(test_iterator),\n         model.predict(tf_regression_dataset(test_iterator).batch(32))\n    ),\n    figsize=(17, 5)\n)\n</pre> plot_predictions(      (         true_values(test_iterator),          model.predict(tf_regression_dataset(test_iterator).batch(32))     ),     figsize=(17, 5) ) <pre>47/47 [==============================] - 0s 9ms/step\n</pre> Out[26]: <pre>&lt;AxesSubplot:xlabel='Hours [h]', ylabel='Hours [h]'&gt;</pre> In\u00a0[27]: Copied! <pre>from ceruleo.models.keras.catalog.CNLSTM import CNLSTM\n\nmodel = CNLSTM(\n    train_iterator.shape,\n    n_conv_layers=3,\n    initial_convolutional_size=64,\n    layers_recurrent=[32, 32],\n    hidden_size=(10, 15),\n    dense_layer_size=25,\n    dropout=0.1,\n)\nmodel.summary()\nmodel.compile(loss='mae', \n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              weighted_metrics=[])\nmodel.fit(tf_regression_dataset(train_iterator).batch(32),\n         validation_data=tf_regression_dataset(val_iterator).batch(32),\n         epochs=25,\n         callbacks=[EarlyStopping(patience=5)])\n</pre> from ceruleo.models.keras.catalog.CNLSTM import CNLSTM  model = CNLSTM(     train_iterator.shape,     n_conv_layers=3,     initial_convolutional_size=64,     layers_recurrent=[32, 32],     hidden_size=(10, 15),     dense_layer_size=25,     dropout=0.1, ) model.summary() model.compile(loss='mae',                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),               weighted_metrics=[]) model.fit(tf_regression_dataset(train_iterator).batch(32),          validation_data=tf_regression_dataset(val_iterator).batch(32),          epochs=25,          callbacks=[EarlyStopping(patience=5)]) <pre>Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv1d_6 (Conv1D)           (None, 32, 64)            1856      \n                                                                 \n max_pooling1d (MaxPooling1D  (None, 16, 64)           0         \n )                                                               \n                                                                 \n conv1d_7 (Conv1D)           (None, 16, 128)           16512     \n                                                                 \n max_pooling1d_1 (MaxPooling  (None, 8, 128)           0         \n 1D)                                                             \n                                                                 \n conv1d_8 (Conv1D)           (None, 8, 256)            65792     \n                                                                 \n max_pooling1d_2 (MaxPooling  (None, 4, 256)           0         \n 1D)                                                             \n                                                                 \n flatten (Flatten)           (None, 1024)              0         \n                                                                 \n dense_4 (Dense)             (None, 150)               153750    \n                                                                 \n dropout (Dropout)           (None, 150)               0         \n                                                                 \n reshape (Reshape)           (None, 10, 15)            0         \n                                                                 \n lstm (LSTM)                 (None, 10, 32)            6144      \n                                                                 \n lstm_1 (LSTM)               (None, 32)                8320      \n                                                                 \n dropout_1 (Dropout)         (None, 32)                0         \n                                                                 \n flatten_1 (Flatten)         (None, 32)                0         \n                                                                 \n dense_5 (Dense)             (None, 25)                825       \n                                                                 \n dropout_2 (Dropout)         (None, 25)                0         \n                                                                 \n dense_6 (Dense)             (None, 1)                 26        \n                                                                 \n=================================================================\nTotal params: 253,225\nTrainable params: 253,225\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/25\n497/497 [==============================] - 9s 15ms/step - loss: 87.4043 - val_loss: 74.3738\nEpoch 2/25\n497/497 [==============================] - 7s 14ms/step - loss: 75.8393 - val_loss: 63.5628\nEpoch 3/25\n497/497 [==============================] - 7s 14ms/step - loss: 64.1269 - val_loss: 53.3127\nEpoch 4/25\n497/497 [==============================] - 7s 14ms/step - loss: 53.0450 - val_loss: 42.7237\nEpoch 5/25\n497/497 [==============================] - 7s 14ms/step - loss: 43.0962 - val_loss: 35.2467\nEpoch 6/25\n497/497 [==============================] - 7s 15ms/step - loss: 35.2277 - val_loss: 28.3911\nEpoch 7/25\n497/497 [==============================] - 7s 15ms/step - loss: 29.9046 - val_loss: 26.4496\nEpoch 8/25\n497/497 [==============================] - 7s 15ms/step - loss: 26.3572 - val_loss: 24.9972\nEpoch 9/25\n497/497 [==============================] - 7s 15ms/step - loss: 24.3208 - val_loss: 23.1588\nEpoch 10/25\n497/497 [==============================] - 7s 14ms/step - loss: 23.0418 - val_loss: 24.5769\nEpoch 11/25\n497/497 [==============================] - 7s 14ms/step - loss: 22.0749 - val_loss: 23.9962\nEpoch 12/25\n497/497 [==============================] - 7s 14ms/step - loss: 21.9218 - val_loss: 25.8152\nEpoch 13/25\n497/497 [==============================] - 7s 14ms/step - loss: 21.5623 - val_loss: 23.5591\nEpoch 14/25\n497/497 [==============================] - 7s 14ms/step - loss: 21.2474 - val_loss: 23.1916\n</pre> Out[27]: <pre>&lt;keras.callbacks.History at 0x7f3fb84f0400&gt;</pre> In\u00a0[29]: Copied! <pre>plot_predictions( \n    (\n        true_values(val_iterator),\n        model.predict(tf_regression_dataset(val_iterator).batch(32))\n    ),\n    figsize=(17, 5)\n)\n</pre> plot_predictions(      (         true_values(val_iterator),         model.predict(tf_regression_dataset(val_iterator).batch(32))     ),     figsize=(17, 5) ) <pre>51/51 [==============================] - 1s 10ms/step\n</pre> Out[29]: <pre>&lt;AxesSubplot:xlabel='Hours [h]', ylabel='Hours [h]'&gt;</pre> In\u00a0[31]: Copied! <pre>plot_predictions( \n    (\n        true_values(test_iterator),\n         model.predict(tf_regression_dataset(test_iterator).batch(32))\n    ),\n    figsize=(17, 5)\n)\n</pre> plot_predictions(      (         true_values(test_iterator),          model.predict(tf_regression_dataset(test_iterator).batch(32))     ),     figsize=(17, 5) ) <pre>47/47 [==============================] - 0s 10ms/step\n</pre> Out[31]: <pre>&lt;AxesSubplot:xlabel='Hours [h]', ylabel='Hours [h]'&gt;</pre> In\u00a0[14]: Copied! <pre>from ceruleo.models.keras.catalog.XCM import XCM, explain\n</pre> from ceruleo.models.keras.catalog.XCM import XCM, explain In\u00a0[25]: Copied! <pre>model_XCM, model_extras = XCM( train_iterator.shape, n_filters=32, filter_window=3)\nmodel_XCM.compile(\n    jit_compile=True,\n    loss='mae',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n    weighted_metrics=[])\nmodel_XCM.fit(\n    tf_regression_dataset(train_iterator).batch(32),\n    validation_data=tf_regression_dataset(val_iterator).batch(32),\n    epochs=25,\n    callbacks=[EarlyStopping(patience=5)])\n</pre> model_XCM, model_extras = XCM( train_iterator.shape, n_filters=32, filter_window=3) model_XCM.compile(     jit_compile=True,     loss='mae',     optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),     weighted_metrics=[]) model_XCM.fit(     tf_regression_dataset(train_iterator).batch(32),     validation_data=tf_regression_dataset(val_iterator).batch(32),     epochs=25,     callbacks=[EarlyStopping(patience=5)]) <pre>Epoch 1/25\n</pre> <pre>2022-09-09 11:38:25.016636: I tensorflow/stream_executor/cuda/cuda_dnn.cc:5025] Disabling cuDNN frontend for the following convolution:\n  input: {count: 32 feature_map_count: 1 spatial: 63 14  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX}\n  filter: {output_feature_map_count: 32 input_feature_map_count: 1 layout: OutputInputYX shape: 32 1 }\n  {zero_padding: 0 0  pad_alignment: default filter_strides: 1 1  dilation_rates: 1 1 }\n  ... because it uses an identity activation.\n</pre> <pre>    492/Unknown - 8s 11ms/step - loss: 90.8212</pre> <pre>2022-09-09 11:38:32.329728: I tensorflow/stream_executor/cuda/cuda_dnn.cc:5025] Disabling cuDNN frontend for the following convolution:\n  input: {count: 31 feature_map_count: 1 spatial: 63 14  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX}\n  filter: {output_feature_map_count: 32 input_feature_map_count: 1 layout: OutputInputYX shape: 32 1 }\n  {zero_padding: 0 0  pad_alignment: default filter_strides: 1 1  dilation_rates: 1 1 }\n  ... because it uses an identity activation.\n</pre> <pre>495/495 [==============================] - 11s 18ms/step - loss: 91.1590 - val_loss: 84.5694\nEpoch 2/25\n</pre> <pre>2022-09-09 11:38:34.641340: I tensorflow/stream_executor/cuda/cuda_dnn.cc:5025] Disabling cuDNN frontend for the following convolution:\n  input: {count: 28 feature_map_count: 1 spatial: 63 14  value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX}\n  filter: {output_feature_map_count: 32 input_feature_map_count: 1 layout: OutputInputYX shape: 32 1 }\n  {zero_padding: 0 0  pad_alignment: default filter_strides: 1 1  dilation_rates: 1 1 }\n  ... because it uses an identity activation.\n</pre> <pre>495/495 [==============================] - 6s 12ms/step - loss: 83.6028 - val_loss: 75.1526\nEpoch 3/25\n495/495 [==============================] - 6s 12ms/step - loss: 68.7464 - val_loss: 58.8427\nEpoch 4/25\n495/495 [==============================] - 6s 12ms/step - loss: 48.8319 - val_loss: 38.0875\nEpoch 5/25\n495/495 [==============================] - 6s 12ms/step - loss: 32.0768 - val_loss: 24.3888\nEpoch 6/25\n495/495 [==============================] - 6s 12ms/step - loss: 25.8530 - val_loss: 20.6111\nEpoch 7/25\n495/495 [==============================] - 6s 13ms/step - loss: 24.1897 - val_loss: 19.9455\nEpoch 8/25\n495/495 [==============================] - 6s 12ms/step - loss: 23.7096 - val_loss: 24.6311\nEpoch 9/25\n495/495 [==============================] - 6s 13ms/step - loss: 23.2487 - val_loss: 22.9623\nEpoch 10/25\n495/495 [==============================] - 6s 12ms/step - loss: 22.9390 - val_loss: 20.5767\nEpoch 11/25\n495/495 [==============================] - 7s 13ms/step - loss: 22.7250 - val_loss: 24.7373\nEpoch 12/25\n495/495 [==============================] - 6s 12ms/step - loss: 22.6077 - val_loss: 20.7243\n</pre> Out[25]: <pre>&lt;keras.callbacks.History at 0x7f78504e3100&gt;</pre> In\u00a0[26]: Copied! <pre>plot_predictions( \n    (\n        true_values(val_iterator),\n        model_XCM.predict(tf_regression_dataset(val_iterator).batch(32))\n    ),\n    figsize=(17, 5)\n)\n</pre> plot_predictions(      (         true_values(val_iterator),         model_XCM.predict(tf_regression_dataset(val_iterator).batch(32))     ),     figsize=(17, 5) )  <pre>53/53 [==============================] - 1s 12ms/step\n</pre> Out[26]: <pre>&lt;AxesSubplot:xlabel='Hours [h]', ylabel='Hours [h]'&gt;</pre> In\u00a0[27]: Copied! <pre>plot_predictions( \n    (\n        true_values(test_iterator),\n        model_XCM.predict(tf_regression_dataset(test_iterator).batch(32))\n    ),\n    figsize=(17, 5)\n)\n</pre> plot_predictions(      (         true_values(test_iterator),         model_XCM.predict(tf_regression_dataset(test_iterator).batch(32))     ),     figsize=(17, 5) )  <pre>47/47 [==============================] - 0s 9ms/step\n</pre> Out[27]: <pre>&lt;AxesSubplot:xlabel='Hours [h]', ylabel='Hours [h]'&gt;</pre> In\u00a0[70]: Copied! <pre>it = iter(val_iterator)\nfor i in range(45):\n    X, y, sw = next(it)\n</pre> it = iter(val_iterator) for i in range(45):     X, y, sw = next(it) In\u00a0[71]: Copied! <pre>mmap, v = explain(model_extras, X)\n</pre> mmap, v = explain(model_extras, X) We can evaluate how each feature contribute positively or negatively to the final result In\u00a0[72]: Copied! <pre>from ceruleo.graphics.explanations import XCM_explanation\nXCM_explanation(mmap, v);\n</pre> from ceruleo.graphics.explanations import XCM_explanation XCM_explanation(mmap, v); In\u00a0[30]: Copied! <pre>from tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import (Conv1D, \n                                     SeparableConv1D, \n                                     GlobalMaxPooling1D,\n                                     SpatialDropout1D,\n                                     LayerNormalization,\n                                     Dropout,\n                                     Flatten, Dense, Lambda)\nfrom ceruleo.models.keras.layers import LASSOLayer\n\ninput = Input(train_iterator.shape)\nx = LASSOLayer(0.005)(input)\nx = Conv1D(32, 3, padding='same', activation='relu')(x)\nx = Conv1D(32, 3, padding='same', activation='relu')(x)\nx = Conv1D(32, 3, padding='same', activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(32, activation='relu')(x)\nx = Dense(1, activation='linear')(x)\n\nmodel = Model(input, x)\nmodel.compile(loss='mae', \n              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              weighted_metrics=[])\n</pre>  from tensorflow.keras import Model, Input from tensorflow.keras.layers import (Conv1D,                                       SeparableConv1D,                                       GlobalMaxPooling1D,                                      SpatialDropout1D,                                      LayerNormalization,                                      Dropout,                                      Flatten, Dense, Lambda) from ceruleo.models.keras.layers import LASSOLayer  input = Input(train_iterator.shape) x = LASSOLayer(0.005)(input) x = Conv1D(32, 3, padding='same', activation='relu')(x) x = Conv1D(32, 3, padding='same', activation='relu')(x) x = Conv1D(32, 3, padding='same', activation='relu')(x) x = GlobalMaxPooling1D()(x) x = Dense(32, activation='relu')(x) x = Dense(1, activation='linear')(x)  model = Model(input, x) model.compile(loss='mae',                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),               weighted_metrics=[]) In\u00a0[31]: Copied! <pre>model.fit(tf_regression_dataset(train_iterator).batch(32),\n         validation_data=tf_regression_dataset(val_iterator).batch(32),\n         epochs=25,\n         callbacks=[EarlyStopping(patience=5)])\n</pre>  model.fit(tf_regression_dataset(train_iterator).batch(32),          validation_data=tf_regression_dataset(val_iterator).batch(32),          epochs=25,          callbacks=[EarlyStopping(patience=5)]) <pre>Epoch 1/25\n492/492 [==============================] - 6s 12ms/step - loss: 78.5318 - Number of features: 284.2159 - val_loss: 41.2835 - val_Number of features: 188.0000\nEpoch 2/25\n492/492 [==============================] - 6s 12ms/step - loss: 38.8475 - Number of features: 168.5764 - val_loss: 38.0698 - val_Number of features: 155.0000\nEpoch 3/25\n492/492 [==============================] - 6s 12ms/step - loss: 36.6685 - Number of features: 145.1996 - val_loss: 36.1835 - val_Number of features: 135.0000\nEpoch 4/25\n492/492 [==============================] - 6s 12ms/step - loss: 35.5707 - Number of features: 132.4623 - val_loss: 35.2223 - val_Number of features: 132.0000\nEpoch 5/25\n492/492 [==============================] - 6s 12ms/step - loss: 35.1087 - Number of features: 130.2790 - val_loss: 33.9896 - val_Number of features: 130.0000\nEpoch 6/25\n492/492 [==============================] - 6s 13ms/step - loss: 34.6573 - Number of features: 130.0000 - val_loss: 33.3626 - val_Number of features: 130.0000\nEpoch 7/25\n492/492 [==============================] - 6s 12ms/step - loss: 34.4908 - Number of features: 130.0000 - val_loss: 32.4213 - val_Number of features: 130.0000\nEpoch 8/25\n492/492 [==============================] - 6s 12ms/step - loss: 34.0361 - Number of features: 128.2505 - val_loss: 31.9390 - val_Number of features: 128.0000\nEpoch 9/25\n492/492 [==============================] - 6s 13ms/step - loss: 33.7632 - Number of features: 128.0000 - val_loss: 31.0827 - val_Number of features: 128.0000\nEpoch 10/25\n492/492 [==============================] - 6s 12ms/step - loss: 33.7174 - Number of features: 126.7923 - val_loss: 30.2859 - val_Number of features: 126.0000\nEpoch 11/25\n492/492 [==============================] - 6s 12ms/step - loss: 33.1516 - Number of features: 125.3523 - val_loss: 30.3310 - val_Number of features: 124.0000\nEpoch 12/25\n492/492 [==============================] - 6s 12ms/step - loss: 33.2585 - Number of features: 123.4216 - val_loss: 29.5472 - val_Number of features: 123.0000\nEpoch 13/25\n492/492 [==============================] - 6s 12ms/step - loss: 32.8898 - Number of features: 122.8676 - val_loss: 29.4038 - val_Number of features: 122.0000\nEpoch 14/25\n492/492 [==============================] - 6s 12ms/step - loss: 32.7780 - Number of features: 120.9002 - val_loss: 29.2233 - val_Number of features: 120.0000\nEpoch 15/25\n492/492 [==============================] - 6s 13ms/step - loss: 32.7945 - Number of features: 120.0000 - val_loss: 28.2997 - val_Number of features: 120.0000\nEpoch 16/25\n492/492 [==============================] - 7s 14ms/step - loss: 32.1608 - Number of features: 118.3361 - val_loss: 29.0127 - val_Number of features: 117.0000\nEpoch 17/25\n492/492 [==============================] - 6s 12ms/step - loss: 32.5062 - Number of features: 115.4358 - val_loss: 27.9307 - val_Number of features: 115.0000\nEpoch 18/25\n492/492 [==============================] - 6s 12ms/step - loss: 31.9253 - Number of features: 113.5499 - val_loss: 28.1563 - val_Number of features: 113.0000\nEpoch 19/25\n492/492 [==============================] - 6s 12ms/step - loss: 31.9012 - Number of features: 110.5173 - val_loss: 27.8570 - val_Number of features: 109.0000\nEpoch 20/25\n492/492 [==============================] - 6s 12ms/step - loss: 31.7711 - Number of features: 108.8493 - val_loss: 27.7092 - val_Number of features: 108.0000\nEpoch 21/25\n492/492 [==============================] - 6s 12ms/step - loss: 31.6239 - Number of features: 107.4236 - val_loss: 27.5277 - val_Number of features: 107.0000\nEpoch 22/25\n492/492 [==============================] - 6s 12ms/step - loss: 31.6395 - Number of features: 107.0000 - val_loss: 27.0174 - val_Number of features: 107.0000\nEpoch 23/25\n492/492 [==============================] - 6s 12ms/step - loss: 31.4427 - Number of features: 107.0000 - val_loss: 27.0937 - val_Number of features: 107.0000\nEpoch 24/25\n492/492 [==============================] - 6s 12ms/step - loss: 31.6115 - Number of features: 107.0000 - val_loss: 26.3777 - val_Number of features: 107.0000\nEpoch 25/25\n492/492 [==============================] - 6s 12ms/step - loss: 31.3101 - Number of features: 106.4236 - val_loss: 26.7442 - val_Number of features: 105.0000\n</pre> Out[31]: <pre>&lt;keras.callbacks.History at 0x7f85c8c39960&gt;</pre> In\u00a0[32]: Copied! <pre>plot_predictions( \n    (\n        true_values(val_iterator),\n        model.predict(tf_regression_dataset(val_iterator).batch(32))\n    ),\n    figsize=(17, 5)\n)\n</pre> plot_predictions(      (         true_values(val_iterator),         model.predict(tf_regression_dataset(val_iterator).batch(32))     ),     figsize=(17, 5) )  <pre>58/58 [==============================] - 1s 9ms/step\n</pre> Out[32]: <pre>&lt;AxesSubplot:xlabel='Hours [h]', ylabel='Hours [h]'&gt;</pre> In\u00a0[33]: Copied! <pre>plot_predictions( \n    (\n        true_values(test_iterator),\n        model.predict(tf_regression_dataset(test_iterator).batch(32))\n    ),\n    figsize=(17, 5)\n)\n</pre> plot_predictions(      (         true_values(test_iterator),         model.predict(tf_regression_dataset(test_iterator).batch(32))     ),     figsize=(17, 5) )  <pre>47/47 [==============================] - 0s 10ms/step\n</pre> Out[33]: <pre>&lt;AxesSubplot:xlabel='Hours [h]', ylabel='Hours [h]'&gt;</pre> In\u00a0[35]: Copied! <pre>from ceruleo.graphics.explanations import show_LASSOLayer\nshow_LASSOLayer(model, test_iterator.shape, scale=True, figsize=(17, 5));\n</pre> from ceruleo.graphics.explanations import show_LASSOLayer show_LASSOLayer(model, test_iterator.shape, scale=True, figsize=(17, 5)); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"models/Models/#notebook-tensorflow-models","title":"Notebook: Tensorflow Models\u00b6","text":""},{"location":"models/Models/#load-the-dataset","title":"Load the dataset\u00b6","text":""},{"location":"models/Models/#create-a-transformer-for-a-dataset","title":"Create a transformer for a dataset\u00b6","text":"<p>The transformer is going to scale a subset of features to the (-1, 1) range.</p>"},{"location":"models/Models/#split-train-test-validation","title":"Split train-test-validation\u00b6","text":""},{"location":"models/Models/#fitting-the-transformer","title":"Fitting the transformer\u00b6","text":""},{"location":"models/Models/#build-the-windoweddatasetiterator","title":"Build the WindowedDatasetIterator\u00b6","text":""},{"location":"models/Models/#tensorflow-keras","title":"Tensorflow // Keras\u00b6","text":"<p>In order to use the ceruleo iterators with Tensorflow with just need to use a function that builds a tf.Data instance from a generator called <code>ceruleo.models.keras.dataset.tf_regression_dataset</code> .</p> <p>But first we can just create a regular Tensorflow Model</p>"},{"location":"models/Models/#build-tensorflow-model","title":"Build tensorflow model\u00b6","text":""},{"location":"models/Models/#validation-set-results","title":"Validation set results\u00b6","text":""},{"location":"models/Models/#test-set-results","title":"Test set results\u00b6","text":""},{"location":"models/Models/#model-catalog","title":"Model Catalog\u00b6","text":"<p>We have multiple keras models implemented. This models are just functions that return a uncompiled model. There are a few model published in literature ready to use.</p> <p>In this case we are using the one proposed in</p> <p>Temporal Convolutional Memory Networks for Remaining Useful Life Estimation of Industrial Machinery by Lahiru Jayasinghe, Tharaka Samarasinghe, Chau Yuen, Jenny Chen Ni Low, Shuzhi Sam Ge</p> <p>Reference</p>"},{"location":"models/Models/#explainable-model","title":"Explainable model\u00b6","text":""},{"location":"models/Models/#xcm","title":"XCM\u00b6","text":"<p>XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification Kevin Fauvel, Tao Lin, V\u00e9ronique Masson, \u00c9lisa Fromont and Alexandre Termier</p> <p>\"XCM allow the full exploitation of a faithful post hoc model-specific explainability method by identifying the observed variables and timestamps of the input data that are important for predictions\"</p> <p>XCM enables an identification of the regions of the input data that are important for predictions.</p>"},{"location":"models/Models/#lassolayer","title":"LASSOLayer\u00b6","text":"<p>LASSOLAYER: NONLINEAR FEATURE SELECTION BY SWITCHING ONE-TO-ONE LINKS, Akihito Sudo, Teng Teck Hou, Masaki Yamaguchi, Yoshinori Tone</p> <p>LASSOLayer performs feature selection inside a depp learning architecture. We can use the same architecture we defien at the beggining of this guide but putting before the convolutional layers the LASSOLayer for performing feature selection</p>"},{"location":"models/Models_sklearn/","title":"Notebook: Scikit-learn Models","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sbn\n\nsbn.set()\n</pre> import matplotlib.pyplot as plt import seaborn as sbn  sbn.set() In\u00a0[3]: Copied! <pre>from ceruleo.dataset.catalog.CMAPSS import CMAPSSDataset\n</pre> from ceruleo.dataset.catalog.CMAPSS import CMAPSSDataset <pre>2024-02-24 22:44:16.622494: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-02-24 22:44:16.624330: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2024-02-24 22:44:16.651060: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 22:44:16.651085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 22:44:16.652011: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-02-24 22:44:16.656662: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2024-02-24 22:44:16.657264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-02-24 22:44:17.314001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/luciano/venvs/ceruleo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[4]: Copied! <pre>train_dataset = CMAPSSDataset(train=True, models='FD001')\n\ntest_dataset = CMAPSSDataset(train=False, models='FD001')[15:30]\n</pre> train_dataset = CMAPSSDataset(train=True, models='FD001')  test_dataset = CMAPSSDataset(train=False, models='FD001')[15:30] In\u00a0[5]: Copied! <pre>from ceruleo.transformation.functional.transformers import Transformer\nfrom ceruleo.transformation.features.selection import ByNameFeatureSelector\nfrom ceruleo.transformation.functional.pipeline.pipeline import make_pipeline\nfrom ceruleo.transformation.features.scalers import MinMaxScaler\nfrom ceruleo.dataset.catalog.CMAPSS import sensor_indices\nFEATURES = [train_dataset[0].columns[i] for i in sensor_indices]\n</pre> from ceruleo.transformation.functional.transformers import Transformer from ceruleo.transformation.features.selection import ByNameFeatureSelector from ceruleo.transformation.functional.pipeline.pipeline import make_pipeline from ceruleo.transformation.features.scalers import MinMaxScaler from ceruleo.dataset.catalog.CMAPSS import sensor_indices FEATURES = [train_dataset[0].columns[i] for i in sensor_indices]  In\u00a0[6]: Copied! <pre>transformer = Transformer(\n    pipelineX=make_pipeline(\n        ByNameFeatureSelector(features=FEATURES), \n        MinMaxScaler(range=(-1, 1))\n\n    ), \n    pipelineY=make_pipeline(\n        ByNameFeatureSelector(features=['RUL']),  \n    )\n)\n</pre>  transformer = Transformer(     pipelineX=make_pipeline(         ByNameFeatureSelector(features=FEATURES),          MinMaxScaler(range=(-1, 1))      ),      pipelineY=make_pipeline(         ByNameFeatureSelector(features=['RUL']),       ) )   In\u00a0[7]: Copied! <pre>from sklearn.model_selection import train_test_split\n</pre> from sklearn.model_selection import train_test_split In\u00a0[8]: Copied! <pre>train_dataset, val_dataset = train_test_split(train_dataset, train_size=0.9)\n</pre> train_dataset, val_dataset = train_test_split(train_dataset, train_size=0.9) In\u00a0[9]: Copied! <pre>len(train_dataset), len(val_dataset), len(test_dataset)\n</pre> len(train_dataset), len(val_dataset), len(test_dataset) Out[9]: <pre>(90, 10, 15)</pre> In\u00a0[10]: Copied! <pre>import sklearn.pipeline as sk_pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.compose import TransformedTargetRegressor\nfrom ceruleo.models.sklearn import EstimatorWrapper, TimeSeriesWindowTransformer, CeruleoRegressor\nfrom sklearn.linear_model import Ridge\n</pre> import sklearn.pipeline as sk_pipeline from sklearn.ensemble import RandomForestRegressor from sklearn.compose import TransformedTargetRegressor from ceruleo.models.sklearn import EstimatorWrapper, TimeSeriesWindowTransformer, CeruleoRegressor from sklearn.linear_model import Ridge  In\u00a0[11]: Copied! <pre>regressor = CeruleoRegressor(\n    TimeSeriesWindowTransformer(\n        transformer,\n        window_size=32,\n        padding=True,\n        step=1),   \n    Ridge(alpha=15))\n\nregressor.fit(train_dataset)\n</pre>  regressor = CeruleoRegressor(     TimeSeriesWindowTransformer(         transformer,         window_size=32,         padding=True,         step=1),        Ridge(alpha=15))  regressor.fit(train_dataset) Out[11]: <pre>CeruleoRegressor(regressor=Ridge(alpha=15),\n                 ts_window_transformer=TimeSeriesWindowTransformer(transformer=&lt;ceruleo.transformation.functional.transformers.Transformer object at 0x7d9eea07f2d0&gt;,\n                                                                   window_size=32))</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0CeruleoRegressoriNot fitted<pre>CeruleoRegressor(regressor=Ridge(alpha=15),\n                 ts_window_transformer=TimeSeriesWindowTransformer(transformer=&lt;ceruleo.transformation.functional.transformers.Transformer object at 0x7d9eea07f2d0&gt;,\n                                                                   window_size=32))</pre> regressor: Ridge<pre>Ridge(alpha=15)</pre> \u00a0Ridge?Documentation for Ridge<pre>Ridge(alpha=15)</pre> ts_window_transformer: TimeSeriesWindowTransformer<pre>TimeSeriesWindowTransformer(transformer=&lt;ceruleo.transformation.functional.transformers.Transformer object at 0x7d9eea07f2d0&gt;,\n                            window_size=32)</pre> transformer: Transformer<pre>{'features': None, 'pipelineX': [('ByNameFeatureSelector', ['SensorMeasure2', 'SensorMeasure3', 'SensorMeasure4', 'SensorMeasure7', 'SensorMeasure8', 'SensorMeasure9', 'SensorMeasure11', 'SensorMeasure12', 'SensorMeasure13', 'SensorMeasure14', 'SensorMeasure15', 'SensorMeasure17', 'SensorMeasure20', 'SensorMeasure21']), ('MinMaxScaler', {'Min': SensorMeasure2      641.2100\nSensorMeasure3     1571.0400\nSensorMeasure4     1382.2500\nSensorMeasure7      549.8500\nSensorMeasure8     2387.9000\nSensorMeasure9     9021.7300\nSensorMeasure11      46.8500\nSensorMeasure12     518.8300\nSensorMeasure13    2387.8800\nSensorMeasure14    8099.9400\nSensorMeasure15       8.3249\nSensorMeasure17     388.0000\nSensorMeasure20      38.1400\nSensorMeasure21      22.8942\ndtype: float64, 'Max': SensorMeasure2      644.5300\nSensorMeasure3     1616.9100\nSensorMeasure4     1441.4900\nSensorMeasure7      556.0600\nSensorMeasure8     2388.5600\nSensorMeasure9     9244.5900\nSensorMeasure11      48.5300\nSensorMeasure12     523.3800\nSensorMeasure13    2388.5600\nSensorMeasure14    8293.7200\nSensorMeasure15       8.5678\nSensorMeasure17     400.0000\nSensorMeasure20      39.4300\nSensorMeasure21      23.6184\ndtype: float64})], 'pipelineY': [('ByNameFeatureSelector', ['RUL'])]}</pre> pipelineX: Pipeline<pre>Pipeline(final_step=MinMaxScaler(name='MinMaxScaler', range=(-1, 1)))</pre> final_step: MinMaxScaler<pre>MinMaxScaler</pre> MinMaxScaler<pre>MinMaxScaler</pre> pipelineY: Pipeline<pre>Pipeline(final_step=ByNameFeatureSelector(features=['RUL'],\n                                          name='ByNameFeatureSelector'))</pre> final_step: ByNameFeatureSelector<pre>ByNameFeatureSelector : [RUL]</pre> ByNameFeatureSelector<pre>ByNameFeatureSelector : [RUL]</pre> In\u00a0[12]: Copied! <pre>fig, ax = plt.subplots(figsize=(17, 5))\nax.plot(regressor.predict(val_dataset))\nax.plot(regressor.ts_window_transformer.true_values(val_dataset))\n</pre> fig, ax = plt.subplots(figsize=(17, 5)) ax.plot(regressor.predict(val_dataset)) ax.plot(regressor.ts_window_transformer.true_values(val_dataset)) Out[12]: <pre>[&lt;matplotlib.lines.Line2D at 0x7d9ee8207310&gt;]</pre> In\u00a0[13]: Copied! <pre>fig, ax = plt.subplots(figsize=(17, 5))\nax.plot(regressor.predict(test_dataset))\nax.plot(regressor.ts_window_transformer.true_values(test_dataset))\n</pre> fig, ax = plt.subplots(figsize=(17, 5)) ax.plot(regressor.predict(test_dataset)) ax.plot(regressor.ts_window_transformer.true_values(test_dataset)) Out[13]: <pre>[&lt;matplotlib.lines.Line2D at 0x7d9ee7f39810&gt;]</pre> In\u00a0[14]: Copied! <pre>from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor \nfrom ceruleo.models.sklearn import CeruleoMetricWrapper\n\n\ntransformer = Transformer(\n    pipelineX=make_pipeline(\n        ByNameFeatureSelector(features=FEATURES), \n        MinMaxScaler(range=(-1, 1))\n\n    ), \n    pipelineY=make_pipeline(\n        ByNameFeatureSelector(features=['RUL']),  \n    )\n)\n\nregressor_gs = CeruleoRegressor(\n    TimeSeriesWindowTransformer(\n        transformer,\n        window_size=32,\n        padding=True,\n        step=1),   \n    Ridge(alpha=15)\n)\ngrid_search = GridSearchCV(\n    estimator=regressor_gs,\n     param_grid={\n        'ts_window_transformer__window_size': [5, 10],         \n        'regressor': [Ridge(alpha=15), RandomForestRegressor(max_depth=5)]\n    },\n    scoring=CeruleoMetricWrapper('neg_mean_absolute_error'),\n    cv=2\n)\n\n\ngrid_search.fit(train_dataset)\n</pre> from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestRegressor  from ceruleo.models.sklearn import CeruleoMetricWrapper   transformer = Transformer(     pipelineX=make_pipeline(         ByNameFeatureSelector(features=FEATURES),          MinMaxScaler(range=(-1, 1))      ),      pipelineY=make_pipeline(         ByNameFeatureSelector(features=['RUL']),       ) )  regressor_gs = CeruleoRegressor(     TimeSeriesWindowTransformer(         transformer,         window_size=32,         padding=True,         step=1),        Ridge(alpha=15) ) grid_search = GridSearchCV(     estimator=regressor_gs,      param_grid={         'ts_window_transformer__window_size': [5, 10],                  'regressor': [Ridge(alpha=15), RandomForestRegressor(max_depth=5)]     },     scoring=CeruleoMetricWrapper('neg_mean_absolute_error'),     cv=2 )   grid_search.fit(train_dataset) Out[14]: <pre>GridSearchCV(cv=2,\n             estimator=CeruleoRegressor(regressor=Ridge(alpha=15),\n                                        ts_window_transformer=TimeSeriesWindowTransformer(transformer=&lt;ceruleo.transformation.functional.transformers.Transformer object at 0x7d9ee81eac90&gt;,\n                                                                                          window_size=32)),\n             param_grid={'regressor': [Ridge(alpha=15),\n                                       RandomForestRegressor(max_depth=5)],\n                         'ts_window_transformer__window_size': [5, 10]},\n             scoring=&lt;ceruleo.models.sklearn.CeruleoMetricWrapper object at 0x7d9eea3868d0&gt;)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0GridSearchCV?Documentation for GridSearchCViFitted<pre>GridSearchCV(cv=2,\n             estimator=CeruleoRegressor(regressor=Ridge(alpha=15),\n                                        ts_window_transformer=TimeSeriesWindowTransformer(transformer=&lt;ceruleo.transformation.functional.transformers.Transformer object at 0x7d9ee81eac90&gt;,\n                                                                                          window_size=32)),\n             param_grid={'regressor': [Ridge(alpha=15),\n                                       RandomForestRegressor(max_depth=5)],\n                         'ts_window_transformer__window_size': [5, 10]},\n             scoring=&lt;ceruleo.models.sklearn.CeruleoMetricWrapper object at 0x7d9eea3868d0&gt;)</pre> estimator: CeruleoRegressor<pre>CeruleoRegressor(regressor=Ridge(alpha=15),\n                 ts_window_transformer=TimeSeriesWindowTransformer(transformer=&lt;ceruleo.transformation.functional.transformers.Transformer object at 0x7d9ee81eac90&gt;,\n                                                                   window_size=32))</pre> regressor: Ridge<pre>Ridge(alpha=15)</pre> \u00a0Ridge?Documentation for Ridge<pre>Ridge(alpha=15)</pre> ts_window_transformer: TimeSeriesWindowTransformer<pre>TimeSeriesWindowTransformer(transformer=&lt;ceruleo.transformation.functional.transformers.Transformer object at 0x7d9ee81eac90&gt;,\n                            window_size=32)</pre> transformer: Transformer<pre>{'features': None, 'pipelineX': [('ByNameFeatureSelector', []), ('MinMaxScaler', {'Min': None, 'Max': None})], 'pipelineY': [('ByNameFeatureSelector', [])]}</pre> pipelineX: Pipeline<pre>Pipeline(final_step=MinMaxScaler(name='MinMaxScaler', range=(-1, 1)))</pre> final_step: MinMaxScaler<pre>MinMaxScaler</pre> MinMaxScaler<pre>MinMaxScaler</pre> pipelineY: Pipeline<pre>Pipeline(final_step=ByNameFeatureSelector(features=['RUL'],\n                                          name='ByNameFeatureSelector'))</pre> final_step: ByNameFeatureSelector<pre>ByNameFeatureSelector : []</pre> ByNameFeatureSelector<pre>ByNameFeatureSelector : []</pre> In\u00a0[15]: Copied! <pre>fig, ax = plt.subplots(figsize=(17, 5))\nax.plot(regressor.ts_window_transformer.true_values(test_dataset), label='True values')\nax.plot(regressor.predict(test_dataset), label='Previous estimator')\n\nax.plot(grid_search.best_estimator_.predict(test_dataset), label='Best estimator')\nax.legend()\n</pre> fig, ax = plt.subplots(figsize=(17, 5)) ax.plot(regressor.ts_window_transformer.true_values(test_dataset), label='True values') ax.plot(regressor.predict(test_dataset), label='Previous estimator')  ax.plot(grid_search.best_estimator_.predict(test_dataset), label='Best estimator') ax.legend()  Out[15]: <pre>&lt;matplotlib.legend.Legend at 0x7d9ee7f3b190&gt;</pre>"},{"location":"models/Models_sklearn/#notebook-scikit-learn-models","title":"Notebook: Scikit-learn Models\u00b6","text":""},{"location":"models/Models_sklearn/#load-the-dataset","title":"Load the dataset\u00b6","text":""},{"location":"models/Models_sklearn/#create-a-transformer-for-a-dataset","title":"Create a transformer for a dataset\u00b6","text":""},{"location":"models/Models_sklearn/#split-train-test-validation","title":"Split train-test-validation\u00b6","text":""},{"location":"models/Models_sklearn/#models","title":"Models\u00b6","text":""},{"location":"models/Models_sklearn/#scikit-learn","title":"Scikit-learn\u00b6","text":""},{"location":"models/Models_sklearn/#val-dataset-results","title":"Val dataset results\u00b6","text":""},{"location":"models/Models_sklearn/#test-dataset-results","title":"Test dataset results\u00b6","text":""},{"location":"models/Models_sklearn/#parameters-grid-search","title":"Parameters grid search\u00b6","text":""},{"location":"models/baseline/","title":"Baseline models","text":""},{"location":"models/baseline/#ceruleo.models.baseline.BaselineModel","title":"<code>BaselineModel</code>","text":"<p>Predict the RUL using the mean or the median value of the duration of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Method for computing the duration of the dataset. Possible values are: 'mean' and 'median'</p> <code>'mean'</code> Source code in <code>ceruleo/models/baseline.py</code> <pre><code>class BaselineModel:\n    \"\"\"\n    Predict the RUL using the mean or the median value of the duration of the dataset\n\n    Parameters:\n        mode: Method for computing the duration of the dataset. Possible values are: 'mean' and 'median'\n    \"\"\"\n\n    def __init__(self, mode: str = \"mean\", RUL_threshold: Optional[float] = None):\n        self.mode = mode\n        self.RUL_threshold = RUL_threshold\n\n    def fit(self, ds: Union[TransformedDataset, AbstractPDMDataset]):\n        \"\"\"Compute the mean or median RUL using the given dataset\n\n        Parameters:\n            ds:  Dataset from which obtain the true RUL\n        \"\"\"\n        true = []\n        for y in iterate_over_target(ds):\n            y = y\n            degrading_start, time = FittedLife.compute_time_feature(\n                y, self.RUL_threshold\n            )\n\n            true.append(y.iloc[0] + time[degrading_start])\n\n        if self.mode == \"mean\":\n            self.fitted_RUL = np.mean(true)\n        elif self.mode == \"median\":\n            self.fitted_RUL = np.median(true)\n\n    def predict(self, ds: TransformedDataset) -&gt; np.ndarray:\n        \"\"\"\n        Predict the whole life using the fitted values\n\n        Parameters:\n            ds: Dataset iterator from which obtain the true RUL\n\n        Returns:\n            Predicted RUL\n        \"\"\"\n        output = []\n        for y in iterate_over_target(ds):\n            _, time = FittedLife.compute_time_feature(y, self.RUL_threshold)\n            y_pred = np.clip(self.fitted_RUL - time, 0, self.fitted_RUL)\n            output.append(y_pred)\n        return np.concatenate(output)\n</code></pre>"},{"location":"models/baseline/#ceruleo.models.baseline.BaselineModel.fit","title":"<code>fit(ds)</code>","text":"<p>Compute the mean or median RUL using the given dataset</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Union[TransformedDataset, AbstractPDMDataset]</code> <p>Dataset from which obtain the true RUL</p> required Source code in <code>ceruleo/models/baseline.py</code> <pre><code>def fit(self, ds: Union[TransformedDataset, AbstractPDMDataset]):\n    \"\"\"Compute the mean or median RUL using the given dataset\n\n    Parameters:\n        ds:  Dataset from which obtain the true RUL\n    \"\"\"\n    true = []\n    for y in iterate_over_target(ds):\n        y = y\n        degrading_start, time = FittedLife.compute_time_feature(\n            y, self.RUL_threshold\n        )\n\n        true.append(y.iloc[0] + time[degrading_start])\n\n    if self.mode == \"mean\":\n        self.fitted_RUL = np.mean(true)\n    elif self.mode == \"median\":\n        self.fitted_RUL = np.median(true)\n</code></pre>"},{"location":"models/baseline/#ceruleo.models.baseline.BaselineModel.predict","title":"<code>predict(ds)</code>","text":"<p>Predict the whole life using the fitted values</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>TransformedDataset</code> <p>Dataset iterator from which obtain the true RUL</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted RUL</p> Source code in <code>ceruleo/models/baseline.py</code> <pre><code>def predict(self, ds: TransformedDataset) -&gt; np.ndarray:\n    \"\"\"\n    Predict the whole life using the fitted values\n\n    Parameters:\n        ds: Dataset iterator from which obtain the true RUL\n\n    Returns:\n        Predicted RUL\n    \"\"\"\n    output = []\n    for y in iterate_over_target(ds):\n        _, time = FittedLife.compute_time_feature(y, self.RUL_threshold)\n        y_pred = np.clip(self.fitted_RUL - time, 0, self.fitted_RUL)\n        output.append(y_pred)\n    return np.concatenate(output)\n</code></pre>"},{"location":"models/baseline/#ceruleo.models.baseline.FixedValueBaselineModel","title":"<code>FixedValueBaselineModel</code>","text":"<p>A model that predicts always  the same duration for each run-to-failure cycle</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float</code> <p>Fixed RUL</p> required Source code in <code>ceruleo/models/baseline.py</code> <pre><code>class FixedValueBaselineModel:\n    \"\"\"\n    A model that predicts always  the same duration for each run-to-failure cycle\n\n    Parameters:\n        value: Fixed RUL\n    \"\"\"\n\n    def __init__(self, *, value: float):\n        self.value = value\n\n    def fit(self, *args):\n        return self\n\n    def predict(\n        self, ds: TransformedDataset, RUL_threshold: Optional[float] = None\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Predict the whole life using the fixed values\n\n        Parameters:\n            ds: Dataset iterator from which obtain the true RUL\n\n        Returns:\n            Predicted RUL\n        \"\"\"\n        output = []\n        for y in iterate_over_target(ds):\n            _, time = FittedLife.compute_time_feature(y, RUL_threshold)\n            y_pred = np.clip(self.value - time, 0, self.value)\n            output.append(y_pred)\n        return np.concatenate(output)\n</code></pre>"},{"location":"models/baseline/#ceruleo.models.baseline.FixedValueBaselineModel.predict","title":"<code>predict(ds, RUL_threshold=None)</code>","text":"<p>Predict the whole life using the fixed values</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>TransformedDataset</code> <p>Dataset iterator from which obtain the true RUL</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted RUL</p> Source code in <code>ceruleo/models/baseline.py</code> <pre><code>def predict(\n    self, ds: TransformedDataset, RUL_threshold: Optional[float] = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Predict the whole life using the fixed values\n\n    Parameters:\n        ds: Dataset iterator from which obtain the true RUL\n\n    Returns:\n        Predicted RUL\n    \"\"\"\n    output = []\n    for y in iterate_over_target(ds):\n        _, time = FittedLife.compute_time_feature(y, RUL_threshold)\n        y_pred = np.clip(self.value - time, 0, self.value)\n        output.append(y_pred)\n    return np.concatenate(output)\n</code></pre>"},{"location":"models/models_tf/","title":"Notebooks: Baselines models","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre>import matplotlib.pyplot as plt\nimport seaborn as sbn\n\nsbn.set()\n</pre> import matplotlib.pyplot as plt import seaborn as sbn  sbn.set() In\u00a0[3]: Copied! <pre>from ceruleo.dataset.catalog.CMAPSS import CMAPSSDataset\n</pre> from ceruleo.dataset.catalog.CMAPSS import CMAPSSDataset <pre>2022-08-11 14:38:02.949070: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2022-08-11 14:38:02.951705: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2022-08-11 14:38:02.951713: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n</pre> In\u00a0[4]: Copied! <pre>from sklearn.model_selection import train_test_split\n\ndataset = CMAPSSDataset(train=True, models='FD001')\ntrain_dataset, test_dataset = train_test_split(dataset, train_size=0.8)\n</pre> from sklearn.model_selection import train_test_split  dataset = CMAPSSDataset(train=True, models='FD001') train_dataset, test_dataset = train_test_split(dataset, train_size=0.8) In\u00a0[5]: Copied! <pre>from ceruleo.transformation.functional.transformers import Transformer\nfrom ceruleo.transformation.features.selection import ByNameFeatureSelector\nfrom ceruleo.transformation.functional.pipeline.pipeline import make_pipeline\nfrom ceruleo.transformation.features.scalers import MinMaxScaler\nfrom ceruleo.dataset.catalog.CMAPSS import sensor_indices\nfrom ceruleo.iterators.utils import true_values\n\nFEATURES = [train_dataset[0].columns[i] for i in sensor_indices]\n</pre> from ceruleo.transformation.functional.transformers import Transformer from ceruleo.transformation.features.selection import ByNameFeatureSelector from ceruleo.transformation.functional.pipeline.pipeline import make_pipeline from ceruleo.transformation.features.scalers import MinMaxScaler from ceruleo.dataset.catalog.CMAPSS import sensor_indices from ceruleo.iterators.utils import true_values  FEATURES = [train_dataset[0].columns[i] for i in sensor_indices]  In\u00a0[6]: Copied! <pre>transformer = Transformer(\n    pipelineX=make_pipeline(\n        ByNameFeatureSelector(features=FEATURES), \n        MinMaxScaler(range=(-1, 1))\n\n    ), \n    pipelineY=make_pipeline(\n        ByNameFeatureSelector(features=['RUL']),  \n    )\n)\n\ntransformer.fit(train_dataset)\n\ntransformed_train_dataset = train_dataset.map(transformer)\ntransformed_test_dataset = test_dataset.map(transformer)\n</pre>  transformer = Transformer(     pipelineX=make_pipeline(         ByNameFeatureSelector(features=FEATURES),          MinMaxScaler(range=(-1, 1))      ),      pipelineY=make_pipeline(         ByNameFeatureSelector(features=['RUL']),       ) )  transformer.fit(train_dataset)  transformed_train_dataset = train_dataset.map(transformer) transformed_test_dataset = test_dataset.map(transformer) In\u00a0[7]: Copied! <pre>from ceruleo.models.baseline import BaselineModel\nmodel_mean = BaselineModel(mode='mean')\nmodel_mean.fit(transformed_train_dataset)\n\nmodel_median = BaselineModel(mode='median')\nmodel_median.fit(transformed_train_dataset)\n</pre> from ceruleo.models.baseline import BaselineModel model_mean = BaselineModel(mode='mean') model_mean.fit(transformed_train_dataset)  model_median = BaselineModel(mode='median') model_median.fit(transformed_train_dataset) <pre>&lt;class 'ceruleo.dataset.transformed.TransformedDataset'&gt;\n&lt;class 'ceruleo.dataset.transformed.TransformedDataset'&gt;\n</pre> In\u00a0[8]: Copied! <pre>from ceruleo.models.baseline import FixedValueBaselineModel\nmodel_fixed = FixedValueBaselineModel(value=100)\n</pre> from ceruleo.models.baseline import FixedValueBaselineModel model_fixed = FixedValueBaselineModel(value=100)  In\u00a0[9]: Copied! <pre>fig, ax = plt.subplots(figsize=(17, 5))\nax.plot(true_values(transformed_test_dataset), \n        label='True RUL')\nax.plot(model_mean.predict(transformed_test_dataset), \n        label='Mean baseline model')\nax.plot(model_median.predict(transformed_test_dataset), \n        label='Median baseline model')\nax.plot(model_fixed.predict(transformed_test_dataset), \n        label='Fixed model')\nax.legend()\n</pre> fig, ax = plt.subplots(figsize=(17, 5)) ax.plot(true_values(transformed_test_dataset),          label='True RUL') ax.plot(model_mean.predict(transformed_test_dataset),          label='Mean baseline model') ax.plot(model_median.predict(transformed_test_dataset),          label='Median baseline model') ax.plot(model_fixed.predict(transformed_test_dataset),          label='Fixed model') ax.legend() <pre>&lt;class 'ceruleo.dataset.transformed.TransformedDataset'&gt;\n&lt;class 'ceruleo.dataset.transformed.TransformedDataset'&gt;\n&lt;class 'ceruleo.dataset.transformed.TransformedDataset'&gt;\n&lt;class 'ceruleo.dataset.transformed.TransformedDataset'&gt;\n</pre> Out[9]: <pre>&lt;matplotlib.legend.Legend at 0x7f77869e5ea0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"models/models_tf/#notebooks-baselines-models","title":"Notebooks: Baselines models\u00b6","text":""},{"location":"models/models_tf/#load-the-dataset","title":"Load the dataset\u00b6","text":""},{"location":"models/models_tf/#create-a-transformer","title":"Create a transformer\u00b6","text":""},{"location":"models/models_tf/#baseline-model","title":"Baseline model\u00b6","text":""},{"location":"models/models_tf/#fixed-value-baseline-model","title":"Fixed Value Baseline Model\u00b6","text":""},{"location":"models/models_tf/#results","title":"Results\u00b6","text":""},{"location":"models/sklearn/","title":"Sklearn","text":"<p>This module provides interoperability </p> <p>Scikit learned models can be used wit the ceruleo Transformers</p> <p>The <code>TimeSeriesWindowTransformer</code>  is a scikit-learn transformers that takes a transformer and iterator paramaters, to build a <code>WindowedDatasetIterator</code> and generate the X and y.</p> <p>Since the X and Y are generated by one scikit-learn transformer step, <code>EstimatorWrapper</code> takes the (X,y) output of a <code>TimeSeriesWindowTransformer</code> and calls the rest of the scikit-learn pipeline spreading the (X,y)  to positional parameters.</p> <p>Finally <code>CeruleoRegressor</code>, is a class similar to the <code>sklearn.compose.TransformedTargetRegressor</code>. Takes the transformer, and a regressor scikit-learn pipeline or model, and automatically builds a scikit-learn pipeline using <code>WindowedDatasetIterator</code> and <code>EstimatorWrapper</code>.</p>"},{"location":"models/sklearn/#ceruleo.models.sklearn.CeruleoMetricWrapper","title":"<code>CeruleoMetricWrapper</code>","text":"<p>A wrapper around sklearn metrics</p> <pre><code>Example:\n'''\n    grid_search = GridSearchCV(\n        estimator=regressor_gs,\n        param_grid={\n            'regressor': [RandomForestRegressor(max_depth=5)]\n        },\n        scoring=CeruleoMetricWrapper('neg_mean_absolute_error')\n    )\n'''\n</code></pre> Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>class CeruleoMetricWrapper:\n    \"\"\"\n    A wrapper around sklearn metrics\n\n        Example:\n        '''\n            grid_search = GridSearchCV(\n                estimator=regressor_gs,\n                param_grid={\n                    'regressor': [RandomForestRegressor(max_depth=5)]\n                },\n                scoring=CeruleoMetricWrapper('neg_mean_absolute_error')\n            )\n        '''\n    \"\"\"\n\n    def __init__(self, scoring):\n        if callable(scoring):\n            self.scorers = scoring\n        elif isinstance(scoring, str):\n            self.scorers = get_scorer(scoring)\n\n    def __call__(self, estimator: CeruleoRegressor, dataset, y=None):\n        X, y = estimator.ts_window_transformer.transform(dataset)\n        return self.scorers(estimator, X, y)\n</code></pre>"},{"location":"models/sklearn/#ceruleo.models.sklearn.CeruleoRegressor","title":"<code>CeruleoRegressor</code>","text":"<p>               Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>A regressor wrapper similar to sklearn.compose.TransformedTargetRegressor</p> <p>Parameters:</p> Name Type Description Default <code>features_transformer</code> <p>The transformer</p> required <code>regressor</code> <p>A scikit-learn regressor</p> required Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>class CeruleoRegressor(RegressorMixin, BaseEstimator):\n    \"\"\"\n    A regressor wrapper similar to sklearn.compose.TransformedTargetRegressor\n\n\n    Parameters:\n        features_transformer: The transformer\n        regressor: A scikit-learn regressor\n    \"\"\"\n\n    def __init__(\n        self, ts_window_transformer: TimeSeriesWindowTransformer, regressor, **kwargs\n    ):\n        self.ts_window_transformer = ts_window_transformer\n        self.regressor = regressor\n        self._update()\n\n    def _update(self):\n        self.wrapped_regressor = EstimatorWrapper(self.regressor)\n\n        self.pipe = sk_pipeline.make_pipeline(\n            self.ts_window_transformer, self.wrapped_regressor\n        )\n\n    def fit(self, dataset: AbstractPDMDataset, y=None):\n        self.pipe.fit(dataset)\n        return self\n\n    def predict(self, data):\n        if isinstance(data, np.ndarray):\n            return self.regressor.predict(data)\n        else:\n            return self.pipe.predict(data)\n\n    def score(self, dataset, y=None):\n        X, y = self.ts_window_transformer.transform(dataset)\n        return super().score(dataset, y)\n\n    def set_params(self, **kwargs):\n        super().set_params(**kwargs)\n        self._update()\n        return self\n</code></pre>"},{"location":"models/sklearn/#ceruleo.models.sklearn.EstimatorWrapper","title":"<code>EstimatorWrapper</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Wrapper around sklearn estimators to allow calling the fit and predict</p> <p>The transformer keeps the X and y together. This wrapper divide the X,y and call the fit(X,y) and predict(X,y) of the estimator</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>BaseEstimator</code> <p>A scikit-learn estimator</p> required Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>class EstimatorWrapper(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Wrapper around sklearn estimators to allow calling the fit and predict\n\n    The transformer keeps the X and y together. This wrapper\n    divide the X,y and call the fit(X,y) and predict(X,y) of the estimator\n\n    Parameters:\n        estimator: A scikit-learn estimator\n    \"\"\"\n\n    def __init__(self, estimator: BaseEstimator):\n        self.estimator = estimator\n\n    def fit(self, Xy: Tuple[np.ndarray, np.ndarray], y=None, **fit_params):\n        X, y = Xy\n        self.estimator.fit(X, y, **fit_params)\n        return self\n\n    def predict(self, Xy, **transform_params):\n        X, y = Xy\n        return self.estimator.predict(X, **transform_params)\n</code></pre>"},{"location":"models/sklearn/#ceruleo.models.sklearn.TimeSeriesWindowTransformer","title":"<code>TimeSeriesWindowTransformer</code>","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>A scikit-learn transformer for obtaining a windowed time-series from the run-to-cycle failures</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>Transformer</code> <p>A scikit-learn transformer</p> required <code>window_size</code> <code>int</code> <p>Window size of the iterator</p> required <code>step</code> <code>int</code> <p>Stride of the iterators</p> <code>1</code> <code>horizon</code> <code>int</code> <p>Horizon of the predictions</p> <code>1</code> <code>shuffler</code> <code>AbstractShuffler</code> <p>Shuffler of the data</p> <code>NotShuffled()</code> <code>sample_weight</code> <code>SampleWeight</code> <p>Sample weights callable</p> <code>NotWeighted()</code> <code>right_closed</code> <code>bool</code> <p>Wether to include the last point in each sliding window</p> <code>True</code> <code>padding</code> <code>bool</code> <p>Wether to pad when the lookback window is not complete</p> <code>True</code> Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>class TimeSeriesWindowTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    A scikit-learn transformer for obtaining a windowed time-series from the run-to-cycle failures\n\n    Parameters:\n        transformer: A scikit-learn transformer\n        window_size: Window size of the iterator\n        step: Stride of the iterators\n        horizon: Horizon of the predictions\n        shuffler: Shuffler of the data\n        sample_weight: Sample weights callable\n        right_closed: Wether to include the last point in each sliding window\n        padding: Wether to pad when the lookback window is not complete\n    \"\"\"\n\n    def __init__(\n        self,\n        transformer: Transformer,\n        window_size: int,\n        step: int = 1,\n        horizon: int = 1,\n        shuffler: AbstractShuffler = NotShuffled(),\n        sample_weight: SampleWeight = NotWeighted(),\n        right_closed: bool = True,\n        padding: bool = True,\n    ):\n        self.transformer = transformer\n        self.window_size = window_size\n        self.horizon = horizon\n        self.step = step\n        self.shuffler = shuffler\n        self.sample_weight = sample_weight\n        self.right_closed = right_closed\n        self.padding = padding\n\n    def fit(self, dataset: AbstractPDMDataset):\n        \"\"\"\n        Fit the transformer with the given dataset\n\n        Parameters:\n            dataset: Dataset to fit the transformer\n        \"\"\"\n        self.transformer.fit(dataset)\n        return self\n\n    def _iterator(self, dataset: AbstractPDMDataset):\n        return WindowedDatasetIterator(\n            dataset.map(self.transformer),\n            self.window_size,\n            self.step,\n            self.horizon,\n            shuffler=self.shuffler,\n            sample_weight=self.sample_weight,\n            right_closed=self.right_closed,\n            padding=self.padding,\n        )\n\n    def transform(self, dataset: AbstractPDMDataset):\n        X, y, sw = self._iterator(dataset).get_data()\n        return X, y.ravel()\n\n    def true_values(self, dataset: AbstractPDMDataset):\n        X, y, sw = self._iterator(dataset).get_data()\n        return y.ravel()\n\n    def get_params(self, deep=None):\n        params = super().get_params(deep)\n        if deep:\n            params[\"ts_window_transformer__transformer\"] = self.transformer.get_params(\n                deep\n            )\n        return params\n</code></pre>"},{"location":"models/sklearn/#ceruleo.models.sklearn.TimeSeriesWindowTransformer.fit","title":"<code>fit(dataset)</code>","text":"<p>Fit the transformer with the given dataset</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>AbstractPDMDataset</code> <p>Dataset to fit the transformer</p> required Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>def fit(self, dataset: AbstractPDMDataset):\n    \"\"\"\n    Fit the transformer with the given dataset\n\n    Parameters:\n        dataset: Dataset to fit the transformer\n    \"\"\"\n    self.transformer.fit(dataset)\n    return self\n</code></pre>"},{"location":"models/sklearn/#ceruleo.models.sklearn.fit_batch","title":"<code>fit_batch(model, train_batcher, val_batcher, n_epochs=15)</code>","text":"<p>Fit the model using the given batcher</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>SKLearn Model</p> required <code>train_batcher</code> <code>Batcher</code> <p>Train dataset batcher</p> required <code>val_batcher</code> <code>Batcher</code> <p>Validation dataset batcher</p> required <code>n_epochs</code> <p>Number of epochs, by default 15</p> <code>15</code> <p>Returns:</p> Name Type Description <code>model</code> <code>Model</code> <p>the model</p> <code>history</code> <code>List</code> <p>history of errors</p> Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>def fit_batch(\n    model, train_batcher: Batcher, val_batcher: Batcher, n_epochs=15\n) -&gt; Tuple[\"Model\", List]:\n    \"\"\"\n    Fit the model using the given batcher\n\n    Parameters:\n        model: SKLearn Model\n        train_batcher: Train dataset batcher\n        val_batcher: Validation dataset batcher\n        n_epochs: Number of epochs, by default 15\n\n    Returns:\n        model: the model\n        history: history of errors\n    \"\"\"\n\n    history = []\n    for r in range(n_epochs):\n        for X, y in train_batcher:\n            model.partial_fit(np.reshape(X, (X.shape[0], X.shape[1] * X.shape[2])), y)\n        y_true, y_pred = predict(val_batcher)\n        history.append(mean_squared_error(y_true, y_pred))\n    return model, history\n</code></pre>"},{"location":"models/sklearn/#ceruleo.models.sklearn.predict","title":"<code>predict(model, dataset_iterator)</code>","text":"<p>Get the predictions for the given iterator</p> <p>Parameters:</p> Name Type Description Default <code>dataset_iterator</code> <code>WindowedDatasetIterator</code> <p>Dataset iterator from which obtain data to predict</p> required <p>Returns:</p> Type Description <p>Array with the predictiosn</p> Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>def predict(model, dataset_iterator: WindowedDatasetIterator):\n    \"\"\"\n    Get the predictions for the given iterator\n\n    Parameters:\n        dataset_iterator: Dataset iterator from which obtain data to predict\n\n    Returns:\n       Array with the predictiosn\n    \"\"\"\n    X, _, _ = dataset_iterator.get_data()\n    return model.predict(X)\n</code></pre>"},{"location":"models/sklearn/#ceruleo.models.sklearn.predict_batch","title":"<code>predict_batch(model, dataset_batcher)</code>","text":"<p>Predict the values using the given batcher</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>SKLearn model</p> required <code>dataset_batcher</code> <code>Batcher</code> <p>The batcher</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>RUL Prediction array</p> Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>def predict_batch(model, dataset_batcher: Batcher) -&gt; np.ndarray:\n    \"\"\"\n    Predict the values using the given batcher\n\n    Parameters:\n        model: SKLearn model\n        dataset_batcher: The batcher\n\n\n    Returns:\n        RUL Prediction array\n    \"\"\"\n    y_pred = []\n    for X, y in dataset_batcher:\n        y_pred.extend(\n            np.squeeze(\n                model.predict(np.reshape(X, (X.shape[0], X.shape[1] * X.shape[2])))\n            )\n        )\n    return np.squeeze(np.array(y_pred))\n</code></pre>"},{"location":"models/sklearn/#ceruleo.models.sklearn.train_model","title":"<code>train_model(model, train_iterator, val_windowed_iterator=None, **fit_kwargs)</code>","text":"<p>Fit the model with the given dataset iterator Parameters:     train_iterator: Training Iterator</p> <p>Other Parameters:</p> Name Type Description <code>fit_kwargs</code> <p>Arguments for the fit method</p> <p>Returns:</p> Type Description <p>A SKLearn model</p> Source code in <code>ceruleo/models/sklearn.py</code> <pre><code>def train_model(\n    model,\n    train_iterator: WindowedDatasetIterator,\n    val_windowed_iterator: Optional[WindowedDatasetIterator] = None,\n    **fit_kwargs,\n):\n    \"\"\"\n    Fit the model with the given dataset iterator\n    Parameters:\n        train_iterator: Training Iterator\n\n    Keyword arguments:\n        fit_kwargs: Arguments for the fit method\n\n    Returns:\n        A SKLearn model\n    \"\"\"\n    X, y, sample_weight = train_iterator.get_data()\n\n    params = {}\n    try:\n        from xgboost import XGBRegressor\n\n        if val_windowed_iterator is not None and isinstance(model, XGBRegressor):\n            X_val, y_val, _ = val_windowed_iterator.get_data()\n            fit_kwargs.update({\"eval_set\": [(X_val, y_val)]})\n    except Exception as e:\n        logger.error(e)\n\n    return model.fit(X, y.ravel(), **fit_kwargs, sample_weight=sample_weight)\n</code></pre>"},{"location":"models/keras/","title":"Reference","text":""},{"location":"models/keras/#dataset","title":"Dataset","text":""},{"location":"models/keras/#ceruleo.models.keras.dataset.tf_autoencoder_dataset","title":"<code>tf_autoencoder_dataset(iterator)</code>","text":"<p>Create an autoencoder tf.data.Dataset from the iterator</p> <p>The dataset is constructed from a generator</p> <p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>WindowedDatasetIterator</code> <p>The data iterator</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>A tensorflow dataset</p> Source code in <code>ceruleo/models/keras/dataset.py</code> <pre><code>def tf_autoencoder_dataset(iterator: WindowedDatasetIterator) -&gt; tf.data.Dataset:\n    \"\"\"\n    Create an autoencoder tf.data.Dataset from the iterator\n\n    The dataset is constructed from a generator\n\n    Parameters:\n        iterator: The data iterator\n\n    Returns:\n        A tensorflow dataset\n    \"\"\"\n    n_features = iterator.n_features\n\n    def gen_train():\n        for X, y, sw in iterator:\n            yield X, X, sw\n\n    a = tf.data.Dataset.from_generator(\n        gen_train,\n        (tf.float32, tf.float32, tf.float32),\n        (\n            tf.TensorShape([iterator.window_size, n_features]),\n            tf.TensorShape([iterator.window_size, n_features]),\n            tf.TensorShape([1]),\n        ),\n    )\n\n    return a\n</code></pre>"},{"location":"models/keras/#ceruleo.models.keras.dataset.tf_regression_dataset","title":"<code>tf_regression_dataset(iterator)</code>","text":"<p>Create a forecast tf.data.Dataset from the iterator</p> <p>The dataset is constructed from a generator</p> <p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>WindowedDatasetIterator</code> <p>The data iterator</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>A tensorflow dataset</p> Source code in <code>ceruleo/models/keras/dataset.py</code> <pre><code>def tf_regression_dataset(iterator: WindowedDatasetIterator) -&gt; tf.data.Dataset:\n    \"\"\"\n    Create a forecast tf.data.Dataset from the iterator\n\n    The dataset is constructed from a generator\n\n    Parameters:\n        iterator: The data iterator\n\n    Returns:\n        A tensorflow dataset\n    \"\"\"\n    n_features = iterator.n_features\n\n    def generator_function():\n        for X, y, sw in iterator:\n            yield X, y, sw\n\n    a = tf.data.Dataset.from_generator(\n        generator_function,\n        output_signature=(\n            tf.TensorSpec(shape=(iterator.window_size, n_features), dtype=tf.float32),\n            tf.TensorSpec(shape=(iterator.horizon, 1), dtype=tf.float32),\n            tf.TensorSpec(shape=(1), dtype=tf.float32),\n        ),\n    )\n\n    return a\n</code></pre>"},{"location":"models/keras/#ceruleo.models.keras.dataset.tf_seq_to_seq_dataset","title":"<code>tf_seq_to_seq_dataset(iterator)</code>","text":"<p>Create a sequence to sequence tf.data.Dataset from the iterator</p> <p>The dataset is constructed from a generator</p> <p>Parameters:</p> Name Type Description Default <code>iterator</code> <code>WindowedDatasetIterator</code> <p>The data iterator</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>A tensorflow dataset</p> Source code in <code>ceruleo/models/keras/dataset.py</code> <pre><code>def tf_seq_to_seq_dataset(iterator: WindowedDatasetIterator) -&gt; tf.data.Dataset:\n    \"\"\"\n    Create a sequence to sequence tf.data.Dataset from the iterator\n\n    The dataset is constructed from a generator\n\n    Parameters:\n        iterator: The data iterator\n\n    Returns:\n        A tensorflow dataset\n    \"\"\"\n    n_features = iterator.n_features\n\n    def generator_function():\n        for X, y, sw in iterator:\n            yield X, y, sw\n\n    a = tf.data.Dataset.from_generator(\n        generator_function,\n        output_signature=(\n            tf.TensorSpec(shape=(iterator.window_size, n_features), dtype=tf.float32),\n            tf.TensorSpec(\n                shape=(iterator.window_size, iterator.horizon), dtype=tf.float32\n            ),\n            tf.TensorSpec(shape=(1), dtype=tf.float32),\n        ),\n    )\n\n    return a\n</code></pre>"},{"location":"models/keras/#callbacks","title":"Callbacks","text":""},{"location":"models/keras/#ceruleo.models.keras.callbacks.PredictionCallback","title":"<code>PredictionCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Generate a plot after each epoch with the predictions</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model used predict</p> required <code>output_path</code> <code>Path</code> <p>Path of the output image</p> required <code>dataset</code> <code>Dataset</code> <p>The dataset that want to be plotted</p> required Source code in <code>ceruleo/models/keras/callbacks.py</code> <pre><code>class PredictionCallback(Callback):\n    \"\"\"\n    Generate a plot after each epoch with the predictions\n\n    Parameters:\n        model: The model used predict\n        output_path: Path of the output image\n        dataset: The dataset that want to be plotted\n    \"\"\"\n\n    def __init__(\n        self,\n        output_path: Path,\n        dataset: tf.data.Dataset,\n        units: str = \"\",\n        filename_suffix: str = \"\",\n    ):\n        super().__init__()\n        self.output_path = output_path\n        self.dataset = dataset\n        self.units = units\n        self.suffix = filename_suffix\n        if len(filename_suffix) &gt; 0:\n            self.output_path = self.output_path.with_stem(\n                filename_suffix + \"_\" + self.output_path.stem\n            )\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.dataset)\n        y_true = true_values(self.dataset)\n        ax = plot_predictions(\n            PredictionResult(\"Model\", y_true, y_pred),\n            figsize=(17, 5),\n            units=self.units,\n        )\n        ax.legend()\n\n        ax.figure.savefig(self.output_path, dpi=ax.figure.dpi)\n\n        plt.close(ax.figure)\n</code></pre>"},{"location":"models/keras/#losses","title":"Losses","text":""},{"location":"models/keras/#ceruleo.models.keras.losses.AsymmetricLossPM","title":"<code>AsymmetricLossPM</code>","text":"<p>               Bases: <code>LossFunctionWrapper</code></p> <p>Customizable Asymmetric Loss Functions for Machine Learning-based Predictive Maintenance</p> <p>Ehrig, L., Atzberger, D., Hagedorn, B., Klimke, J., &amp; D\u00f6llner, J. (2020, October). Customizable Asymmetric Loss Functions for Machine Learning-based Predictive Maintenance. In 2020 8th International Conference on Condition Monitoring and Diagnosis (CMD) (pp. 250-253). IEEE.</p> <p>Reference</p> <p>Parameters:</p> Name Type Description Default <code>theta_l</code> <code>float</code> <p>Linear to exponential change point for overpredictions</p> required <code>alpha_l</code> <code>float</code> <p>Quadratic term parameters for overpredictions</p> required <code>gamma_l</code> <code>float</code> <p>Exponential term parameters for overpredictions</p> required <code>theta_r</code> <code>float</code> <p>Linear to exponential change point for underpredictions</p> required <code>alpha_r</code> <code>float</code> <p>Quadratic term parameters for underpredictions</p> required <code>gamma_r</code> <code>float</code> <p>Exponential term parameters for underpredictions</p> required <code>relative_weight</code> <code>bool</code> <p>Wether to use weigthing relative to the RUL</p> <code>True</code> Source code in <code>ceruleo/models/keras/losses.py</code> <pre><code>class AsymmetricLossPM(LossFunctionWrapper):\n    \"\"\"\n    Customizable Asymmetric Loss Functions for Machine Learning-based Predictive Maintenance\n\n\n    Ehrig, L., Atzberger, D., Hagedorn, B., Klimke, J., &amp; D\u00f6llner, J. (2020, October).\n    Customizable Asymmetric Loss Functions for Machine Learning-based Predictive Maintenance.\n    In 2020 8th International Conference on Condition Monitoring and Diagnosis\n    (CMD) (pp. 250-253). IEEE.\n\n    [Reference](https://ieeexplore.ieee.org/document/9287246)\n\n    Parameters:\n        theta_l: Linear to exponential change point for overpredictions\n        alpha_l: Quadratic term parameters for overpredictions\n        gamma_l: Exponential term parameters for overpredictions\n        theta_r: Linear to exponential change point for underpredictions\n        alpha_r: Quadratic term parameters for underpredictions\n        gamma_r: Exponential term parameters for underpredictions\n        relative_weight: Wether to use weigthing relative to the RUL\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        theta_l: float,\n        alpha_l: float,\n        gamma_l: float,\n        theta_r: float,\n        alpha_r: float,\n        gamma_r: float,\n        relative_weight: bool = True,\n        name=\"asymmetric_loss_pm\",\n    ):\n        super().__init__(\n            asymmetric_loss_pm,\n            theta_l=theta_l,\n            alpha_l=alpha_l,\n            gamma_l=gamma_l,\n            theta_r=theta_r,\n            alpha_r=alpha_r,\n            gamma_r=gamma_r,\n            relative_weight=relative_weight,\n            name=name,\n        )\n</code></pre>"},{"location":"models/keras/#ceruleo.models.keras.losses.asymmetric_loss_pm","title":"<code>asymmetric_loss_pm(y_true, y_pred, *, theta_l, alpha_l, gamma_l, theta_r, alpha_r, gamma_r, relative_weight=True)</code>","text":"<p>Customizable Asymmetric Loss Functions for Machine Learning-based Predictive Maintenance</p> <p>Ehrig, L., Atzberger, D., Hagedorn, B., Klimke, J., &amp; D\u00f6llner, J. (2020, October). Customizable Asymmetric Loss Functions for Machine Learning-based Predictive Maintenance. In 2020 8th International Conference on Condition Monitoring and Diagnosis (CMD) (pp. 250-253). IEEE.</p> <p>Reference</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>True RUL values</p> required <code>y_pred</code> <p>Predicted RUL values</p> required <code>theta_l</code> <p>Linear to exponential change point for overpredictions (Positive)</p> required <code>alpha_l</code> <p>Quadratic term parameters for overpredictions</p> required <code>gamma_l</code> <p>Exponential term parameters for overpredictions</p> required <code>theta_r</code> <p>Linear to exponential change point for underpredictions</p> required <code>alpha_r</code> <p>Quadratic term parameters for underpredictions</p> required <code>gamma_r</code> <p>Exponential term parameters for underpredictions</p> required <code>relative_weight</code> <code>bool</code> <p>Wether to use weigthing relative to the RUL</p> <code>True</code> <p>Returns:</p> Name Type Description <code>l</code> <code>float</code> <p>the loss computed</p> Source code in <code>ceruleo/models/keras/losses.py</code> <pre><code>def asymmetric_loss_pm(\n    y_true,\n    y_pred,\n    *,\n    theta_l,\n    alpha_l,\n    gamma_l,\n    theta_r,\n    alpha_r,\n    gamma_r,\n    relative_weight: bool = True,\n) -&gt; float:\n    \"\"\"\n    Customizable Asymmetric Loss Functions for Machine Learning-based Predictive Maintenance\n\n\n    Ehrig, L., Atzberger, D., Hagedorn, B., Klimke, J., &amp; D\u00f6llner, J. (2020, October).\n    Customizable Asymmetric Loss Functions for Machine Learning-based Predictive Maintenance.\n    In 2020 8th International Conference on Condition Monitoring and Diagnosis\n    (CMD) (pp. 250-253). IEEE.\n\n    [Reference](https://ieeexplore.ieee.org/document/9287246)\n\n    Parameters:\n        y_true: True RUL values\n        y_pred: Predicted RUL values\n        theta_l: Linear to exponential change point for overpredictions (Positive)\n        alpha_l: Quadratic term parameters for overpredictions\n        gamma_l: Exponential term parameters for overpredictions\n        theta_r: Linear to exponential change point for underpredictions\n        alpha_r: Quadratic term parameters for underpredictions\n        gamma_r: Exponential term parameters for underpredictions\n        relative_weight: Wether to use weigthing relative to the RUL\n\n    Returns:\n        l: the loss computed\n    \"\"\"\n\n    errors = y_true - y_pred\n    weight = tf.abs(errors) / (\n        tf.clip_by_value(y_true, clip_value_min=0.9, clip_value_max=np.inf)\n    )\n\n    ll_exp = tf.cast(K.less(errors, -theta_l), errors.dtype)\n    ll_quad = (1 - ll_exp) * tf.cast(K.less(errors, 0), errors.dtype)\n\n    lr_exp = tf.cast(K.greater(errors, theta_r), errors.dtype)\n    lr_quad = (1 - lr_exp) * tf.cast(K.greater(errors, 0), errors.dtype)\n    ll_exp = ll_exp * (\n        alpha_l\n        * theta_l\n        * (theta_l + 2 * gamma_l * (K.exp((K.abs(errors) - theta_l) / (gamma_l)) - 1))\n    )\n    ll_quad = ll_quad * alpha_l * K.pow(errors, 2)\n\n    lr_exp = lr_exp * (\n        alpha_r\n        * theta_r\n        * (theta_r + 2 * gamma_r * (K.exp((errors - theta_r) / (gamma_r)) - 1))\n    )\n    lr_quad = lr_quad * alpha_r * K.pow(errors, 2)\n\n    if relative_weight:\n        a = tf.reduce_mean(weight * (ll_exp + ll_quad + lr_exp + lr_quad))\n    else:\n        a = tf.reduce_mean(ll_exp + ll_quad + lr_exp + lr_quad)\n\n    return a\n</code></pre>"},{"location":"models/keras/#ceruleo.models.keras.losses.relative_mae","title":"<code>relative_mae(C=0.9)</code>","text":"<p>MAE weighted by the relative error</p> <p>Parameters:</p> Name Type Description Default <code>C</code> <code>float</code> <p>Minimal value for the RUL</p> <code>0.9</code> <p>Returns:</p> Type Description <p>The loss function</p> Source code in <code>ceruleo/models/keras/losses.py</code> <pre><code>def relative_mae(C: float = 0.9):\n    \"\"\"\n    MAE weighted by the relative error\n\n    Parameters:\n        C: Minimal value for the RUL\n\n    Returns:\n        The loss function\n    \"\"\"\n    mae = tf.keras.losses.MeanAbsoluteError()\n\n    def concrete_relative_mae(y_true, y_pred):\n        errors = y_true - y_pred\n        sw = tf.abs(errors) / (\n            tf.clip_by_value(y_true, clip_value_min=C, clip_value_max=np.inf)\n        )\n        return mae(y_true, y_pred, sample_weight=sw)\n\n    return concrete_relative_mae\n</code></pre>"},{"location":"models/keras/#ceruleo.models.keras.losses.relative_mse","title":"<code>relative_mse(C=0.9)</code>","text":"<p>MSE weighted by the relative error</p> <p>Parameters:</p> Name Type Description Default <code>C</code> <code>float</code> <p>Minimal value for the RUL</p> <code>0.9</code> <p>Returns:</p> Type Description <p>The loss function</p> Source code in <code>ceruleo/models/keras/losses.py</code> <pre><code>def relative_mse(C: float = 0.9):\n    \"\"\"\n    MSE weighted by the relative error\n\n    Parameters:\n        C: Minimal value for the RUL\n\n    Returns:\n        The loss function\n    \"\"\"\n    mse = tf.keras.losses.MeanSquaredError()\n\n    def concrete_relative_mse(y_true, y_pred):\n        errors = y_true - y_pred\n        sw = tf.abs(errors) / (\n            tf.clip_by_value(y_true, clip_value_min=C, clip_value_max=np.inf)\n        )\n        return mse(y_true, y_pred, sample_weight=sw)\n\n    return concrete_relative_mse\n</code></pre>"},{"location":"models/keras/#ceruleo.models.keras.losses.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_pred)</code>","text":"<p>Root mean squared error</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array</code> <p>True RUL values</p> required <code>y_pred</code> <code>array</code> <p>Predicted RUL values</p> required Source code in <code>ceruleo/models/keras/losses.py</code> <pre><code>def root_mean_squared_error(y_true: np.array, y_pred: np.array) -&gt; float:\n    \"\"\"Root mean squared error\n\n    Parameters:\n            y_true: True RUL values\n            y_pred: Predicted RUL values\n    \"\"\"\n    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=0))\n</code></pre>"},{"location":"models/keras/#layers","title":"Layers","text":""},{"location":"models/keras/#ceruleo.models.keras.layers.ConcreteDropout","title":"<code>ConcreteDropout</code>","text":"<p>               Bases: <code>Layer</code></p> <p>Concrete Dropout layer class from https://arxiv.org/abs/1705.07832. Dropout Feature Ranking for Deep Learning Models Chun-Hao Chang Ladislav Rampasek Anna Goldenberg</p> <p>Parameters:</p> Name Type Description Default <code>dropout_regularizer</code> <p>Positive float, satisfying $dropout_regularizer = 2 / (    au * N)$ with model precision $      au$ (inverse observation noise) and N the number of instances in the dataset. The factor of two should be ignored for cross-entropy loss, and used only for the eucledian loss.</p> <code>1e-05</code> <code>init_min</code> <p>Minimum value for the randomly initialized dropout rate, in [0, 1].</p> <code>0.1</code> <code>init_min</code> <p>Maximum value for the randomly initialized dropout rate, in [0, 1], with init_min &lt;= init_max.</p> <code>0.1</code> <code>name</code> <p>String, name of the layer.</p> <code>None</code> Source code in <code>ceruleo/models/keras/layers.py</code> <pre><code>class ConcreteDropout(Layer):\n    \"\"\"\n    Concrete Dropout layer class from https://arxiv.org/abs/1705.07832.\n    Dropout Feature Ranking for Deep Learning Models\n    Chun-Hao Chang\n    Ladislav Rampasek\n    Anna Goldenberg\n\n    Parameters:\n        dropout_regularizer: Positive float, satisfying $dropout_regularizer = 2 / (\\tau * N)$\n            with model precision $\\tau$ (inverse observation noise) and\n            N the number of instances in the dataset.\n            The factor of two should be ignored for cross-entropy loss,\n            and used only for the eucledian loss.\n        init_min: Minimum value for the randomly initialized dropout rate, in [0, 1].\n        init_min: Maximum value for the randomly initialized dropout rate, in [0, 1],\n            with init_min &lt;= init_max.\n        name: String, name of the layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dropout_regularizer=1e-5,\n        init_min=0.1,\n        init_max=0.9,\n        name=None,\n        training=True,\n        **kwargs,\n    ):\n        super(ConcreteDropout, self).__init__(name=name, **kwargs)\n        assert init_min &lt;= init_max, \"init_min must be lower or equal to init_max.\"\n\n        self.dropout_regularizer = dropout_regularizer\n\n        self.p_logit = None\n        self.p = None\n        self.init_min = np.log(init_min) - np.log(1.0 - init_min)\n        self.init_max = np.log(init_max) - np.log(1.0 - init_max)\n        self.training = training\n\n    def build(self, input_shape):\n        self.window = input_shape[-2]\n        self.number_of_features = input_shape[-1]\n        input_shape = tensor_shape.TensorShape(input_shape)\n\n        self.p_logit = self.add_weight(\n            name=\"p_logit\",\n            shape=[self.number_of_features],\n            initializer=tf.random_uniform_initializer(self.init_min, self.init_max),\n            dtype=tf.float32,\n            trainable=True,\n        )\n\n    def concrete_dropout(self, p, x):\n        eps = K.cast_to_floatx(K.epsilon())\n        temp = 1.0 / 10.0\n        unif_noise = K.random_uniform(shape=[self.number_of_features])\n        drop_prob = (\n            K.log(p + eps)\n            - K.log(1.0 - p + eps)\n            + K.log(unif_noise + eps)\n            - K.log(1.0 - unif_noise + eps)\n        )\n        drop_prob = K.sigmoid(drop_prob / temp)\n        random_tensor = 1.0 - drop_prob\n\n        retain_prob = 1.0 - p\n        x *= random_tensor\n        x /= retain_prob\n        return x\n\n    def call(self, inputs, training=True):\n        p = K.sigmoid(self.p_logit)\n\n        dropout_regularizer = p * K.log(p)\n        dropout_regularizer += (1.0 - p) * K.log(1.0 - p)\n        dropout_regularizer *= self.dropout_regularizer * self.number_of_features\n        regularizer = K.sum(dropout_regularizer)\n        self.add_loss(regularizer)\n\n        x = self.concrete_dropout(p, inputs)\n\n        return x\n</code></pre>"},{"location":"models/keras/#ceruleo.models.keras.layers.LASSOLayer","title":"<code>LASSOLayer</code>","text":"<p>               Bases: <code>Layer</code></p> <p>LASSO Layer</p> <p>Parameters:</p> Name Type Description Default <code>l1</code> <code>float</code> <p>L1 regularization parameter</p> required Source code in <code>ceruleo/models/keras/layers.py</code> <pre><code>class LASSOLayer(Layer):\n    \"\"\"\n    LASSO Layer\n\n    Parameters:\n        l1: L1 regularization parameter\n    \"\"\"\n\n    def __init__(self, l1: float):\n        super(LASSOLayer, self).__init__()\n        self.l1 = l1\n        self.kernel_regularizer = regularizers.L1(l1)\n\n    def build(self, input_shape):\n        W_size = np.prod(input_shape[1:])\n        self.w = self.add_weight(\n            shape=(W_size,),\n            initializer=\"random_normal\",\n            trainable=True,\n            regularizer=self.kernel_regularizer,\n            constraint=ZeroWeights(self.l1),\n        )\n\n        self.input_reshape = Reshape((W_size,))\n        self.output_reshape = Reshape(input_shape[1:])\n\n    def call(self, inputs):\n        x = self.input_reshape(inputs)\n\n        x = tf.math.multiply(self.w, x)\n\n        self.add_metric(\n            tf.math.reduce_sum(tf.cast(tf.abs(self.w) &gt; 0, tf.float32)),\n            name=\"Number of features\",\n        )\n        return self.output_reshape(x)\n</code></pre>"},{"location":"models/keras/#ceruleo.models.keras.layers.ResidualShrinkageBlock","title":"<code>ResidualShrinkageBlock</code>","text":"<p>               Bases: <code>Layer</code></p> <p>ResidualShrinkageBlock</p> Source code in <code>ceruleo/models/keras/layers.py</code> <pre><code>class ResidualShrinkageBlock(Layer):\n    \"\"\"\n    ResidualShrinkageBlock\n\n    \"\"\"\n\n    def build(self, input_shape):\n        self.blocks = []\n        for i in range(2):\n            self.blocks.append(\n                Sequential(\n                    [\n                        BatchNormalization(),\n                        Activation(\"relu\"),\n                        Conv2D(1, (1, 1), padding=\"same\"),\n                    ]\n                )\n            )\n\n        self.abs = Lambda(lambda x: tf.abs(x))\n        self.abs_mean = Sequential(\n            [\n                RemoveDimension(3),\n                Permute((1, 2)),\n                GlobalAveragePooling1D(),\n                ExpandDimension(2),\n                ExpandDimension(3),\n            ]\n        )\n        self.shrinkage = Sequential(\n            [\n                Flatten(),\n                Dense(input_shape[-1]),\n                BatchNormalization(),\n                Activation(\"relu\"),\n                Dense(\n                    input_shape[-1], kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n                ),\n                Activation(\"sigmoid\"),\n                ExpandDimension(2),\n                ExpandDimension(3),\n            ]\n        )\n\n    def call(self, inputs):\n        x = ExpandDimension()(inputs)\n\n        x = self.blocks[0](x)\n\n        residual = self.blocks[1](x)\n\n        residual_abs = self.abs(residual)\n\n        abs_mean = self.abs_mean(residual_abs)\n\n        scales = self.shrinkage(abs_mean)\n        thres = abs_mean * scales\n        thres = Permute((2, 1, 3))(thres)\n        sub = (residual_abs) - thres\n\n        zeros = sub - sub\n\n        n_sub = maximum([sub, zeros])\n\n        residual = ops.sign(residual) * n_sub\n        residual = RemoveDimension(3)(residual)\n        return residual + inputs\n</code></pre>"},{"location":"models/keras/catalog/models/","title":"Keras models","text":""},{"location":"models/keras/catalog/models/#ceruleo.models.keras.catalog.CNLSTM.CNLSTM","title":"<code>CNLSTM(input_shape, *, n_conv_layers, initial_convolutional_size, layers_recurrent, hidden_size, dense_layer_size=50, dropout=0.1)</code>","text":"<p>The network contains stacked layers of 1-dimensional convolutional layers followed by max poolings</p> <ul> <li>Temporal Convolutional Memory Networks forRemaining Useful Life Estimation of Industrial Machinery</li> <li>Lahiru Jayasinghe, Tharaka Samarasinghe, Chau Yuen, Jenny Chen Ni Low, Shuzhi Sam Ge</li> </ul> <p>Reference</p> <p>Parameters:</p> <pre><code>input_shape: Input shape of the iterator\nn_conv_layers: Number of convolutional layers. Each convolutional layers is composed by:\n    a 1D-convolution:  kernelsize=2, strides=1,padding=same, activation=ReLu\n    and a 1D-max-pooling   poolsize=2, strides=2, padding=same\ninitial_convolutional_size: The number of filters of the first convolutional layers.\n      Next ones will have the power of 2 of the previous one\nlayers_recurrent: Number of current layers. Each recurrent layer is composed by an LSTM layer\nhidden_size: After the convolutional layers the signal is projected via a RELU layer and then\n             reshaped again as a time series of size (hidden_size[0], hidden_size[1])\ndropout: Droput factor\ndense_layer_size: Size of the dense layer before the head\n</code></pre> Source code in <code>ceruleo/models/keras/catalog/CNLSTM.py</code> <pre><code>def CNLSTM(\n    input_shape: Tuple[int, int],\n    *,\n    n_conv_layers: int,\n    initial_convolutional_size: int,\n    layers_recurrent: List[int],\n    hidden_size: Tuple[int, int],\n    dense_layer_size: int = 50,\n    dropout: float = 0.1,\n):\n    \"\"\"\n    The network contains stacked layers of 1-dimensional convolutional layers\n    followed by max poolings\n\n    * Temporal Convolutional Memory Networks forRemaining Useful Life Estimation of Industrial Machinery\n    * Lahiru Jayasinghe, Tharaka Samarasinghe, Chau Yuen, Jenny Chen Ni Low, Shuzhi Sam Ge\n\n    [Reference](https://ieeexplore.ieee.org/abstract/document/8754956?casa_token=B_YvavFGulsAAAAA:f2k2I8pH1lM3sOcSGlXEF29seYPK1GPa9Od2-TwnhNeFyWvRRUAqkUOdWUNIyy9FPJHhsGM)\n\n    Parameters:\n\n        input_shape: Input shape of the iterator\n        n_conv_layers: Number of convolutional layers. Each convolutional layers is composed by:\n            a 1D-convolution:  kernelsize=2, strides=1,padding=same, activation=ReLu\n            and a 1D-max-pooling   poolsize=2, strides=2, padding=same\n        initial_convolutional_size: The number of filters of the first convolutional layers.\n              Next ones will have the power of 2 of the previous one\n        layers_recurrent: Number of current layers. Each recurrent layer is composed by an LSTM layer\n        hidden_size: After the convolutional layers the signal is projected via a RELU layer and then\n                     reshaped again as a time series of size (hidden_size[0], hidden_size[1])\n        dropout: Droput factor\n        dense_layer_size: Size of the dense layer before the head\n    \"\"\"\n\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n\n    for n_filters in range(n_conv_layers):\n        model.add(\n            Conv1D(\n                filters=initial_convolutional_size,\n                strides=1,\n                kernel_size=2,\n                padding=\"same\",\n                activation=\"relu\",\n            )\n        )\n        model.add(MaxPool1D(pool_size=2, strides=2))\n        initial_convolutional_size = initial_convolutional_size * 2\n\n    model.add(Flatten())\n    model.add(Dense(hidden_size[0] * hidden_size[1], activation=\"relu\"))\n    model.add(Dropout(dropout))\n    model.add(Reshape(hidden_size))\n\n    for i, n_filters in enumerate(layers_recurrent):\n\n        model.add(\n            LSTM(\n                n_filters, return_sequences=i &lt; len(layers_recurrent) - 1\n            )\n        )\n\n    model.add(Dropout(dropout))\n\n    model.add(Flatten())\n    model.add(Dense(dense_layer_size, activation=\"relu\"))\n    model.add(Dropout(dropout))\n    model.add(Dense(1, activation=\"relu\"))\n    return model\n</code></pre>"},{"location":"models/keras/catalog/models/#ceruleo.models.keras.catalog.InceptionTime.InceptionTime","title":"<code>InceptionTime(input_shape, *, nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, bottleneck_size=32, inception_number=3)</code>","text":"<p>InceptionTime</p> <p>Ismail Fawaz, H., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... &amp; Petitjean, F. (2020). Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery, 34(6), 1936-1962.</p> <p>Reference</p> <p>Parameters:</p> <pre><code>input_shape: Input shape\nnb_filters: number of fiters\nuse_residual: Wether to use residual connections\nuse_bottleneck : bool, optional\n    _description_, by default True\ndepth: max_depth\nkernel_size: kernel size\nbottleneck_size: bottleneck size\ninception_number: iception number\n</code></pre> <p>Returns:</p> <pre><code>model: Model\n</code></pre> Source code in <code>ceruleo/models/keras/catalog/InceptionTime.py</code> <pre><code>def InceptionTime(\n    input_shape: Tuple[int, int],\n    *,\n    nb_filters: int = 32,\n    use_residual: bool = True,\n    use_bottleneck: bool = True,\n    depth: int = 6,\n    kernel_size: int = 41,\n    bottleneck_size: int = 32,\n    inception_number: int = 3,\n) -&gt; tf.keras.Model:\n    \"\"\"InceptionTime\n\n\n    Ismail Fawaz, H., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... &amp; Petitjean, F. (2020).\n    Inceptiontime: Finding alexnet for time series classification.\n    Data Mining and Knowledge Discovery, 34(6), 1936-1962.\n\n    [Reference](https://link.springer.com/article/10.1007/s10618-020-00710-y)\n\n\n    Parameters:\n\n        input_shape: Input shape\n        nb_filters: number of fiters\n        use_residual: Wether to use residual connections\n        use_bottleneck : bool, optional\n            _description_, by default True\n        depth: max_depth\n        kernel_size: kernel size\n        bottleneck_size: bottleneck size\n        inception_number: iception number\n\n    Returns:\n\n        model: Model\n    \"\"\"\n\n    input = Input(shape=input_shape)\n\n    x = input\n    input_res = input\n\n    for d in range(depth):\n\n        x = _inception_module(\n            x,\n            use_bottleneck,\n            bottleneck_size,\n            kernel_size,\n            inception_number,\n            nb_filters,\n        )\n\n        if use_residual and d % 3 == 2:\n            x = _shortcut_layer(input_res, x)\n            input_res = x\n\n    gap_layer = GlobalAveragePooling1D()(x)\n\n    output_layer = Dense(1, activation=\"relu\")(gap_layer)\n\n    model = Model(inputs=input, outputs=output_layer)\n\n    return model\n</code></pre>"},{"location":"models/keras/catalog/models/#ceruleo.models.keras.catalog.MSWRLRCN.MSWRLRCN","title":"<code>MSWRLRCN(input_shape)</code>","text":"A new deep learning approach to remaining useful life estimation of bearings <p>Yongyi Chen, Dan Zhang, Wen-an Zhang</p> <p>Reference</p> Source code in <code>ceruleo/models/keras/catalog/MSWRLRCN.py</code> <pre><code>def MSWRLRCN(input_shape: Tuple[int, int]):\n    \"\"\"MSWR-LRCN: A new deep learning approach to remaining useful life estimation of bearings\n        Yongyi Chen, Dan Zhang, Wen-an Zhang\n\n    [Reference](https://doi.org/10.1016/j.conengprac.2021.104969)\n    \"\"\"\n\n    def ConvBlock(n_filters: int, kernel_size: int):\n        return Sequential(\n            [\n                Conv1D(n_filters, kernel_size, padding='same'),\n                BatchNormalization(),\n                ReLU(),\n                MaxPooling1D(2),\n            ]\n        )\n\n    input = Input(input_shape)\n    x = input\n    x1 = ConvBlock(input_shape[-1], 16)(x)\n    x = ConvBlock(input_shape[-1], 32)(x1)\n    x = ResidualShrinkageBlock()(x)\n    x = GlobalAveragePooling1D()(x)\n\n    x1 = Bidirectional(LSTM(20))(x1)\n    x = Concatenate()([x1, x])\n    x = ExpandDimension(2)(x)\n\n    x = Bidirectional(LSTM(20, return_sequences=True))(x)\n    x = tf.keras.activations.softsign(x)\n    x = Bidirectional(LSTM(20, return_sequences=True))(x)\n    x = tf.keras.activations.softsign(x)\n    x = Bidirectional(LSTM(20))(x)\n    x = tf.keras.activations.softsign(x)\n    x = Dense(64, activation='relu' )(x)\n    x = Dense(1, activation='relu' )(x)\n    return Model(inputs=[input], outputs=[x])\n</code></pre>"},{"location":"models/keras/catalog/models/#ceruleo.models.keras.catalog.MultiScaleConvolutional.MultiScaleConvolutionalModel","title":"<code>MultiScaleConvolutionalModel(input_shape, *, n_msblocks, scales, n_hidden, l2=0.5, dropout=0.5, activation='relu')</code>","text":"<p>Remaining useful life prediction using multi-scale deep convolutional neural network</p> <p>Li, H., Zhao, W., Zhang, Y., &amp; Zio, E. (2020).  Remaining useful life prediction using multi-scale deep convolutional neural network.  Applied Soft Computing, 89, 106113.</p> <p>(Reference)[https://www.sciencedirect.com/science/article/pii/S1568494620300533?casa_token=wp27UPxwVTIAAAAA:e_fMuKsvfQf8VZ7DgsXKCi6mpnrcx2hI0tbfe5xrLRhxmc2vaR-uW_Qq23v5yBqziBoSM5gu]</p> <p>Parameters:</p> <pre><code>input_shape: Input shape of the iterator\nn_msblocks: Number of scale blocks\nscales: List of scales\nn_hidden: Hidden convolutional dimensions\nl2: regularization\ndropout: Dropout factor\nactivation\n</code></pre> Source code in <code>ceruleo/models/keras/catalog/MultiScaleConvolutional.py</code> <pre><code>def MultiScaleConvolutionalModel(\n    input_shape: Tuple[int, int],\n    *,\n    n_msblocks: int,\n    scales: list,\n    n_hidden: int,\n    l2: float = 0.5,\n    dropout: float = 0.5,\n    activation: str = 'relu'\n):\n    \"\"\"Remaining useful life prediction using multi-scale deep convolutional neural network\n\n\n    Li, H., Zhao, W., Zhang, Y., &amp; Zio, E. (2020). \n    Remaining useful life prediction using multi-scale deep convolutional neural network. \n    Applied Soft Computing, 89, 106113.\n\n    (Reference)[https://www.sciencedirect.com/science/article/pii/S1568494620300533?casa_token=wp27UPxwVTIAAAAA:e_fMuKsvfQf8VZ7DgsXKCi6mpnrcx2hI0tbfe5xrLRhxmc2vaR-uW_Qq23v5yBqziBoSM5gu]\n\n    Parameters:\n\n        input_shape: Input shape of the iterator\n        n_msblocks: Number of scale blocks\n        scales: List of scales\n        n_hidden: Hidden convolutional dimensions\n        l2: regularization\n        dropout: Dropout factor\n        activation\n    \"\"\"\n\n    def _create_scale_conv(hidden, scale):\n        return Conv2D(hidden, (scale, 1), padding=\"same\", activation=activation)\n\n    input = Input(input_shape)\n    x = input\n    x = ExpandDimension()(x)\n    for i in range(n_msblocks):\n        convs = [\n            _create_scale_conv(n_hidden, scale)(x) for scale in scales\n        ]\n        x = Add()(convs)\n    x = Conv2D(10, (10, 1), padding=\"same\", activation=activation)(x)\n    x = Conv2D(1, (3, 1), padding=\"same\", activation=activation)(x)\n    x = Flatten()(x)\n    x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(l2))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"tanh\")(x)\n    x = Dropout(dropout)(x)\n    x = Dense(1, activation='relu')(x)\n    return Model(inputs=[input], outputs=[x])\n</code></pre>"},{"location":"models/keras/catalog/models/#ceruleo.models.keras.catalog.MVCNN.MVCNN","title":"<code>MVCNN(input_shape, *, window=64, dropout=0.1)</code>","text":"<p>Model presented in Remaining useful life estimation in prognostics using deep convolution neural networks</p> <p>Li, X., Ding, Q., &amp; Sun, J. Q. (2018). Remaining useful life estimation in prognostics using deep convolution neural networks. Reliability Engineering &amp; System Safety, 172, 1-11.</p> <p>Deafult parameters reported in the article:</p> <pre><code>Number of filters:      10\nWindow size:    30/20/30/15\nFilter length: 10\n\nNeurons in fully-connected layer        100\nDropout rate    0.5\nbatch_size = 512\n</code></pre> <p>Parameters:</p> <pre><code>window: Window size of the convolutional windows\ndropout:\n</code></pre> Source code in <code>ceruleo/models/keras/catalog/MVCNN.py</code> <pre><code>def MVCNN(\n    input_shape: Tuple[int, int],\n    *,\n    window = 64,\n    dropout: float=0.1,\n\n):\n    \"\"\"\n    Model presented in Remaining useful life estimation in prognostics using deep convolution neural networks\n\n    Li, X., Ding, Q., &amp; Sun, J. Q. (2018).\n    Remaining useful life estimation in prognostics using deep convolution neural networks.\n    Reliability Engineering &amp; System Safety, 172, 1-11.\n\n    Deafult parameters reported in the article:\n\n        Number of filters:\t10\n        Window size:\t30/20/30/15\n        Filter length: 10\n\n        Neurons in fully-connected layer\t100\n        Dropout rate\t0.5\n        batch_size = 512\n\n\n    Parameters:\n\n        window: Window size of the convolutional windows\n        dropout: \n\n    \"\"\"\n    n_features = input_shape[1]\n    window = input_shape[0]\n\n    input = Input(shape=input_shape)\n    x = input\n    x = Permute((2, 1))(x)\n    x = Reshape((input_shape[0], input_shape[1], window))(x)\n\n    x = Conv2D(window, (1, 1), activation=\"relu\", padding=\"same\")(x)\n\n    x1 = Conv2D(window, (2, 2), activation=\"relu\", padding=\"same\")(x)\n    x1 = Conv2D(window, (2, 2), activation=\"relu\", padding=\"same\")(x1)\n\n    x2 = Conv2D(window, (3, 3), activation=\"relu\", padding=\"same\")(x)\n    x2 = Conv2D(window, (3, 3), activation=\"relu\", padding=\"same\")(x2)\n\n    x3 = Conv2D(window, (5, 5), activation=\"relu\", padding=\"same\")(x)\n    x3 = Conv2D(window, (5, 5), activation=\"relu\", padding=\"same\")(x3)\n\n    x = Concatenate(axis=1)([x, x1, x2, x3])\n    x = Conv2D(window, input_shape)(x)\n    x = Flatten()(x)\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(dropout)(x)\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(dropout)(x)\n    output = Dense(1)(x)\n    model = Model(\n        inputs=[input],\n        outputs=[output],\n    )\n    return model\n</code></pre>"},{"location":"models/keras/catalog/models/#ceruleo.models.keras.catalog.XiangQiangJianQiao.XiangQiangJianQiaoModel","title":"<code>XiangQiangJianQiaoModel(input_shape, *, convolutional_layers=[(10, 10), (10, 10), (10, 10)], dropout=0.5, dense_dimension=100, convolutional_activation='tanh')</code>","text":"<p>Model presented in \"Remaining useful life estimation in prognostics using deep convolution neural networks\"</p> <p>Xiang Li, Qian Ding, Jian-Qiao Sun</p> <p>Deafult parameters reported in the article:</p> <pre><code>Number of filters:      10\nWindow size:    30/20/30/15\nFilter length: 10\n\nNeurons in fully-connected layer:       100\nDropout rate:   0.5\nbatch_size: 512\n</code></pre> <p>Parameters:</p> <pre><code>input_shape: input shape of the iterator\n\nconvolutional_layers : List[Tuple[int, int]], optional\n    List of tuples with the convolutional dimensions\n    Each element of the list is a tuple that contains\n    (number of filter, kernel size)\ndropout: Dropout rate\ndense_dimension: Dimension of the fully connected layer\nconvolutional_activation: Activation of the convolutional layers\n</code></pre> <p>Return:</p> <pre><code>model: tf.keras.Model\n</code></pre> Source code in <code>ceruleo/models/keras/catalog/XiangQiangJianQiao.py</code> <pre><code>def XiangQiangJianQiaoModel(\n    input_shape: Tuple[int, int],\n    *,\n    convolutional_layers: List[Tuple[int, int]] = [(10, 10), (10, 10), (10, 10)],\n    dropout: float = 0.5,\n    dense_dimension: int = 100,\n    convolutional_activation: str = \"tanh\",\n) -&gt; tf.keras.Model:\n    \"\"\"Model presented in \"Remaining useful life estimation in prognostics using deep convolution neural networks\"\n\n\n    Xiang Li, Qian Ding, Jian-Qiao Sun\n\n\n    Deafult parameters reported in the article:\n\n        Number of filters:\t10\n        Window size:\t30/20/30/15\n        Filter length: 10\n\n        Neurons in fully-connected layer:\t100\n        Dropout rate:\t0.5\n        batch_size: 512\n\n    Parameters:\n\n        input_shape: input shape of the iterator\n\n        convolutional_layers : List[Tuple[int, int]], optional\n            List of tuples with the convolutional dimensions\n            Each element of the list is a tuple that contains\n            (number of filter, kernel size)\n        dropout: Dropout rate\n        dense_dimension: Dimension of the fully connected layer\n        convolutional_activation: Activation of the convolutional layers\n\n    Return:\n\n        model: tf.keras.Model\n\n    \"\"\"\n\n    input = Input(shape=input_shape)\n    x = input\n\n    x = ExpandDimension()(x)\n    for n_filters, filter_size in convolutional_layers:\n        x = Conv2D(\n            n_filters,\n            (filter_size, 1),\n            padding=\"same\",\n            activation=convolutional_activation,\n        )(x)\n\n    x = Conv2D(1, (3, 1), padding=\"same\", activation=convolutional_activation)(x)\n\n    x = Flatten()(x)\n    x = Dropout(dropout)(x)\n    x = Dense(dense_dimension, activation=convolutional_activation)(x)\n    output = Dense(1, activation=\"relu\")(x)\n    model = Model(\n        inputs=[input],\n        outputs=[output],\n    )\n    return model\n</code></pre>"},{"location":"models/keras/catalog/models/#ceruleo.models.keras.catalog.XCM.XCM","title":"<code>XCM(input_shape, *, n_filters=128, filter_window=7)</code>","text":"<p>A modified version of the model presented in  XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification</p> <p>The classification head was replaced by an regression head</p> <p>Fauvel, K., Lin, T., Masson, V., Fromont, \u00c9., &amp; Termier, A. (2021).  Xcm: An explainable convolutional neural network for multivariate time series classification. Mathematics,  9(23), 3137.</p> Source code in <code>ceruleo/models/keras/catalog/XCM.py</code> <pre><code>def XCM(input_shape: Tuple[int, int], *, n_filters: int = 128, filter_window: int = 7):\n    \"\"\"\n    A modified version of the model presented in  XCM: An Explainable Convolutional Neural Network for Multivariate Time Series Classification\n\n    The classification head was replaced by an regression head\n\n    Fauvel, K., Lin, T., Masson, V., Fromont, \u00c9., &amp; Termier, A. (2021). \n    Xcm: An explainable convolutional neural network for multivariate time series classification. Mathematics, \n    9(23), 3137.\n\n\n    \"\"\"\n\n    input = Input(input_shape)\n    x = input\n\n    x2d = ExpandDimension()(x)\n    x2d = Conv2D(n_filters, (input_shape[0], 1), padding=\"same\", name=\"first_conv2d\")(\n        x2d\n    )\n\n    x2d = BatchNormalization()(x2d)\n\n    x2d = Activation(\"relu\")(x2d)\n    model_fisrt_conv2d = Model(\n        inputs=[input],\n        outputs=[x2d],\n    )\n    x2d_input = Input(shape=x2d.shape[1:])\n\n    x2d = Conv2D(\n        1,\n        (1, 1),\n        padding=\"same\",\n        activation=\"relu\",\n    )(x2d_input)\n\n    x2d = RemoveDimension(3)(x2d)\n\n    input = Input(shape=(input_shape[0], input_shape[1]))\n    x = input\n    x1d = Conv1D(\n        n_filters,\n        filter_window,\n        padding=\"same\",\n    )(x)\n\n    x1d = BatchNormalization()(x1d)\n\n    x1d = Activation(\"relu\")(x1d)\n    model_fisrt_conv1d = Model(\n        inputs=[input],\n        outputs=[x1d],\n    )\n    x1d_input = Input(shape=x1d.shape[1:])\n\n    x1d = Conv1D(\n        1,\n        1,\n        padding=\"same\",\n        activation=\"relu\",\n    )(x1d_input)\n\n    x = Concatenate()([x2d, x1d])\n\n    x = Conv1D(n_filters, filter_window, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = GlobalAveragePooling1D()(x)\n\n    x = Dense(n_filters, activation=\"relu\")(x)\n    output = Dense(1, activation=\"relu\", name=\"RUL_regressor\")(x)\n\n    model_regression = Model(inputs=[x2d_input, x1d_input], outputs=[output])\n    model_input = Input(shape=(input_shape[0], input_shape[1]))\n    conv2d = model_fisrt_conv2d(model_input)\n    conv1d = model_fisrt_conv1d(model_input)\n    output = model_regression([conv2d, conv1d])\n\n    model = Model(\n        inputs=[model_input],\n        outputs=output,\n    )\n    return model,  (model_fisrt_conv1d, model_fisrt_conv2d, model_regression)\n</code></pre>"},{"location":"results/results/","title":"Results","text":"<p>Compute evaluating results of fitted models</p> <p>One of the most important issues regarding PM is the ability to compare and evaluate different methods.</p> <p>The main data structure used in the results module is a dictionary in which each of the keys is the model name,  and the elements are a list of PredictionResult.  Each element of the model array is interpreted as a Fold in CV settings. </p> <p>Additionally to the regression error, it is possible to compute some metrics more easily interpretable. In this context, two metrics were defined in [1], namely:</p> <ul> <li>Frequency of Unexpected Breaks (\u03c1UB) - the percentage of failures not prevented;</li> <li>Amount of Unexploited Lifetime (\u03c1UL) - the average number of time that could have been run before failure if the preventative maintenance suggested by the maintenance management mod-ule had not been performed.</li> </ul> <p>When compaing between the predicted end of life with respect to the true end of that particular life three scenarios can happen:</p> <ul> <li>The predicted end of life occurs before the true one. In that case, the predictions were pessimistic and the tool could have been used more time.</li> <li>The remaining useful life arrives at zero after the true remaining useful life. In that case, we incur the risk of the tool breaking.  </li> <li>The predicted line coincides with the true line. In that case, we don\u2019t have unexploited time, and the risk of breakage can be considered 0.</li> </ul> <p>Since usually the breakages are considered more harmful, a possible approach to preventing unexpected failures is to consider a more conservative maintenance approach, providing maintenance tasks recommendations some time before the end of life predicted. In that way, a conservative window can be defined in which the recommendation of making maintenance task should be performed at time T-predicted - conservative window size.</p> <p>[1] Machine Learning for Predictive Maintenance: A Multiple Classifiers Approach     Susto, G. A., Schirru, A., Pampuri, S., McLoone, S., &amp; Beghi, A. (2015).</p>"},{"location":"results/results/#ceruleo.results.results.CVResults","title":"<code>CVResults</code>","text":"<p>Compute the error histogram</p> <p>Compute the error with respect to the RUL considering the results of different folds</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>List[List]</code> <p>List with the true values of each hold-out set of a cross validation</p> required <code>y_pred</code> <code>List[List]</code> <p>List with the predictions of each hold-out set of a cross validation</p> required <code>nbins</code> <code>int</code> <p>Number of bins to compute the histogram</p> <code>5</code> Source code in <code>ceruleo/results/results.py</code> <pre><code>class CVResults:\n    \"\"\"\n    Compute the error histogram\n\n    Compute the error with respect to the RUL considering the results of different\n    folds\n\n    Parameters:\n        y_true: List with the true values of each hold-out set of a cross validation\n        y_pred: List with the predictions of each hold-out set of a cross validation\n        nbins: Number of bins to compute the histogram\n\n    \"\"\"\n\n    def __init__(\n        self,\n        y_true: List[List],\n        y_pred: List[List],\n        nbins: int = 5,\n        bin_edges: Optional[np.array] = None,\n    ):\n        if bin_edges is None:\n            max_value = np.max([np.max(y) for y in y_true])\n            bin_edges = np.linspace(0, max_value, nbins + 1)\n        self.n_folds = len(y_true)\n        self.n_bins = len(bin_edges) - 1\n        self.bin_edges = bin_edges\n        self.mean_error = np.zeros((self.n_folds, self.n_bins))\n        self.mae = np.zeros((self.n_folds, self.n_bins))\n        self.mse = np.zeros((self.n_folds, self.n_bins))\n        self.errors = []\n        for i, (y_pred, y_true) in enumerate(zip(y_pred, y_true)):\n            self._add_fold_result(i, y_pred, y_true)\n\n    def _add_fold_result(self, fold: int, y_pred: np.array, y_true: np.array):\n        y_pred = np.squeeze(y_pred)\n        y_true = np.squeeze(y_true)\n\n        for j in range(len(self.bin_edges) - 1):\n            mask = (y_true &gt;= self.bin_edges[j]) &amp; (y_true &lt;= self.bin_edges[j + 1])\n            indices = np.where(mask)[0]\n\n            if len(indices) == 0:\n                continue\n\n            errors = y_true[indices] - y_pred[indices]\n\n            self.mean_error[fold, j] = np.mean(errors)\n\n            self.mae[fold, j] = np.mean(np.abs(errors))\n            self.mse[fold, j] = np.mean((errors) ** 2)\n            self.errors.append(errors)\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.FittedLife","title":"<code>FittedLife</code>","text":"<p>Represent a Fitted run-to-cycle failure</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array</code> <p>The true RUL target</p> required <code>y_pred</code> <code>array</code> <p>The predicted target</p> required <code>time</code> <code>Optional[Union[array, int]]</code> <p>Time feature</p> <code>None</code> <code>fit_line_not_increasing</code> <code>Optional[bool]</code> <p>Wether the fitted line can increase or not.</p> <code>False</code> <code>RUL_threshold</code> <code>Optional[float]</code> <p>Indicates the thresholding value used during  the fit</p> <code>None</code> Source code in <code>ceruleo/results/results.py</code> <pre><code>class FittedLife:\n    \"\"\"Represent a Fitted run-to-cycle failure\n\n    Parameters:\n        y_true: The true RUL target\n        y_pred: The predicted target\n        time: Time feature\n        fit_line_not_increasing: Wether the fitted line can increase or not.\n        RUL_threshold: Indicates the thresholding value used during  the fit\n\n    \"\"\"\n\n    def __init__(\n        self,\n        y_true: np.array,\n        y_pred: np.array,\n        time: Optional[Union[np.array, int]] = None,\n        fit_line_not_increasing: Optional[bool] = False,\n        RUL_threshold: Optional[float] = None,\n    ):\n        self.fit_line_not_increasing = fit_line_not_increasing\n        y_true = np.squeeze(y_true)\n        y_pred = np.squeeze(y_pred)\n\n        if time is not None:\n            self.degrading_start = FittedLife._degrading_start(y_true, RUL_threshold)\n            if isinstance(time, np.ndarray):\n                self.time = time\n            else:\n                self.time = np.array(np.linspace(0, y_true[0], n=len(y_true)))\n\n        else:\n            self.degrading_start, self.time = FittedLife.compute_time_feature(\n                y_true, RUL_threshold\n            )\n\n        # self.y_pred_fitted_picewise = self._fit_picewise_linear_regression(y_pred)\n        # self.y_true_fitted_picewise = self._fit_picewise_linear_regression(y_true)\n\n        self.RUL_threshold = RUL_threshold\n        self.y_pred = y_pred\n        self.y_true = y_true\n\n        self.y_pred_fitted_coefficients = np.polyfit(self.time, self.y_pred, 1)\n        p = np.poly1d(self.y_pred_fitted_coefficients)\n        self.y_pred_fitted = p(self.time)\n\n        self.y_true_fitted_coefficients = np.polyfit(self.time, self.y_true, 1)\n        p = np.poly1d(self.y_true_fitted_coefficients)\n        self.y_true_fitted = p(self.time)\n\n    @staticmethod\n    def compute_time_feature(\n        y_true: np.array, RUL_threshold: Optional[float] = None\n    ) -&gt; Tuple[float, np.ndarray]:\n        \"\"\"Compute the time feature based on the target\n\n        Parameters:\n            y_true: RUL target\n            RUL_threshold:\n\n        Returns:\n            Degrading start time and time\n        \"\"\"\n        degrading_start = FittedLife._degrading_start(y_true, RUL_threshold)\n        time = FittedLife._compute_time(y_true, degrading_start)\n        return degrading_start, time\n\n    @staticmethod\n    def _degrading_start(\n        y_true: np.array, RUL_threshold: Optional[float] = None\n    ) -&gt; float:\n        \"\"\"\n        Obtain the index when the life value is lower than the RUL_threshold\n\n        Parameters:\n            y_true: Array of true values of the RUL of the life\n            RUL_threshold: float\n\n\n        Return:\n            If RUL_threshold is None, the degrading start if the first index.\n            Otherwise it is the first index in which y_true &lt; RUL_threshold\n        \"\"\"\n        degrading_start = 0\n        if RUL_threshold is not None:\n            degrading_start_i = np.where(y_true &lt; RUL_threshold)\n            if len(degrading_start_i[0]) &gt; 0:\n                degrading_start = degrading_start_i[0][0]\n        else:\n            d = np.diff(y_true) == 0\n            while (degrading_start &lt; len(d)) and (d[degrading_start]):\n                degrading_start += 1\n        return degrading_start\n\n    @staticmethod\n    def _compute_time(y_true: np.array, degrading_start: int) -&gt; np.array:\n        \"\"\"\n        Compute the passage of time from the true RUL\n\n        The passage of time is computed as the cumulative sum of the first\n        difference of the true labels. In case there are tresholded values,\n        the time steps of the thresholded zone is assumed to be as the median values\n        of the time steps computed in the zones of the life in which we have information.\n\n        Parameters:\n            y_true: The true RUL labels\n            degrading_start: The index in which the true RUL values starts to be lower than the treshold\n\n        Returns:\n            Time component\n        \"\"\"\n        if len(y_true) == 1:\n            return np.array([0])\n\n        time_diff = np.diff(np.squeeze(y_true)[degrading_start:][::-1])\n        time = np.zeros(len(y_true))\n        if degrading_start &gt; 0:\n            if len(time_diff) &gt; 0:\n                time[0 : degrading_start + 1] = np.median(time_diff)\n            else:\n                time[0 : degrading_start + 1] = 1\n        time[degrading_start + 1 :] = time_diff\n\n        return np.cumsum(time)\n\n    def _fit_picewise_linear_regression(self, y: np.array) -&gt; PiecewesieLinearFunction:\n        \"\"\"\n        Fit the array trough a picewise linear regression\n\n        Parameters:\n            y: Points to be fitted\n        Returns:\n            The Picewise linear function fitted\n        \"\"\"\n        pwlr = PiecewiseLinearRegression(not_increasing=self.fit_line_not_increasing)\n        for j in range(len(y)):\n            pwlr.add_point(self.time[j], y[j])\n        line = pwlr.finish()\n\n        return line\n\n    def rmse(self, sample_weight=None) -&gt; float:\n        N = len(self.y_pred)\n        sw = compute_sample_weight(sample_weight, self.y_true[:N], self.y_pred)\n        return np.sqrt(np.mean(sw * (self.y_true[:N] - self.y_pred) ** 2))\n\n    def mae(self, sample_weight=None) -&gt; float:\n        N = len(self.y_pred)\n        sw = compute_sample_weight(sample_weight, self.y_true[:N], self.y_pred)\n        return np.mean(sw * np.abs(self.y_true[:N] - self.y_pred))\n\n    def noisiness(self) -&gt; float:\n        \"\"\"\n        How much the predictions resembles a line\n\n        This metric is computed as the mse of the fitted values\n        with respect to the least squares fitted line of this\n        values\n\n        Returns:\n            The Mean Absolute Error of the fitted values with respect to the least squares fitted line\n        \"\"\"\n        return mae(self.y_pred_fitted, self.y_pred)\n\n    def slope_resemblance(self):\n        m1 = self.y_true_fitted_coefficients[0]\n        m2 = self.y_pred_fitted_coefficients[0]\n        d = np.arctan((m1 - m2) / (1 + m1 * m2))\n        d = d / (np.pi / 2)\n        return 1 - np.abs((d / (np.pi / 2)))\n\n    def predicted_end_of_life(self):\n        z = np.where(self.y_pred == 0)[0]\n        if len(z) == 0:\n            return self.time[len(self.y_pred) - 1] + self.y_pred[-1]\n        else:\n            return self.time[z[0]]\n\n    def end_of_life(self):\n        z = np.where(self.y_true == 0)[0]\n        if len(z) == 0:\n            return self.time[len(self.y_pred) - 1] + self.y_true[-1]\n        else:\n            return self.time[z[0]]\n\n    def maintenance_point(self, m: float = 0) -&gt; float:\n        \"\"\"\n        Compute the maintenance point\n\n        The maintenance point is computed as the predicted end of life - m\n\n        Parameters:\n            m: Fault horizon  Defaults to 0.\n\n        Returns:\n            Time of maintenance\n        \"\"\"\n        return self.predicted_end_of_life() - m\n\n    def unexploited_lifetime(self, m: float = 0) -&gt; float:\n        \"\"\"\n        Compute the unexploited lifetime given a fault horizon window\n\n        Machine Learning for Predictive Maintenance: A Multiple Classifiers Approach\n        Susto, G. A., Schirru, A., Pampuri, S., McLoone, S., &amp; Beghi, A. (2015).\n\n        Parameters:\n            m: Fault horizon window. Defaults to 0.\n\n        Returns:\n            Unexploited lifetime\n        \"\"\"\n\n        if self.maintenance_point(m) &lt; self.end_of_life():\n            return self.end_of_life() - self.maintenance_point(m)\n        else:\n            return 0\n\n    def unexpected_break(self, m: float = 0, tolerance: float = 0) -&gt; bool:\n        \"\"\"\n        Compute weather an unexpected break will produce using a fault horizon window of size m\n\n        Machine Learning for Predictive Maintenance: A Multiple Classifiers Approach\n        Susto, G. A., Schirru, A., Pampuri, S., McLoone, S., &amp; Beghi, A. (2015).\n\n        Parameters:\n            m: Fault horizon window.\n\n        Returns:\n            A boolean indicating if an unexpected break will occur\n        \"\"\"\n        if self.maintenance_point(m) - tolerance &lt; self.end_of_life():\n            return False\n        else:\n            return True\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.FittedLife.compute_time_feature","title":"<code>compute_time_feature(y_true, RUL_threshold=None)</code>  <code>staticmethod</code>","text":"<p>Compute the time feature based on the target</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array</code> <p>RUL target</p> required <code>RUL_threshold</code> <code>Optional[float]</code> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[float, ndarray]</code> <p>Degrading start time and time</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>@staticmethod\ndef compute_time_feature(\n    y_true: np.array, RUL_threshold: Optional[float] = None\n) -&gt; Tuple[float, np.ndarray]:\n    \"\"\"Compute the time feature based on the target\n\n    Parameters:\n        y_true: RUL target\n        RUL_threshold:\n\n    Returns:\n        Degrading start time and time\n    \"\"\"\n    degrading_start = FittedLife._degrading_start(y_true, RUL_threshold)\n    time = FittedLife._compute_time(y_true, degrading_start)\n    return degrading_start, time\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.FittedLife.maintenance_point","title":"<code>maintenance_point(m=0)</code>","text":"<p>Compute the maintenance point</p> <p>The maintenance point is computed as the predicted end of life - m</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>float</code> <p>Fault horizon  Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>Time of maintenance</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def maintenance_point(self, m: float = 0) -&gt; float:\n    \"\"\"\n    Compute the maintenance point\n\n    The maintenance point is computed as the predicted end of life - m\n\n    Parameters:\n        m: Fault horizon  Defaults to 0.\n\n    Returns:\n        Time of maintenance\n    \"\"\"\n    return self.predicted_end_of_life() - m\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.FittedLife.noisiness","title":"<code>noisiness()</code>","text":"<p>How much the predictions resembles a line</p> <p>This metric is computed as the mse of the fitted values with respect to the least squares fitted line of this values</p> <p>Returns:</p> Type Description <code>float</code> <p>The Mean Absolute Error of the fitted values with respect to the least squares fitted line</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def noisiness(self) -&gt; float:\n    \"\"\"\n    How much the predictions resembles a line\n\n    This metric is computed as the mse of the fitted values\n    with respect to the least squares fitted line of this\n    values\n\n    Returns:\n        The Mean Absolute Error of the fitted values with respect to the least squares fitted line\n    \"\"\"\n    return mae(self.y_pred_fitted, self.y_pred)\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.FittedLife.unexpected_break","title":"<code>unexpected_break(m=0, tolerance=0)</code>","text":"<p>Compute weather an unexpected break will produce using a fault horizon window of size m</p> <p>Machine Learning for Predictive Maintenance: A Multiple Classifiers Approach Susto, G. A., Schirru, A., Pampuri, S., McLoone, S., &amp; Beghi, A. (2015).</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>float</code> <p>Fault horizon window.</p> <code>0</code> <p>Returns:</p> Type Description <code>bool</code> <p>A boolean indicating if an unexpected break will occur</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def unexpected_break(self, m: float = 0, tolerance: float = 0) -&gt; bool:\n    \"\"\"\n    Compute weather an unexpected break will produce using a fault horizon window of size m\n\n    Machine Learning for Predictive Maintenance: A Multiple Classifiers Approach\n    Susto, G. A., Schirru, A., Pampuri, S., McLoone, S., &amp; Beghi, A. (2015).\n\n    Parameters:\n        m: Fault horizon window.\n\n    Returns:\n        A boolean indicating if an unexpected break will occur\n    \"\"\"\n    if self.maintenance_point(m) - tolerance &lt; self.end_of_life():\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.FittedLife.unexploited_lifetime","title":"<code>unexploited_lifetime(m=0)</code>","text":"<p>Compute the unexploited lifetime given a fault horizon window</p> <p>Machine Learning for Predictive Maintenance: A Multiple Classifiers Approach Susto, G. A., Schirru, A., Pampuri, S., McLoone, S., &amp; Beghi, A. (2015).</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>float</code> <p>Fault horizon window. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>float</code> <p>Unexploited lifetime</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def unexploited_lifetime(self, m: float = 0) -&gt; float:\n    \"\"\"\n    Compute the unexploited lifetime given a fault horizon window\n\n    Machine Learning for Predictive Maintenance: A Multiple Classifiers Approach\n    Susto, G. A., Schirru, A., Pampuri, S., McLoone, S., &amp; Beghi, A. (2015).\n\n    Parameters:\n        m: Fault horizon window. Defaults to 0.\n\n    Returns:\n        Unexploited lifetime\n    \"\"\"\n\n    if self.maintenance_point(m) &lt; self.end_of_life():\n        return self.end_of_life() - self.maintenance_point(m)\n    else:\n        return 0\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.MetricsResult","title":"<code>MetricsResult</code>  <code>dataclass</code>","text":"<p>An object that store regression metrics and times</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>@dataclass\nclass MetricsResult:\n    \"\"\"An object that store regression metrics and times\"\"\"\n\n    mae: float\n    mse: float\n    fitting_time: float = field(default_factory=lambda: 0)\n    prediction_time: float = field(default_factory=lambda: 0)\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.PredictionResult","title":"<code>PredictionResult</code>  <code>dataclass</code>","text":"<p>A prediction result is composed by a name</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>@dataclass\nclass PredictionResult:\n    \"\"\"A prediction result is composed by a name\"\"\"\n\n    name: str\n    true_RUL: np.ndarray\n    predicted_RUL: np.ndarray\n    metrics: MetricsResult = field(default_factory=lambda: MetricsResult(0, 0))\n\n    def compute_metrics(self):\n        self.metrics.mae = mae(self.true_RUL, self.predicted_RUL)\n        self.metrics.mse = mse(self.true_RUL, self.predicted_RUL)\n\n    def __post_init__(self):    \n        self.true_RUL = np.squeeze(self.true_RUL)\n        self.predicted_RUL = np.squeeze(self.predicted_RUL)\n        self.compute_metrics()\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.cv_regression_metrics","title":"<code>cv_regression_metrics(results_dict, threshold=np.inf)</code>","text":"<pre><code>Compute regression metrics for each model\n\nParameters:\n</code></pre> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p> <pre><code>    data: Dictionary with the model predictions.\n</code></pre> <p>=======         data: Dictionary with the model predictions.</p> <p>7862b3c56dbc36f72b3c7f87d9b39e1ae78b4ddc         threshold: Compute metrics errors only in RUL values less than the threshold</p> <pre><code>Returns:\n    A dictionary with the following structure:\n        d: { ['Model]:\n                {\n                    'MAE': {\n                        'mean':\n                        'std':\n                    },\n                    'MAE SW': {\n                        'mean':\n                        'std':\n                    },\n                    'MSE': {\n                        'mean':\n                        'std':\n                    },\n                }\n            ]\n</code></pre> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p> <pre><code>    d: { ['Model]:\n            {\n                'MAE': {\n                    'mean':\n                    'std':\n                },\n                'MAE SW': {\n                    'mean':\n                    'std':\n                },\n                'MSE': {\n                    'mean':\n                    'std':\n                },\n            }\n        ]\n</code></pre> <p>=======</p> <p>7862b3c56dbc36f72b3c7f87d9b39e1ae78b4ddc</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def cv_regression_metrics(\n    results_dict: Dict[str, List[PredictionResult]], threshold: float = np.inf\n) -&gt; dict:\n    \"\"\"\n    Compute regression metrics for each model\n\n    Parameters:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\n        data: Dictionary with the model predictions.\n\n=======\n        data: Dictionary with the model predictions.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 7862b3c56dbc36f72b3c7f87d9b39e1ae78b4ddc\n        threshold: Compute metrics errors only in RUL values less than the threshold\n\n    Returns:\n        A dictionary with the following structure:\n            d: { ['Model]:\n                    {\n                        'MAE': {\n                            'mean':\n                            'std':\n                        },\n                        'MAE SW': {\n                            'mean':\n                            'std':\n                        },\n                        'MSE': {\n                            'mean':\n                            'std':\n                        },\n                    }\n                ]\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\n        d: { ['Model]:\n                {\n                    'MAE': {\n                        'mean':\n                        'std':\n                    },\n                    'MAE SW': {\n                        'mean':\n                        'std':\n                    },\n                    'MSE': {\n                        'mean':\n                        'std':\n                    },\n                }\n            ]\n\n=======\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 7862b3c56dbc36f72b3c7f87d9b39e1ae78b4ddc\n    \"\"\"\n    out = {}\n    for model_name in results_dict.keys():\n        out[model_name] = cv_regression_metrics_single_model(\n            results_dict[model_name], threshold\n        )\n    return out\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.models_cv_results","title":"<code>models_cv_results(results_dict, nbins)</code>","text":"<p>Create a dictionary with the result of each cross validation of the model</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def models_cv_results(\n    results_dict: Dict[str, List[PredictionResult]], nbins: int\n) -&gt; Tuple[np.ndarray, Dict[str, CVResults]]:\n    \"\"\"Create a dictionary with the result of each cross validation of the model\"\"\"\n    max_y_value = np.max(\n        [\n            r.true_RUL.max()\n            for model_name in results_dict.keys()\n            for r in results_dict[model_name]\n        ]\n    )\n    bin_edges = np.linspace(0, max_y_value, nbins + 1)\n    model_results = {}\n    for model_name in results_dict.keys():\n        model_results[model_name] = model_cv_results(\n            results_dict[model_name], bin_edges=bin_edges\n        )\n\n    return bin_edges, model_results\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.split_lives","title":"<code>split_lives(results, RUL_threshold=None, fit_line_not_increasing=False, time=None)</code>","text":"<p>Divide an array of predictions into a list of FittedLife Object</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>The true RUL target</p> required <code>y_pred</code> <p>The predicted RUL</p> required <code>fit_line_not_increasing</code> <code>Optional[bool]</code> <p>Weather the fit line can increase, by default False</p> <code>False</code> <code>time</code> <code>Optional[int]</code> <p>A vector with timestamps. If omitted will be computed from y_true</p> <code>None</code> <p>Returns:</p> Type Description <code>List[FittedLife]</code> <p>FittedLife list</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def split_lives(\n    results: PredictionResult,\n    RUL_threshold: Optional[float] = None,\n    fit_line_not_increasing: Optional[bool] = False,\n    time: Optional[int] = None,\n) -&gt; List[FittedLife]:\n    \"\"\"\n    Divide an array of predictions into a list of FittedLife Object\n\n    Parameters:\n        y_true: The true RUL target\n        y_pred: The predicted RUL\n        fit_line_not_increasing: Weather the fit line can increase, by default False\n        time:  A vector with timestamps. If omitted will be computed from y_true\n\n    Returns:\n       FittedLife list\n    \"\"\"\n    lives = []\n    for r in split_lives_indices(results.true_RUL):\n        if np.any(np.isnan(results.predicted_RUL[r])):\n            continue\n        lives.append(\n            FittedLife(\n                results.true_RUL[r],\n                results.predicted_RUL[r],\n                RUL_threshold=RUL_threshold,\n                fit_line_not_increasing=fit_line_not_increasing,\n                time=time,\n            )\n        )\n    return lives\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.split_lives_indices","title":"<code>split_lives_indices(y_true)</code>","text":"<p>Obtain a list of indices for each life</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>array</code> <p>True vector with the RUL</p> required <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>A list with the indices belonging to each life</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def split_lives_indices(y_true: np.array) -&gt; List[List[int]]:\n    \"\"\"\n    Obtain a list of indices for each life\n\n    Parameters:\n        y_true: True vector with the RUL\n\n    Returns:\n        A list with the indices belonging to each life\n    \"\"\"\n    assert len(y_true) &gt;= 2\n    lives_indices = (\n        [0]\n        + (np.where(np.diff(np.squeeze(y_true)) &gt; 0)[0] + 1).tolist()\n        + [len(y_true)]\n    )\n    indices = []\n    for i in range(len(lives_indices) - 1):\n        r = range(lives_indices[i], lives_indices[i + 1])\n        if len(r) &lt;= 1:\n            continue\n        indices.append(r)\n    return indices\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.unexpected_breaks","title":"<code>unexpected_breaks(d, window_size, step)</code>","text":"<p>Compute the risk of unexpected breaks with respect to the maintenance window size</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>List[PredictionResult]</code> <p>Dictionary with the results</p> required <code>window_size</code> <code>int</code> <p>Maximum size of the maintenance windows</p> required <code>step</code> <code>int</code> <p>Number of points in which compute the risks. step different maintenance windows will be used.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple of np.arrays with: - Maintenance window size evaluated - Risk computed for every window size used</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def unexpected_breaks(\n    d: List[PredictionResult], window_size: int, step: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the risk of unexpected breaks with respect to the maintenance window size\n\n    Parameters:\n        d: Dictionary with the results\n        window_size: Maximum size of the maintenance windows\n        step: Number of points in which compute the risks.\n            step different maintenance windows will be used.\n\n    Returns:\n        A tuple of np.arrays with:\n            - Maintenance window size evaluated\n            - Risk computed for every window size used\n    \"\"\"\n\n    bb = [split_lives(fold) for fold in d]\n    return unexpected_breaks_from_cv(bb, window_size, step)\n</code></pre>"},{"location":"results/results/#ceruleo.results.results.unexpected_breaks_from_cv","title":"<code>unexpected_breaks_from_cv(lives, window_size, n)</code>","text":"<p>Compute the risk of unexpected breaks given a Cross-Validation results</p> <p>Parameters:</p> Name Type Description Default <code>lives</code> <code>List[List[FittedLife]]</code> <p>Cross validation results.</p> required <code>window_size</code> <code>int</code> <p>Maximum size of the maintenance window</p> required <code>n</code> <code>int</code> <p>Number of points to evaluate the risk of unexpected breaks</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple of np.arrays with: - Maintenance window size evaluated - Risk computed for every window size used</p> Source code in <code>ceruleo/results/results.py</code> <pre><code>def unexpected_breaks_from_cv(\n    lives: List[List[FittedLife]], window_size: int, n: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the risk of unexpected breaks given a Cross-Validation results\n\n    Parameters:\n        lives: Cross validation results.\n        window_size: Maximum size of the maintenance window\n        n: Number of points to evaluate the risk of unexpected breaks\n\n\n    Returns:\n        A tuple of np.arrays with:\n            - Maintenance window size evaluated\n            - Risk computed for every window size used\n    \"\"\"\n    std_per_window = []\n    mean_per_window = []\n    windows = np.linspace(0, window_size, n)\n    for m in windows:\n        jj = []\n        for r in lives:\n            ul_cv_list = [life.unexpected_break(m) for life in r]\n            jj.extend(ul_cv_list)\n        mean_per_window.append(np.mean(jj))\n        std_per_window.append(np.std(jj))\n    return windows, np.array(mean_per_window), np.array(std_per_window)\n</code></pre>"},{"location":"results/visualization/","title":"Visualization","text":"<p>Visualization utilities for RUL estimation models</p> <p>It is possible to visualize how it grows the unexploited lifetime grows as the conservative window size grows and how the unexpected breaks decrease as the conservative window size grows.</p>"},{"location":"results/visualization/#ceruleo.graphics.results.barplot_errors_wrt_RUL","title":"<code>barplot_errors_wrt_RUL(results_dict, nbins, y_axis_label=None, x_axis_label=None, ax=None, color_palette='hls', **kwargs)</code>","text":"<p>Barlots of difference between true and predicted RUL</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>Dict[str, List[PredictionResult]]</code> <p>Dictionary with the results for each model</p> required <code>nbins</code> <code>int</code> <p>Number of bins in wich divide the RUL target</p> required <code>y_axis_label</code> <p>Y label</p> <code>None</code> <code>x_axis_label</code> <p>X label</p> <code>None</code> <code>ax</code> <p>Axis</p> <code>None</code> <code>color_palette</code> <code>str</code> <code>'hls'</code> Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def barplot_errors_wrt_RUL(\n    results_dict: Dict[str, List[PredictionResult]],\n    nbins: int,\n    y_axis_label=None,\n    x_axis_label=None,\n    ax=None,\n    color_palette: str = \"hls\",\n    **kwargs,\n):\n    \"\"\"\n    Barlots of difference between true and predicted RUL\n\n    Parameters:\n        results_dict: Dictionary with the results for each model\n        nbins: Number of bins in wich divide the RUL target\n        y_axis_label: Y label\n        x_axis_label: X label\n        ax: Axis\n        color_palette: \n\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(**kwargs)\n    bin_edges, model_results = models_cv_results(results_dict, nbins)\n    return _cv_barplot_errors_wrt_RUL_multiple_models(\n        bin_edges,\n        model_results,\n        ax=ax,\n        y_axis_label=y_axis_label,\n        x_axis_label=x_axis_label,\n        color_palette=color_palette,\n    )\n</code></pre>"},{"location":"results/visualization/#ceruleo.graphics.results.boxplot_errors_wrt_RUL","title":"<code>boxplot_errors_wrt_RUL(results_dict, nbins, y_axis_label=None, x_axis_label=None, ax=None, **kwargs)</code>","text":"<p>Boxplots of difference between true and predicted RUL over Cross-validated results</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>Dict[str, List[PredictionResult]]</code> <p>Dictionary with the results of the fitted models</p> required <code>nbins</code> <code>int</code> <p>Number of bins to divide the</p> required <code>y_axis_label</code> <code>Optional[str]</code> <p>Optional string to be added to the y axis</p> <code>None</code> <code>x_axis_label</code> <code>Optional[str]</code> <p>Optional string to be added to the x axis</p> <code>None</code> <code>ax</code> <p>Optional axis in which the plot will be drawed. If an axis is not provided, it will create one.</p> <code>None</code> Return <p>The axis in which the plot has been made</p> Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def boxplot_errors_wrt_RUL(\n    results_dict: Dict[str, List[PredictionResult]],\n    nbins: int,\n    y_axis_label: Optional[str] = None,\n    x_axis_label: Optional[str] = None,\n    ax=None,\n    **kwargs,\n) -&gt; matplotlib.axes.Axes:\n    \"\"\"\n    Boxplots of difference between true and predicted RUL over Cross-validated results\n\n\n    Parameters:\n        results_dict: Dictionary with the results of the fitted models\n        nbins: Number of bins to divide the\n        y_axis_label: Optional string to be added to the y axis\n        x_axis_label: Optional string to be added to the x axis\n        ax: Optional axis in which the plot will be drawed.\n            If an axis is not provided, it will create one.\n\n    Keyword arguments:\n        **kwargs\n\n    Return:\n        The axis in which the plot has been made\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(**kwargs)\n    else:\n        fig = ax.figure\n\n    bin_edges, model_results = models_cv_results(results_dict, nbins)\n    return _boxplot_errors_wrt_RUL_multiple_models(\n        bin_edges,\n        model_results,\n        fig=fig,\n        ax=ax,\n        y_axis_label=y_axis_label,\n        x_axis_label=x_axis_label,\n    )\n</code></pre>"},{"location":"results/visualization/#ceruleo.graphics.results.cv_plot_errors_wrt_RUL","title":"<code>cv_plot_errors_wrt_RUL(bin_edges, error_histogram, **kwargs)</code>","text":"Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def cv_plot_errors_wrt_RUL(bin_edges, error_histogram, **kwargs):\n    \"\"\"\"\"\"\n    fig, ax = plt.subplots(**kwargs)\n    labels = []\n    heights = []\n    xs = []\n    yerr = []\n\n    for i in range(len(error_histogram)):\n        xs.append(i)\n        heights.append(np.mean(error_histogram[i]))\n        yerr.append(np.std(error_histogram[i]))\n        labels.append(f\"[{bin_edges[i]:.1f}, {bin_edges[i+1]:.1f})\")\n\n    ax.bar(height=heights, x=xs, yerr=yerr, tick_label=labels)\n    ax.set_xlabel(\"RUL\")\n    ax.set_ylabel(\"RMSE\")\n\n    return ax\n</code></pre>"},{"location":"results/visualization/#ceruleo.graphics.results.plot_life","title":"<code>plot_life(life, ax=None, units='', markersize=0.7, add_fitted=False, plot_target=True, add_regressed=True, start_x=0, label='', **kwargs)</code>","text":"<p>Plot a single life</p> <p>Parameters:</p> Name Type Description Default <code>life</code> <code>FittedLife</code> <p>A fitted life</p> required <code>ax</code> <code>Optional[Axes]</code> <p>The axis where to plot</p> <code>None</code> <code>units</code> <p>Optional[str], optional description, by default \"\"</p> <code>''</code> <code>markersize</code> <code>float</code> <p>Size of the marker</p> <code>0.7</code> <code>add_fitted</code> <code>bool</code> <p>Wether to add the LS fitted line to the points</p> <code>False</code> <code>plot_target</code> <code>bool</code> <p>Wether to plot the true RUL values</p> <code>True</code> <code>add_regressed</code> <code>bool</code> <code>True</code> <code>start_x</code> <code>int</code> <p>Initial point of the time-indepedent feature to plot</p> <code>0</code> <code>label</code> <code>str</code> <code>''</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The plot axis</p> Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def plot_life(\n    life: FittedLife,\n    ax: Optional[matplotlib.axes.Axes]=None,\n    units: Optional[str] = \"\",\n    markersize: float = 0.7,\n    add_fitted: bool = False,\n    plot_target: bool = True,\n    add_regressed: bool = True,\n    start_x:int= 0,\n    label:str = '',\n    **kwargs,\n) -&gt; matplotlib.axes.Axes:\n    \"\"\"\n    Plot a single life\n\n    Parameters:\n        life: A fitted life\n        ax: The axis where to plot\n        units : Optional[str], optional\n            _description_, by default \"\"\n        markersize: Size of the marker\n        add_fitted: Wether to add the LS fitted line to the points\n        plot_target: Wether to plot the true RUL values\n        add_regressed:\n        start_x: Initial point of the time-indepedent feature to plot\n        label: \n\n    Returns:\n        The plot axis\n    \"\"\"\n    if ax is None:\n        _, ax = plt.subplots(1, 1, **kwargs)\n\n    time = life.time\n\n    ax.plot(\n        life.time[start_x: len(life.y_pred)],\n        life.y_pred[start_x:],\n        \"o\",\n        label=f\"Predicted {label}\",\n        markersize=markersize,\n    )\n    if plot_target:\n        ax.plot(life.time, life.y_true, label=\"True\", linewidth=3)\n    if add_regressed and life.y_true[-1] &gt; 0:\n            time1 = np.hstack((time[-1], time[-1] + life.y_true[-1]))\n            ax.plot(time1, [life.y_true[-1], 0], label=\"Regressed true\")\n    if add_fitted:\n        #time1 = np.hstack(\n        #    (time[len(life.y_pred) - 1], time[len(life.y_pred) - 1] + life.y_pred[-1])\n        #)\n        #ax.plot(time1, [life.y_pred[-1], 0], label=\"Projected end\")\n        ax.plot(\n           life.time,\n           life.y_pred_fitted,\n           label=\"Picewise fitted\",\n        )\n        ax.plot(\n           life.time,\n           life.y_true_fitted,\n           label=\"Picewise fitted\",\n        )\n\n    ax.set_ylabel(units)\n    ax.set_xlabel(units)\n    _, max = ax.get_ylim()\n    ax.set_ylim(0 - max / 10, max)\n    legend = ax.legend(markerscale=15,)\n\n\n\n    return ax\n</code></pre>"},{"location":"results/visualization/#ceruleo.graphics.results.plot_lives","title":"<code>plot_lives(ds)</code>","text":"<p>Plot each life</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>TransformedDataset</code> <p>A transformed dataset</p> required Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def plot_lives(ds: TransformedDataset):\n    \"\"\"\n    Plot each life\n\n    Parameters:\n        ds: A transformed dataset\n    \"\"\"\n    fig, ax = plt.subplots()\n    it = ds\n    for _, y in it:\n        ax.plot(y)\n    return ax\n</code></pre>"},{"location":"results/visualization/#ceruleo.graphics.results.plot_predictions","title":"<code>plot_predictions(result, *, ax=None, units='Hours [h]', markersize=0.7, marker='o', plot_fitted=True, model_name='', **kwargs)</code>","text":"<p>Plots the predicted and the true remaining useful lives</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Union[PredictionResult, Tuple[ndarray, ndarray]]</code> <p>A PredictionResult object or a tuple with (y_true, y_predicted)</p> required <code>ax</code> <code>Optional[Axes]</code> <p>Axis to plot. If it is missing a new figure will be created</p> <code>None</code> <code>units</code> <code>str</code> <p>Units of time to be used in the axis labels</p> <code>'Hours [h]'</code> <code>marker</code> <code>str</code> <p>Marker type</p> <code>'o'</code> <code>markersize</code> <code>float</code> <p>The size of the marker</p> <code>0.7</code> <code>plot_fitted</code> <code>bool</code> <p>Wether to plot a LS line</p> <code>True</code> <code>model_name</code> <code>str</code> <p>Name of the model</p> <code>''</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The axis on which the plot has been made</p> Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def plot_predictions(\n    result: Union[PredictionResult, Tuple[np.ndarray, np.ndarray]],\n    *,\n    ax:Optional[matplotlib.axes.Axes]=None,\n    units: str = \"Hours [h]\",\n    markersize: float = 0.7,\n    marker: str = 'o',\n    plot_fitted: bool  = True,\n    model_name:str = '',\n    **kwargs,\n) -&gt; matplotlib.axes.Axes:\n    \"\"\"\n    Plots the predicted and the true remaining useful lives\n\n    Parameters:\n        result: A PredictionResult object or a tuple with (y_true, y_predicted)\n        ax:  Axis to plot. If it is missing a new figure will be created\n        units: Units of time to be used in the axis labels\n        marker: Marker type\n        markersize: The size of the marker\n        plot_fitted: Wether to plot a LS line\n        model_name: Name of the model\n\n\n    Returns:\n        The axis on which the plot has been made\n    \"\"\"\n    if ax is None:\n        _, ax = plt.subplots(1, 1, **kwargs)\n\n\n    if isinstance(result, PredictionResult):\n        y_predicted = result.predicted_RUL\n        y_true = result.true_RUL\n    else:\n        y_true, y_predicted = result\n    ax.plot(y_predicted, marker, label=f\"Predicted {model_name}\", markersize=markersize)\n    ax.plot(y_true, label=\"True\")\n    x = 0\n\n\n    if plot_fitted:\n        try:\n            fitted = np.hstack([life.y_pred_fitted for life in split_lives(result)])\n            ax.plot(fitted, label='Fitted')\n\n        except:\n\n            pass\n    ax.set_ylabel(units)\n    ax.set_xlabel(units)\n    legend = ax.legend()\n    for l in legend.legend_handles:\n        l.set_markersize(6)\n\n\n    return ax\n</code></pre>"},{"location":"results/visualization/#ceruleo.graphics.results.plot_predictions_grid","title":"<code>plot_predictions_grid(results, ncols=3, alpha=1.0, xlabel=None, ylabel=None, **kwargs)</code>","text":"<p>Plot a matrix of predictions</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Union[PredictionResult, List[PredictionResult]]</code> <p>Dictionary with the results</p> required <code>ncols</code> <code>int</code> <p>Number of colmns in the plot</p> <code>3</code> <code>alpha</code> <code>float</code> <p>Opacity of the predicted curves</p> <code>1.0</code> <code>xlabel</code> <code>Optional[str]</code> <p>Xlabel</p> <code>None</code> <code>ylabel</code> <code>Optional[str]</code> <p>YLabel</p> <code>None</code> Return <p>The axis on which the plot has been made</p> Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def plot_predictions_grid(\n    results: Union[PredictionResult, List[PredictionResult]],\n    ncols: int = 3,\n    alpha: float =1.0,\n    xlabel: Optional[str] = None,\n    ylabel: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Plot a matrix of predictions\n\n    Parameters:\n        results: Dictionary with the results\n        ncols: Number of colmns in the plot\n        alpha: Opacity of the predicted curves\n        xlabel: Xlabel\n        ylabel: YLabel\n\n    Return:\n        The axis on which the plot has been made\n    \"\"\"\n\n    def linear_to_subindices(i, ncols):\n        row = int(i / ncols)\n        col = i % ncols\n        return row, col\n\n    if isinstance(results, PredictionResult):\n        results = [results]\n\n    init = False\n\n    for model_results in results:\n        lives_model = split_lives(model_results)\n        NROW = math.ceil(len(lives_model) / ncols)\n        if not init:\n            fig, ax = plt.subplots(NROW, ncols, squeeze=False, **kwargs)\n\n        for i, life in enumerate(lives_model):\n            row, col = linear_to_subindices(i, ncols)\n\n            if not init:\n                ax[row, col].plot(life.time, life.y_true, label=\"True\")\n\n            ax[row, col].plot(\n                life.time, life.y_pred, label=model_results.name, alpha=alpha\n            )\n            if xlabel is not None:\n                ax[row, col].set_xlabel(xlabel)\n            if ylabel is not None:\n                ax[row, col].set_ylabel(ylabel)\n        init = True\n    for j in range(len(lives_model), NROW * ncols):\n        row, col = linear_to_subindices(j, ncols)\n        fig.delaxes(ax[row, col])\n\n    for a in ax.flatten():\n        a.legend()\n    return ax\n</code></pre>"},{"location":"results/visualization/#ceruleo.graphics.results.plot_unexpected_breaks","title":"<code>plot_unexpected_breaks(results_dict, max_window, n, ax=None, units='', add_shade=True, **kwargs)</code>","text":"<p>Plot the risk of unexpected breaks with respect to the maintenance window</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>dict</code> <p>Dictionary with the results</p> required <code>max_window</code> <code>int</code> <p>Maximum size of the maintenance windows</p> required <code>n</code> <code>int</code> <p>Number of points used to evaluate the window size</p> required <code>ax</code> <code>Optional[Axes]</code> <p>axis on which to draw, by default None</p> <code>None</code> <code>units</code> <code>Optional[str]</code> <p>Units to use in the xlabel, by default \"\"</p> <code>''</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The axis in which the plot was made</p> Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def plot_unexpected_breaks(\n    results_dict: dict,\n    max_window: int,\n    n: int,\n    ax: Optional[matplotlib.axes.Axes] = None,\n    units: Optional[str] = \"\",\n    add_shade: bool = True,\n    **kwargs,\n) -&gt; matplotlib.axes.Axes:\n    \"\"\"\n    Plot the risk of unexpected breaks with respect to the maintenance window\n\n    Parameters:\n        results_dict: Dictionary with the results\n        max_window: Maximum size of the maintenance windows\n        n: Number of points used to evaluate the window size\n        ax: axis on which to draw, by default None\n        units: Units to use in the xlabel, by default \"\"\n\n    Returns:\n        The axis in which the plot was made\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(**kwargs)\n    n_models = len(results_dict)\n    colors = sns.color_palette(\"hls\", n_models)\n    for i, model_name in enumerate(results_dict.keys()):\n        m, mean_ub, std_ub = unexpected_breaks(results_dict[model_name], max_window, n)\n        ax.plot(m, mean_ub, label=model_name, color=colors[i])\n        if add_shade:\n            ax.fill_between(m, mean_ub+std_ub, mean_ub-std_ub, alpha=0.1, color=colors[i])\n\n    ax.set_title(\"Unexpected breaks\")\n    ax.set_xlabel(\"Fault window size\" + units)\n    ax.set_ylabel(\"Risk of breakage\")\n    ax.legend()\n    return ax\n</code></pre>"},{"location":"results/visualization/#ceruleo.graphics.results.shadedline_plot_errors_wrt_RUL","title":"<code>shadedline_plot_errors_wrt_RUL(results_dict, nbins, y_axis_label=None, x_axis_label=None, ax=None, **kwargs)</code>","text":"<p>Shaded line</p> <p>Parameters:</p> Name Type Description Default <code>results_dict</code> <code>dict</code> <p>Dictionary with the results for the model</p> required <code>nbins</code> <code>int</code> <p>Number of bins</p> required <code>y_axis_label</code> <code>Optional[str]</code> <p>Y label</p> <code>None</code> <code>x_axis_label</code> <code>Optional[str]</code> <p>X label</p> <code>None</code> <code>ax</code> <code>Optional[Axes]</code> <p>Plot axis </p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>The plot axis</p> Source code in <code>ceruleo/graphics/results.py</code> <pre><code>def shadedline_plot_errors_wrt_RUL(\n    results_dict: dict,\n    nbins: int,\n    y_axis_label: Optional[str] =None,\n    x_axis_label: Optional[str] =None,\n    ax: Optional[matplotlib.axes.Axes] =None,\n    **kwargs,\n) -&gt; matplotlib.axes.Axes:\n    \"\"\"\n    Shaded line\n\n    Parameters:\n        results_dict: Dictionary with the results for the model\n        nbins: Number of bins\n        y_axis_label: Y label\n        x_axis_label: X label\n        ax: Plot axis \n\n    Returns:\n        The plot axis\n    \"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots(**kwargs)\n\n    bin_edges, model_results = models_cv_results(results_dict, nbins)\n    return _cv_shadedline_plot_errors_wrt_RUL_multiple_models(\n        bin_edges,\n        model_results,\n        ax=ax,\n        bins=nbins,\n        y_axis_label=y_axis_label,\n        x_axis_label=x_axis_label,\n    )\n</code></pre>"},{"location":"transformation/pipeline/","title":"Pipeline","text":""},{"location":"transformation/pipeline/#pipeline","title":"Pipeline","text":""},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transformation pipeline</p> <p>Parameters:</p> Name Type Description Default <code>final_step</code> <p>The final step of the transformation</p> required <code>cache_type</code> <code>CacheStoreType</code> <p>Cache storage mode</p> <code>MEMORY</code> Source code in <code>ceruleo/transformation/functional/pipeline/pipeline.py</code> <pre><code>class Pipeline(BaseEstimator, TransformerMixin):\n    \"\"\"Transformation pipeline\n\n    Parameters:\n        final_step: The final step of the transformation\n        cache_type: Cache storage mode\n    \"\"\"\n\n    final_step: TransformerStep\n    fitted_: bool\n    cache_type: CacheStoreType\n    runner: CachedPipelineRunner\n\n    def __init__(self, final_step, cache_type: CacheStoreType = CacheStoreType.MEMORY):\n        self.final_step = final_step\n        self.fitted_ = False\n        self.cache_type = cache_type\n        self.runner = CachedPipelineRunner(final_step, cache_type)\n\n    def find_node(\n        self, name: str\n    ) -&gt; Union[List[TransformerStep], TransformerStep, None]:\n        \"\"\"Find a transformation node given a name\n\n        Parameters:\n            name: Name of the step to find\n\n        Returns:\n\n            steps: Steps located in the pipeline.\n\n        \"\"\"\n        matches = []\n        for node in dfs_iterator(self.final_step):\n            if node.name == name:\n                matches.append(node)\n\n        if len(matches) == 1:\n            return matches[0]\n        elif len(matches) &gt; 1:\n            return matches\n        else:\n            return None\n\n    def fit(\n        self,\n        dataset: Union[AbstractPDMDataset, pd.DataFrame],\n        show_progress: bool = False,\n    ):\n        \"\"\"Fit a pipeline using a dataset\n\n        The CachedPipelineRunner is called to fit\n\n        Parameters:\n\n            dataset: A dataset of a run-to-failure cycle\n            show_progress: Wether to show the progress when fitting\n\n        Returns:\n            s : Pipeline\n        \"\"\"\n        if isinstance(dataset, pd.DataFrame):\n            dataset = [dataset]\n        c = self.runner.fit(dataset, show_progress=show_progress)\n        self.column_names = c.columns\n        self.fitted_ = True\n\n        return self\n\n    def partial_fit(\n        self,\n        dataset: Union[AbstractPDMDataset, pd.DataFrame],\n        show_progress: bool = False,\n    ):\n        self.fit(dataset, show_progress=show_progress)\n\n    def transform(self, df: Union[pd.DataFrame, Iterable[pd.DataFrame]]):\n        \"\"\"Transform a run-to-cycle failure or a dataset\n\n        The CachedPipelineRunner is called to transform\n\n        Parameters:\n\n            df: A dataset of a run-to-failure cycle\n\n        Returns:\n            s : list of data frames\n        \"\"\"\n        return self.runner.transform(df)\n\n    def description(self):\n        data = []\n        for node in topological_sort_iterator(self):\n            data.append(node.description())\n        return data\n\n    def get_params(self, deep: bool = False):\n        params = {\"cache_type\": self.cache_type, \"final_step\": self.final_step}\n        if deep:\n            for node in topological_sort_iterator(self):\n                p = node.get_params(deep)\n                for k in p.keys():\n                    params[f\"{node.name}__{k}\"] = p[k]\n        return params\n\n    def __sklearn_clone__(self):\n        g = {node: sklearn_clone(node) for node  in dfs_iterator(self.final_step)}\n        for k in g.keys():\n            g[k].clear_connections()\n        for node in dfs_iterator(self.final_step):\n            for next_node in node.next:\n                g[node].add_next(g[next_node])\n\n        return Pipeline(\n            final_step=g[self.final_step],\n            cache_type=self.cache_type\n        )\n</code></pre>"},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.pipeline.Pipeline.find_node","title":"<code>find_node(name)</code>","text":"<p>Find a transformation node given a name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the step to find</p> required <pre><code>steps: Steps located in the pipeline.\n</code></pre> Source code in <code>ceruleo/transformation/functional/pipeline/pipeline.py</code> <pre><code>def find_node(\n    self, name: str\n) -&gt; Union[List[TransformerStep], TransformerStep, None]:\n    \"\"\"Find a transformation node given a name\n\n    Parameters:\n        name: Name of the step to find\n\n    Returns:\n\n        steps: Steps located in the pipeline.\n\n    \"\"\"\n    matches = []\n    for node in dfs_iterator(self.final_step):\n        if node.name == name:\n            matches.append(node)\n\n    if len(matches) == 1:\n        return matches[0]\n    elif len(matches) &gt; 1:\n        return matches\n    else:\n        return None\n</code></pre>"},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.pipeline.Pipeline.fit","title":"<code>fit(dataset, show_progress=False)</code>","text":"<p>Fit a pipeline using a dataset</p> <p>The CachedPipelineRunner is called to fit</p> <p>Parameters:</p> <pre><code>dataset: A dataset of a run-to-failure cycle\nshow_progress: Wether to show the progress when fitting\n</code></pre> <p>Returns:</p> Name Type Description <code>s</code> <p>Pipeline</p> Source code in <code>ceruleo/transformation/functional/pipeline/pipeline.py</code> <pre><code>def fit(\n    self,\n    dataset: Union[AbstractPDMDataset, pd.DataFrame],\n    show_progress: bool = False,\n):\n    \"\"\"Fit a pipeline using a dataset\n\n    The CachedPipelineRunner is called to fit\n\n    Parameters:\n\n        dataset: A dataset of a run-to-failure cycle\n        show_progress: Wether to show the progress when fitting\n\n    Returns:\n        s : Pipeline\n    \"\"\"\n    if isinstance(dataset, pd.DataFrame):\n        dataset = [dataset]\n    c = self.runner.fit(dataset, show_progress=show_progress)\n    self.column_names = c.columns\n    self.fitted_ = True\n\n    return self\n</code></pre>"},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.pipeline.Pipeline.transform","title":"<code>transform(df)</code>","text":"<p>Transform a run-to-cycle failure or a dataset</p> <p>The CachedPipelineRunner is called to transform</p> <p>Parameters:</p> <pre><code>df: A dataset of a run-to-failure cycle\n</code></pre> <p>Returns:</p> Name Type Description <code>s</code> <p>list of data frames</p> Source code in <code>ceruleo/transformation/functional/pipeline/pipeline.py</code> <pre><code>def transform(self, df: Union[pd.DataFrame, Iterable[pd.DataFrame]]):\n    \"\"\"Transform a run-to-cycle failure or a dataset\n\n    The CachedPipelineRunner is called to transform\n\n    Parameters:\n\n        df: A dataset of a run-to-failure cycle\n\n    Returns:\n        s : list of data frames\n    \"\"\"\n    return self.runner.transform(df)\n</code></pre>"},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.pipeline.make_pipeline","title":"<code>make_pipeline(*steps, cache_type=CacheStoreType.MEMORY)</code>","text":"<p>Build a pipeline</p> <p>Example:</p> <pre><code>make_pipeline(\n    ByNameFeatureSelector(features=FEATURES),\n    Clip(lower=-2, upper=2),\n    IndexMeanResampler(rule='500s')\n)\n</code></pre> <p>Parameters:</p> <pre><code>steps: List of steps\ncache_type: Where to store the pipeline intermediate steps\n</code></pre> <p>Returns:</p> <pre><code>TemporisPipeline: The created pipeline\n</code></pre> Source code in <code>ceruleo/transformation/functional/pipeline/pipeline.py</code> <pre><code>def make_pipeline(\n    *steps, cache_type: CacheStoreType = CacheStoreType.MEMORY\n) -&gt; Pipeline:\n    \"\"\"Build a pipeline\n\n    Example:\n\n        make_pipeline(\n            ByNameFeatureSelector(features=FEATURES),\n            Clip(lower=-2, upper=2),\n            IndexMeanResampler(rule='500s')\n        )\n\n    Parameters:\n\n        steps: List of steps\n        cache_type: Where to store the pipeline intermediate steps\n\n    Returns:\n\n        TemporisPipeline: The created pipeline\n    \"\"\"\n    step = steps[0]\n    for next_step in steps[1:]:\n        step = next_step(step)\n\n    return Pipeline(step, cache_type=cache_type)\n</code></pre>"},{"location":"transformation/pipeline/#cache","title":"Cache","text":""},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.cache_store.CacheStoreType","title":"<code>CacheStoreType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Cache store modes</p> <p>Values:</p> <pre><code>SHELVE = 1\nMEMORY = 2\n</code></pre> Source code in <code>ceruleo/transformation/functional/pipeline/cache_store.py</code> <pre><code>class CacheStoreType(Enum):\n    \"\"\"Cache store modes\n\n    Values:\n\n        SHELVE = 1\n        MEMORY = 2    \n    \"\"\"\n    SHELVE = 1\n    MEMORY = 2\n</code></pre>"},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.cache_store.GraphTraversalAbstractStore","title":"<code>GraphTraversalAbstractStore</code>","text":"<p>Abstract Cache for the graph traversal</p> Source code in <code>ceruleo/transformation/functional/pipeline/cache_store.py</code> <pre><code>class GraphTraversalAbstractStore:\n    \"\"\"Abstract Cache for the graph traversal\n    \"\"\"\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def close(self):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def keys(self):\n        raise NotImplementedError\n\n    def pop(self, k):\n        raise NotImplementedError\n</code></pre>"},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.cache_store.GraphTraversalCacheMemory","title":"<code>GraphTraversalCacheMemory</code>","text":"<p>Cache all the intermediate steps in RAM</p> Source code in <code>ceruleo/transformation/functional/pipeline/cache_store.py</code> <pre><code>class GraphTraversalCacheMemory:\n    \"\"\"Cache all the intermediate steps in RAM\n    \"\"\"\n    def __init__(self):\n        self.store = {}\n\n    def __getitem__(self, k):\n        return self.store[k]\n\n    def __setitem__(self, k, v):\n        self.store[k] = v\n\n    def close(self):\n        self.store = {}\n\n    def reset(self):\n        self.store = {}\n\n    def keys(self):\n        return self.store.keys()\n\n    def pop(self, k):\n        return self.store.pop(k)\n</code></pre>"},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.cache_store.GraphTraversalCacheShelveStore","title":"<code>GraphTraversalCacheShelveStore</code>","text":"<p>Cache all the intermediate steps in a Shelve Store</p> <p>Parameters:</p> <pre><code>cache_path: Path where the case is stored\n</code></pre> Source code in <code>ceruleo/transformation/functional/pipeline/cache_store.py</code> <pre><code>class GraphTraversalCacheShelveStore:\n    \"\"\"Cache all the intermediate steps in a Shelve Store\n\n    Parameters:\n\n        cache_path: Path where the case is stored\n    \"\"\"\n    def __init__(self, cache_path: Path = CACHE_PATH):\n        filename = \"\".join(str(uuid.uuid4()).split(\"-\"))\n        self.cache_path = cache_path / \"GraphTraversalCache\" / filename / \"data\"\n        self.cache_path.parent.mkdir(exist_ok=True, parents=True)\n        self.transformed_cache = shelve.open(str(self.cache_path))\n\n    def __getitem__(self, k):\n        return self.transformed_cache[k]\n\n    def __setitem__(self, k, v):\n        self.transformed_cache[k] = v\n\n    def close(self):\n        if self.cache_path.parent.is_dir():\n            self.transformed_cache.close()\n            shutil.rmtree(self.cache_path.parent)\n\n    def reset(self):\n        self.transformed_cache.close()\n        self.transformed_cache = shelve.open(str(self.cache_path))\n\n    def keys(self):\n        return self.transformed_cache.keys()\n\n    def pop(self, k):\n        return self.transformed_cache.pop(k)\n</code></pre>"},{"location":"transformation/pipeline/#cache_1","title":"Cache","text":""},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.runner.CachedPipelineRunner","title":"<code>CachedPipelineRunner</code>","text":"<p>Performs an execution of the transformation graph caching the intermediate results</p> <p>Parameters:</p> <pre><code>final_step: Last step of the graph\ncache_type: Mode for storing the cache\n</code></pre> Source code in <code>ceruleo/transformation/functional/pipeline/runner.py</code> <pre><code>class CachedPipelineRunner:\n    \"\"\"Performs an execution of the transformation graph caching the intermediate results\n\n    Parameters:\n\n        final_step: Last step of the graph\n        cache_type: Mode for storing the cache\n    \"\"\"\n\n    def __init__(\n        self,\n        final_step: TransformerStep,\n        cache_type: CacheStoreType = CacheStoreType.SHELVE,\n    ):\n\n        self.final_step = final_step\n        self.root_nodes = root_nodes(final_step)\n        self.cache_type = cache_type\n\n    def _run(\n        self,\n        dataset: Iterable[pd.DataFrame],\n        fit: bool = True,\n        show_progress: bool = False,\n    ):\n        dataset_size = len(dataset)\n\n        with CachedGraphTraversal(\n            self.root_nodes, dataset, cache_type=self.cache_type\n        ) as cache:\n            for node in topological_sort_iterator(self.final_step):\n                if isinstance(node, TransformerStep) and fit:\n                    if node.prefer_partial_fit:\n                        for dataset_element in range(dataset_size):\n                            d = cache.state_up_to(node, dataset_element)\n                            node.partial_fit(d)\n                    else:\n                        data = pd.concat(\n                            [\n                                cache.state_up_to(node, dataset_element)\n                                for dataset_element in range(dataset_size)\n                            ]\n                        )\n                        node.fit(data)\n\n                dataset_size = len(dataset)\n                # if dataset_size &gt; 1:\n                # self._parallel_transform_step(cache, node, dataset_size, show_progress)\n                # else:\n                self._transform_step(cache, node, dataset_size, show_progress)\n\n            last_state_key = cache.get_keys_of(None)[0]\n            return cache.transformed_cache[last_state_key]\n\n    def fit(self, dataset: Iterable[pd.DataFrame], show_progress: bool = False):\n        return self._run(dataset, fit=True, show_progress=show_progress)\n\n    def _update_step(self, cache, node, dataset_element, new_element):\n        cache.clean_state_up_to(node, dataset_element)\n\n        if len(node.next) &gt; 0:\n            for n in node.next:\n                cache.store(n, node, dataset_element, new_element)\n        else:\n            cache.store(None, node, dataset_element, new_element)\n\n    def _parallel_transform_step(\n        self, cache: CachedGraphTraversal, node, dataset_size: int, show_progress: bool\n    ):\n        if show_progress:\n            bar = tqdm(range(dataset_size))\n            bar.set_description(node.name)\n        else:\n            bar = range(dataset_size)\n\n        producers = []\n\n        queue = JoinableQueue(dataset_size)\n        for dataset_element in range(dataset_size):\n            old_element = cache.state_up_to(\n                node,\n                dataset_element,\n            )\n            producers.append(\n                Process(\n                    target=_transform, args=(node, old_element, dataset_element, queue)\n                )\n            )\n        for p in producers:\n            p.start()\n\n        for _ in bar:\n            dataset_element, new_element = queue.get()\n            queue.task_done()\n            self._update_step(cache, node, dataset_element, new_element)\n        cache.remove_state(node)\n        for p in producers:\n            p.join()\n        queue.join()\n\n    def _transform_step(\n        self, cache: CachedGraphTraversal, node, dataset_size: int, show_progress: bool\n    ):\n        if show_progress:\n            bar = tqdm(range(dataset_size))\n            bar.set_description(node.name)\n        else:\n            bar = range(dataset_size)\n        try:\n            for dataset_element in bar:\n                old_element = cache.state_up_to(node, dataset_element)\n                new_element = node.transform(old_element)\n                self._update_step(cache, node, dataset_element, new_element)\n        except Exception as e:\n            logger.error(f\"There was an error when transforming with {node.name}\")\n            raise\n\n    def transform(self, df: Union[pd.DataFrame, Iterable[pd.DataFrame]]):\n        if isinstance(df, pd.DataFrame):\n            return self._run([df], fit=False)\n        else:\n            return self._run(df, fit=False)\n</code></pre>"},{"location":"transformation/pipeline/#traversal","title":"Traversal","text":""},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.traversal.CachedGraphTraversal","title":"<code>CachedGraphTraversal</code>","text":"<p>Iterator for a graph nodes. </p> <p>The cache data structures has the following form Current Node -&gt; Previous Nodes -&gt; [Transformed Dataset]</p> <ul> <li>cache[n]:     contains a dict with one key for each previous node</li> <li>cachen]     A list with each element of the dataset transformed in     up to n.previous[0]</li> </ul> <p>Parameters:</p> <pre><code>root_nodes: Initial nodes of the graph\ndataset: Each node visit the dataset\ncache_path: Where to store the cache\ncache_type: Mode for storing the intermediate steps\n</code></pre> Source code in <code>ceruleo/transformation/functional/pipeline/traversal.py</code> <pre><code>class CachedGraphTraversal:\n    \"\"\"Iterator for a graph nodes. \n\n\n    The cache data structures has the following form\n    Current Node -&gt; Previous Nodes -&gt; [Transformed Dataset]\n\n    * cache[n]:\n        contains a dict with one key for each previous node\n    * cache[n][n.previous[0]]\n        A list with each element of the dataset transformed in\n        up to n.previous[0]\n\n\n    Parameters:\n\n        root_nodes: Initial nodes of the graph\n        dataset: Each node visit the dataset\n        cache_path: Where to store the cache\n        cache_type: Mode for storing the intermediate steps\n\n    \"\"\"\n    def __init__(\n        self,\n        root_nodes,\n        dataset,\n        cache_path: Optional[Path] = CACHE_PATH,\n        cache_type: CacheStoreType = CacheStoreType.SHELVE,\n    ):\n        if cache_type == CacheStoreType.SHELVE:\n            self.transformed_cache = GraphTraversalCacheShelveStore(cache_path)\n        elif cache_type == CacheStoreType.MEMORY:\n            self.transformed_cache = GraphTraversalCacheMemory()\n\n        for r in root_nodes:\n            for i, df in enumerate(dataset):\n                self.transformed_cache[encode_tuple((r, None, i))] = df\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.transformed_cache.close()\n\n    def clear_cache(self):\n        self.transformed_cache.claer()\n\n    def state_up_to(self, current_node: TransformerStep, dataset_element: int):\n\n        previous_node = current_node.previous\n\n        if len(previous_node) &gt; 1:\n            return [\n                self.transformed_cache[encode_tuple((current_node, p, dataset_element))]\n                for p in previous_node\n            ]\n        else:\n            if len(previous_node) == 1:\n                previous_node = previous_node[0]\n            else:\n                previous_node = None\n\n            return self.transformed_cache[\n                encode_tuple((current_node, previous_node, dataset_element))\n            ]\n\n    def clean_state_up_to(self, current_node: TransformerStep, dataset_element: int):\n\n        previous_node = current_node.previous\n        for p in previous_node:\n            self.transformed_cache[\n                encode_tuple((current_node, p, dataset_element))\n            ] = None\n\n    def store(\n        self,\n        next_node: Optional[TransformerStep],\n        node: TransformerStep,\n        dataset_element: int,\n        new_element: pd.DataFrame,\n    ):\n        self.transformed_cache[\n            encode_tuple((next_node, node, dataset_element))\n        ] = new_element\n\n    def remove_state(self, nodes: Union[TransformerStep, List[TransformerStep]]):\n        if not isinstance(nodes, list):\n            nodes = [nodes]\n        for n in nodes:\n            keys_to_remove = self.get_keys_of(n)\n            for k in keys_to_remove:\n                self.transformed_cache.pop(k)\n\n    def get_keys_of(self, n):\n        return [\n            k\n            for k in self.transformed_cache.keys()\n            if decode_tuple(k)[0] == str(hash(n))\n        ]\n</code></pre>"},{"location":"transformation/pipeline/#utils","title":"Utils","text":""},{"location":"transformation/pipeline/#ceruleo.transformation.functional.pipeline.utils.plot_pipeline","title":"<code>plot_pipeline(pipe, name)</code>","text":"<p>Plot the transformation pipeline</p> <p>Parameters:</p> <pre><code>pipe: The pipeline\nname: Title of the graphic\n</code></pre> <p>Returns:</p> <pre><code>graphic: the diagram\n</code></pre> Source code in <code>ceruleo/transformation/functional/pipeline/utils.py</code> <pre><code>def plot_pipeline(pipe: \"TemporisPipeline\", name: str):\n    \"\"\"Plot the transformation pipeline\n\n    Parameters:\n\n        pipe: The pipeline\n        name: Title of the graphic\n\n    Returns:\n\n        graphic: the diagram\n\n    \"\"\"\n    import graphviz\n    from ceruleo.transformation.functional.pipeline.pipeline import \\\n        Pipeline\n\n    dot = graphviz.Digraph(name, comment=\"Transformation graph\")\n\n    node_name = {}\n    for i, node in enumerate(nodes(pipe)):\n        node_name[node] = str(i) + node.name\n        dot.node(str(i) + node.name, label=str(node))\n\n    for (e1, e2) in edges(pipe):\n        dot.edge(node_name[e1], node_name[e2])\n\n    return dot\n</code></pre>"},{"location":"transformation/transformers/","title":"Trasformers","text":""},{"location":"transformation/transformers/#transformer","title":"Transformer","text":"<p>The transformer is a high-level class that hold at least two transformation pipelines</p> <ul> <li>One related to the transformation of the input of the model</li> <li>The other related to the target of the model.</li> </ul> <p>It allows accessing the information of the transformed data and is the object that uses the dataset iterators to transform the data before feeding it to the model.</p>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.Transformer","title":"<code>Transformer</code>","text":"<p>Transform each life</p> <p>The transformer class is the highest level class of the transformer API. It contains Transformation Pipelines for the input data and the target, and provides mechanism to inspect the structure of the transformed data.</p> <p>Parameters:</p> <pre><code>pipelineX: Pipeline that will be applied to the run-to-cycle data\npipelineY: Pipeline that will be applied to the target.\npipelineMetadata: Pipeline that will be used to extract additional\n                    data from the lives information, by default None\n</code></pre> Source code in <code>ceruleo/transformation/functional/transformers.py</code> <pre><code>class Transformer:\n    \"\"\"Transform each life\n\n    The transformer class is the highest level class of the transformer API.\n    It contains Transformation Pipelines for the input data and the target,\n    and provides mechanism to inspect the structure of the transformed data.\n\n    Parameters:\n\n        pipelineX: Pipeline that will be applied to the run-to-cycle data\n        pipelineY: Pipeline that will be applied to the target.\n        pipelineMetadata: Pipeline that will be used to extract additional\n                            data from the lives information, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        pipelineX: Union[Pipeline, TransformerStep],\n        pipelineY: Optional[Union[Pipeline, TransformerStep]] = None,\n        pipelineMetadata: Optional[Union[Pipeline, TransformerStep]] = None,\n        cache_type: CacheStoreType = CacheStoreType.MEMORY,\n    ):\n        def ensure_pipeline(x, cache_type: CacheStoreType):\n            if isinstance(x, Pipeline):\n                return x\n            return Pipeline(x, cache_type=cache_type)\n        self.cache_type = cache_type\n        self.pipelineX = ensure_pipeline(pipelineX, cache_type)\n        if pipelineY is not None:\n            self.pipelineY = ensure_pipeline(pipelineY, cache_type)\n        else:\n            self.pipelineY = None\n        self.pipelineMetadata = (\n            ensure_pipeline(pipelineMetadata, cache_type)\n            if pipelineMetadata is not None\n            else None\n        )\n        self.features = None\n        self.fitted_ = False\n\n    def _process_selected_features(self):\n        if self.pipelineX[\"selector\"] is not None:\n            selected_columns = self.pipelineX[\"selector\"].get_support(indices=True)\n            self.features = [self.features[i] for i in selected_columns]\n\n    def clone(self):\n        return copy.deepcopy(self)\n\n    def fit(self, dataset, show_progress: bool = False):\n        \"\"\"Fit the transformer with a given dataset.\n\n        The transformer will fit the X transformer,\n        the Y transformer and the metadata transformer\n\n        Parameters:\n            dataset:\n\n        \"\"\"\n        logger.debug(\"Fitting Transformer\")\n\n        self.pipelineX.fit(dataset, show_progress=show_progress)\n        if self.pipelineY is not None:\n            self.pipelineY.fit(dataset, show_progress=show_progress)\n        if self.pipelineMetadata is not None:\n            self.pipelineMetadata.fit(dataset)\n\n        if not isinstance(dataset, pd.DataFrame):\n            self.minimal_df = dataset[0].head(n=20)\n        else:\n            self.minimal_df = dataset.head(n=20)\n        X = self.pipelineX.transform(self.minimal_df)\n        self.number_of_features_ = X.shape[1]\n        self.fitted_ = True\n        self.column_names = self._compute_column_names()\n        return self\n\n    def transform(self, life: pd.DataFrame):\n        \"\"\"Transform a life and obtain the input data, the target and the metadata\n\n        Parameters:\n            life: A life in a form of a DataFrame\n\n        Returns:\n\n            Tuple[np.array, np.array, np.array]\n                * The first element consists of the input transformed\n                * The second element consits of the target transformed\n                * The third element consists of the metadata\n        \"\"\"\n        check_is_fitted(self, \"fitted_\")\n        return (\n            self.transformX(life),\n            self.transformY(life),\n            self.transformMetadata(life),\n        )\n\n    def fit_map(self, dataset, show_progress: bool = False) -&gt; \"TransformedDataset\":\n        self.fit(dataset, show_progress=show_progress)\n        return dataset.map(self)\n\n    def transformMetadata(self, df: pd.DataFrame) -&gt; Optional[any]:\n        if self.pipelineMetadata is not None:\n            return self.pipelineMetadata.transform(df)\n        else:\n            return None\n\n    def transformY(self, life: pd.DataFrame) -&gt; np.array:\n        \"\"\"Get the transformed target from a life\n\n        Parameters\n\n        life: A run-to-failrue cycle in a form of a DataFrame\n\n        Returns\n            t: Target obtained from the life\n        \"\"\"\n        if self.pipelineY is not None:\n            return self.pipelineY.transform(life)\n        else:\n            return None\n\n    def transformX(self, life: pd.DataFrame) -&gt; np.array:\n        \"\"\"Get the transformer input data\n\n        Parameters\n\n            life: A life i an form of a DataFrame\n\n        Returns\n\n            t: Input data transformed\n        \"\"\"\n        return self.pipelineX.transform(life)\n\n    def columns(self) -&gt; List[str]:\n        \"\"\"Columns names after transformation\n\n        Returns:\n\n            c: columns\n        \"\"\"\n        return self.column_names\n\n    @property\n    def n_features(self) -&gt; int:\n        \"\"\"Number of features after transformation\n\n        Returns:\n\n            n: Number of features\n        \"\"\"\n        return self.number_of_features_\n\n    def _compute_column_names(self):\n        return self.pipelineX.column_names\n\n    def description(self):\n        return {\n            \"features\": self.features,\n            \"pipelineX\": transformer_info(self.pipelineX),\n            \"pipelineY\": transformer_info(self.pipelineY),\n        }\n\n    def __str__(self):\n        return str(self.description())\n\n    def get_params(self, deep: bool = False):\n        params = {\n            \"pipelineX\": self.pipelineX,\n            \"pipelineY\": self.pipelineY,\n            \"pipelineMetadata\": self.pipelineMetadata,\n            \"cache_type\": self.cache_type,\n        }\n        if deep:\n            paramsX = self.pipelineX.get_params(deep)\n            paramsY = self.pipelineY.get_params(deep)\n            for k in paramsX.keys():\n                params[f\"pipeline_X__{k}\"] = paramsX[k]\n            for k in paramsY.keys():\n                params[f\"pipeline_Y__{k}\"] = paramsY[k]\n\n        return params\n\n    def set_params(self, **params):\n        pipeline_X_params = {}\n        pipeline_Y_params = {}\n        for k in params.keys():\n            if k.starts_with(\"pipeline_X__\"):\n                new_key = \"__\".join(k.split(\"__\")[1:])\n                pipeline_X_params[new_key] = params[k]\n            elif k.starts_with(\"pipeline_Y__\"):\n                new_key = \"__\".join(k.split(\"__\")[1:])\n                pipeline_Y_params[new_key] = params[k]\n        self.pipelineX = self.pipelineX.set_params(pipeline_X_params)\n        self.pipelineY = self.pipelineY.set_params(pipeline_Y_params)\n        return self\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.Transformer.n_features","title":"<code>n_features: int</code>  <code>property</code>","text":"<p>Number of features after transformation</p> <p>Returns:</p> <pre><code>n: Number of features\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.Transformer.columns","title":"<code>columns()</code>","text":"<p>Columns names after transformation</p> <p>Returns:</p> <pre><code>c: columns\n</code></pre> Source code in <code>ceruleo/transformation/functional/transformers.py</code> <pre><code>def columns(self) -&gt; List[str]:\n    \"\"\"Columns names after transformation\n\n    Returns:\n\n        c: columns\n    \"\"\"\n    return self.column_names\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.Transformer.fit","title":"<code>fit(dataset, show_progress=False)</code>","text":"<p>Fit the transformer with a given dataset.</p> <p>The transformer will fit the X transformer, the Y transformer and the metadata transformer</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> required Source code in <code>ceruleo/transformation/functional/transformers.py</code> <pre><code>def fit(self, dataset, show_progress: bool = False):\n    \"\"\"Fit the transformer with a given dataset.\n\n    The transformer will fit the X transformer,\n    the Y transformer and the metadata transformer\n\n    Parameters:\n        dataset:\n\n    \"\"\"\n    logger.debug(\"Fitting Transformer\")\n\n    self.pipelineX.fit(dataset, show_progress=show_progress)\n    if self.pipelineY is not None:\n        self.pipelineY.fit(dataset, show_progress=show_progress)\n    if self.pipelineMetadata is not None:\n        self.pipelineMetadata.fit(dataset)\n\n    if not isinstance(dataset, pd.DataFrame):\n        self.minimal_df = dataset[0].head(n=20)\n    else:\n        self.minimal_df = dataset.head(n=20)\n    X = self.pipelineX.transform(self.minimal_df)\n    self.number_of_features_ = X.shape[1]\n    self.fitted_ = True\n    self.column_names = self._compute_column_names()\n    return self\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.Transformer.transform","title":"<code>transform(life)</code>","text":"<p>Transform a life and obtain the input data, the target and the metadata</p> <p>Parameters:</p> Name Type Description Default <code>life</code> <code>DataFrame</code> <p>A life in a form of a DataFrame</p> required <pre><code>Tuple[np.array, np.array, np.array]\n    * The first element consists of the input transformed\n    * The second element consits of the target transformed\n    * The third element consists of the metadata\n</code></pre> Source code in <code>ceruleo/transformation/functional/transformers.py</code> <pre><code>def transform(self, life: pd.DataFrame):\n    \"\"\"Transform a life and obtain the input data, the target and the metadata\n\n    Parameters:\n        life: A life in a form of a DataFrame\n\n    Returns:\n\n        Tuple[np.array, np.array, np.array]\n            * The first element consists of the input transformed\n            * The second element consits of the target transformed\n            * The third element consists of the metadata\n    \"\"\"\n    check_is_fitted(self, \"fitted_\")\n    return (\n        self.transformX(life),\n        self.transformY(life),\n        self.transformMetadata(life),\n    )\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.Transformer.transformX","title":"<code>transformX(life)</code>","text":"<p>Get the transformer input data</p> <p>Parameters</p> <pre><code>life: A life i an form of a DataFrame\n</code></pre> <p>Returns</p> <pre><code>t: Input data transformed\n</code></pre> Source code in <code>ceruleo/transformation/functional/transformers.py</code> <pre><code>def transformX(self, life: pd.DataFrame) -&gt; np.array:\n    \"\"\"Get the transformer input data\n\n    Parameters\n\n        life: A life i an form of a DataFrame\n\n    Returns\n\n        t: Input data transformed\n    \"\"\"\n    return self.pipelineX.transform(life)\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.Transformer.transformY","title":"<code>transformY(life)</code>","text":"<p>Get the transformed target from a life</p> <p>Parameters</p> <p>life: A run-to-failrue cycle in a form of a DataFrame</p> <p>Returns     t: Target obtained from the life</p> Source code in <code>ceruleo/transformation/functional/transformers.py</code> <pre><code>def transformY(self, life: pd.DataFrame) -&gt; np.array:\n    \"\"\"Get the transformed target from a life\n\n    Parameters\n\n    life: A run-to-failrue cycle in a form of a DataFrame\n\n    Returns\n        t: Target obtained from the life\n    \"\"\"\n    if self.pipelineY is not None:\n        return self.pipelineY.transform(life)\n    else:\n        return None\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.TransformerIdentity","title":"<code>TransformerIdentity(rul_column='RUL')</code>","text":"<p>Return the Transformer</p> <p>Parameters:</p> <pre><code>rul_column : Name of the RUL Column\n</code></pre> <p>Returns:</p> <pre><code>TransformerIdentity: An identity f(x)=x transformer\n</code></pre> Source code in <code>ceruleo/transformation/functional/transformers.py</code> <pre><code>def TransformerIdentity(rul_column: str = \"RUL\") -&gt; Transformer:\n    \"\"\"Return the Transformer\n\n    Parameters:\n\n        rul_column : Name of the RUL Column\n\n    Returns:\n\n        TransformerIdentity: An identity f(x)=x transformer\n    \"\"\"\n    from ceruleo.transformation.features.selection import ByNameFeatureSelector\n    from ceruleo.transformation.utils import IdentityTransformerStep\n\n    return Transformer(\n        IdentityTransformerStep(), ByNameFeatureSelector(features=[rul_column])\n    )\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformers.transformer_info","title":"<code>transformer_info(transformer)</code>","text":"<p>Obtains the transformer information in a serializable format</p> <p>Parameters:</p> <pre><code>transformer: The transformer step, or pipeline to obtain their underlying information\n</code></pre> <p>Returns:</p> Type Description <p>dict</p> <pre><code>ValueError\n    If the transformer passed as an argument doesn't have\n    the get_params method.\n</code></pre> Source code in <code>ceruleo/transformation/functional/transformers.py</code> <pre><code>def transformer_info(transformer: Optional[Pipeline]):\n    \"\"\"Obtains the transformer information in a serializable format\n\n    Parameters:\n\n        transformer: The transformer step, or pipeline to obtain their underlying information\n\n    Returns:\n        dict\n\n    Raises:\n\n        ValueError\n            If the transformer passed as an argument doesn't have\n            the get_params method.\n    \"\"\"\n    if transformer is None:\n        return \"Missing\"\n\n    data = []\n    Q = topological_sort_iterator(transformer)\n    for q in Q:\n        data.append(q.description())\n    return data\n</code></pre>"},{"location":"transformation/transformers/#transformer-step","title":"Transformer Step","text":"<p>Transformer step is the base class of all transformers</p> <p>The pipeline will use the steps to fit and transform the run-to-failure cycles</p>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformerstep.TransformerStep","title":"<code>TransformerStep</code>","text":"<p>               Bases: <code>TransformerStepMixin</code>, <code>TransformerMixin</code></p> <p>Base class of all transformation step</p> Source code in <code>ceruleo/transformation/functional/transformerstep.py</code> <pre><code>class TransformerStep(TransformerStepMixin, TransformerMixin):\n    \"\"\"Base class of all transformation step\n\n    \"\"\"\n    def partial_fit(self, X:pd.DataFrame, y=None) -&gt; \"TransformerStep\":\n        \"\"\"Fit a single run-to-failure cycle\n\n        Parameters:\n\n            X: Features of the run-to-failure cycle\n\n\n        Returns:\n            TransformerStep: The same step\n        \"\"\"\n        return self\n\n    def fit(self, X, y=None)  -&gt; \"TransformerStep\":\n        \"\"\"Fit the complete set of run-to-failure cycles\n\n        Parameters:\n\n            X: Features of the all the run-to-failure cycles\n\n\n        Returns:\n            TransformerStep: The same step\n        \"\"\"\n        return self\n\n    def find_feature(self, X: pd.DataFrame, name: str) -&gt; Optional[str]:\n        \"\"\"Find the feature that best maches the columns in X\n\n        Parameters:\n            X: A run-to-failure cycle\n            name: The name of the feature to find\n\n        Returns:\n            The name of the columns if it was found, else None\n\n        \"\"\"\n        matches = [c for c in X.columns if name in c]\n        if len(matches) &gt; 0:\n            return matches[0]\n        else:\n            return None\n\n    def description(self):\n        return f\"{self.name}\"\n\n\n\n\n    def __add__(self, other):\n        from ceruleo.transformation.features.operations import Sum\n        from ceruleo.transformation.utils import ensure_step\n        return Sum()([self, ensure_step(other)])\n\n    def __truediv__(self, other):\n        from ceruleo.transformation.features.operations import Divide\n        from ceruleo.transformation.utils import ensure_step\n        return Divide()([self, ensure_step(other)])\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformerstep.TransformerStep.find_feature","title":"<code>find_feature(X, name)</code>","text":"<p>Find the feature that best maches the columns in X</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A run-to-failure cycle</p> required <code>name</code> <code>str</code> <p>The name of the feature to find</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The name of the columns if it was found, else None</p> Source code in <code>ceruleo/transformation/functional/transformerstep.py</code> <pre><code>def find_feature(self, X: pd.DataFrame, name: str) -&gt; Optional[str]:\n    \"\"\"Find the feature that best maches the columns in X\n\n    Parameters:\n        X: A run-to-failure cycle\n        name: The name of the feature to find\n\n    Returns:\n        The name of the columns if it was found, else None\n\n    \"\"\"\n    matches = [c for c in X.columns if name in c]\n    if len(matches) &gt; 0:\n        return matches[0]\n    else:\n        return None\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformerstep.TransformerStep.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the complete set of run-to-failure cycles</p> <p>Parameters:</p> <pre><code>X: Features of the all the run-to-failure cycles\n</code></pre> <p>Returns:</p> Name Type Description <code>TransformerStep</code> <code>TransformerStep</code> <p>The same step</p> Source code in <code>ceruleo/transformation/functional/transformerstep.py</code> <pre><code>def fit(self, X, y=None)  -&gt; \"TransformerStep\":\n    \"\"\"Fit the complete set of run-to-failure cycles\n\n    Parameters:\n\n        X: Features of the all the run-to-failure cycles\n\n\n    Returns:\n        TransformerStep: The same step\n    \"\"\"\n    return self\n</code></pre>"},{"location":"transformation/transformers/#ceruleo.transformation.functional.transformerstep.TransformerStep.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Fit a single run-to-failure cycle</p> <p>Parameters:</p> <pre><code>X: Features of the run-to-failure cycle\n</code></pre> <p>Returns:</p> Name Type Description <code>TransformerStep</code> <code>TransformerStep</code> <p>The same step</p> Source code in <code>ceruleo/transformation/functional/transformerstep.py</code> <pre><code>def partial_fit(self, X:pd.DataFrame, y=None) -&gt; \"TransformerStep\":\n    \"\"\"Fit a single run-to-failure cycle\n\n    Parameters:\n\n        X: Features of the run-to-failure cycle\n\n\n    Returns:\n        TransformerStep: The same step\n    \"\"\"\n    return self\n</code></pre>"},{"location":"transformation/features/cast/","title":"Cast","text":""},{"location":"transformation/features/cast/#cast","title":"Cast","text":"<p>In some cases It's useful to cast the datatype of subset of columns</p>"},{"location":"transformation/features/cast/#ceruleo.transformation.features.cast.CastTo","title":"<code>CastTo</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Cast to a given datatype</p> Example <p>step = CastTo(type='float32')</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>Data Type to cast to</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/cast.py</code> <pre><code>class CastTo(TransformerStep):\n    \"\"\"Cast to a given datatype\n\n    Example:\n        step = CastTo(type='float32')\n\n    Parameters:\n        type: Data Type to cast to\n        name: Name of the step, by default None\n\n    \"\"\"\n\n    def __init__(self, *, type: str, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.type = type\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Cast to a given datatype\n\n        Parameters:\n            X: DataFrame to transform\n\n        Returns:\n            Transformed DataFrame\n        \"\"\"\n        return X.astype(self.type)\n</code></pre>"},{"location":"transformation/features/cast/#ceruleo.transformation.features.cast.CastTo.transform","title":"<code>transform(X)</code>","text":"<p>Cast to a given datatype</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>DataFrame to transform</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame</p> Source code in <code>ceruleo/transformation/features/cast.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Cast to a given datatype\n\n    Parameters:\n        X: DataFrame to transform\n\n    Returns:\n        Transformed DataFrame\n    \"\"\"\n    return X.astype(self.type)\n</code></pre>"},{"location":"transformation/features/cast/#ceruleo.transformation.features.cast.ToDateTime","title":"<code>ToDateTime</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Cast to a datetime</p> Example <p>step = ToDateTime()</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/cast.py</code> <pre><code>class ToDateTime(TransformerStep):\n    \"\"\"Cast to a datetime\n\n    Example:\n        step = ToDateTime()\n\n    Parameters:\n        name: Name of the step, by default None\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        units: str = \"s\",\n        name: Optional[str] = None,\n        columns: Optional[List[any]] = None,\n        index: bool = False,\n\n    ):\n        \"\"\"Cast to a datetime\n\n        Parameters\n        ----------\n        units : str, optional\n            The units to transform, by default \"s\"\n        name : Optional[str], optional\n            Name of the step, by default None\n        columns : Optional[List[any]], optional\n            Columns to transform, by default None\n        index : bool, optional\n            Wether to transform the index or not, by default False\n\n        Raises\n        ------\n        ValueError\n            _description_\n        ValueError\n            _description_\n        \"\"\"\n        super().__init__(name=name)\n        if index is False and columns is None:\n            raise ValueError(\"You must specify the columns to transform\")\n        if columns is not None and index is True:\n            raise ValueError(\"You can't specify columns and index at the same time\")\n        self.index = index\n        self.columns = columns\n        self.units = units\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Cast to a datetime\n\n        Parameters:\n            X: DataFrame to transform\n\n        Returns:\n            Transformed DataFrame\n        \"\"\"\n        X = X.copy()\n        if self.index:\n            X.index = pd.to_datetime(X.index, unit=self.units)\n        else:\n            X[self.columns] = X[self.columns].apply(pd.to_datetime)\n        return X\n</code></pre>"},{"location":"transformation/features/cast/#ceruleo.transformation.features.cast.ToDateTime.__init__","title":"<code>__init__(*, units='s', name=None, columns=None, index=False)</code>","text":"<p>Cast to a datetime</p>"},{"location":"transformation/features/cast/#ceruleo.transformation.features.cast.ToDateTime.__init__--parameters","title":"Parameters","text":"<p>units : str, optional     The units to transform, by default \"s\" name : Optional[str], optional     Name of the step, by default None columns : Optional[List[any]], optional     Columns to transform, by default None index : bool, optional     Wether to transform the index or not, by default False</p>"},{"location":"transformation/features/cast/#ceruleo.transformation.features.cast.ToDateTime.__init__--raises","title":"Raises","text":"<p>ValueError     description ValueError     description</p> Source code in <code>ceruleo/transformation/features/cast.py</code> <pre><code>def __init__(\n    self,\n    *,\n    units: str = \"s\",\n    name: Optional[str] = None,\n    columns: Optional[List[any]] = None,\n    index: bool = False,\n\n):\n    \"\"\"Cast to a datetime\n\n    Parameters\n    ----------\n    units : str, optional\n        The units to transform, by default \"s\"\n    name : Optional[str], optional\n        Name of the step, by default None\n    columns : Optional[List[any]], optional\n        Columns to transform, by default None\n    index : bool, optional\n        Wether to transform the index or not, by default False\n\n    Raises\n    ------\n    ValueError\n        _description_\n    ValueError\n        _description_\n    \"\"\"\n    super().__init__(name=name)\n    if index is False and columns is None:\n        raise ValueError(\"You must specify the columns to transform\")\n    if columns is not None and index is True:\n        raise ValueError(\"You can't specify columns and index at the same time\")\n    self.index = index\n    self.columns = columns\n    self.units = units\n</code></pre>"},{"location":"transformation/features/cast/#ceruleo.transformation.features.cast.ToDateTime.transform","title":"<code>transform(X)</code>","text":"<p>Cast to a datetime</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>DataFrame to transform</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed DataFrame</p> Source code in <code>ceruleo/transformation/features/cast.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Cast to a datetime\n\n    Parameters:\n        X: DataFrame to transform\n\n    Returns:\n        Transformed DataFrame\n    \"\"\"\n    X = X.copy()\n    if self.index:\n        X.index = pd.to_datetime(X.index, unit=self.units)\n    else:\n        X[self.columns] = X[self.columns].apply(pd.to_datetime)\n    return X\n</code></pre>"},{"location":"transformation/features/denoising/","title":"Denoising","text":""},{"location":"transformation/features/denoising/#denoising","title":"Denoising","text":""},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.EWMAFilter","title":"<code>EWMAFilter</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Filter each feature using EWM (Exponential Moving Window)</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>float</code> <p>Time constant of the EMA (Exponential Moving Average)</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>class EWMAFilter(TransformerStep):\n    \"\"\"\n    Filter each feature using EWM (Exponential Moving Window)\n\n    Parameters:\n        span: Time constant of the EMA (Exponential Moving Average)\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(self, span: float, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.span = span\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Return a new dataframe with the features filtered\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DatafFrame with the same index as the input with the features filtered\n        \"\"\"\n        return X.ewm(span=self.span, ignore_na=True).mean()\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.EWMAFilter.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Return a new dataframe with the features filtered</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DatafFrame with the same index as the input with the features filtered</p> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new dataframe with the features filtered\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DatafFrame with the same index as the input with the features filtered\n    \"\"\"\n    return X.ewm(span=self.span, ignore_na=True).mean()\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.GaussianFilter","title":"<code>GaussianFilter</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Apply a gaussian filter</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Size of the gaussian filter</p> required <code>std</code> <code>float</code> <p>Standard deviation of the filter</p> required <code>min_points</code> <code>int</code> <p>Minimun nomber of points of the rolling window, by default 1</p> <code>1</code> <code>center</code> <code>bool</code> <p>Wether the guassian window should be centered, by default False</p> <code>False</code> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>class GaussianFilter(TransformerStep):\n    \"\"\"\n    Apply a gaussian filter\n\n    Parameters:\n        window_size: Size of the gaussian filter\n        std: Standard deviation of the filter\n        min_points: Minimun nomber of points of the rolling window, by default 1\n        center: Wether the guassian window should be centered, by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        window_size: int,\n        std: float,\n        min_points: int = 1,\n        center: bool = False,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.window_size = window_size\n        self.std = std\n        self.center = center\n        self.min_points = min_points\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Return a new dataframe with the features filtered\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DatafFrame with the same index as the input with the features filtered\n        \"\"\"\n        return X.rolling(\n            window=self.window_size,\n            win_type=\"gaussian\",\n            center=self.center,\n            min_periods=self.min_points,\n        ).mean(std=self.std)\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.GaussianFilter.transform","title":"<code>transform(X)</code>","text":"<p>Return a new dataframe with the features filtered</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DatafFrame with the same index as the input with the features filtered</p> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new dataframe with the features filtered\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DatafFrame with the same index as the input with the features filtered\n    \"\"\"\n    return X.rolling(\n        window=self.window_size,\n        win_type=\"gaussian\",\n        center=self.center,\n        min_periods=self.min_points,\n    ).mean(std=self.std)\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.MeanFilter","title":"<code>MeanFilter</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Filter each feature using a rolling mean filter</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>Size of the rolling window</p> required <code>min_periods</code> <code>int</code> <p>Minimum number of non-null points of the rolling window, by default 15</p> <code>15</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> <code>center</code> <p>Wether the guassian window should be centered, by default False</p> <code>True</code> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>class MeanFilter(TransformerStep):\n    \"\"\"\n    Filter each feature using a rolling mean filter\n\n    Parameters:\n        window: Size of the rolling window\n        min_periods: Minimum number of non-null points of the rolling window, by default 15\n        name: Name of the step, by default None\n        center: Wether the guassian window should be centered, by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        window: int,\n        center=True,\n        min_periods: int = 15,\n        name: Optional[str] = None,\n    ):\n        super().__init__(name=name)\n        self.window = window\n        self.min_periods = min_periods\n        self.center = center\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Return a new dataframe with the features filtered\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DatafFrame with the same index as the input with the features filtered\n        \"\"\"\n        return X.rolling(\n            self.window,\n            min_periods=self.min_periods,\n            center=self.center,\n        ).mean(numeric_only=True)\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.MeanFilter.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Return a new dataframe with the features filtered</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DatafFrame with the same index as the input with the features filtered</p> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new dataframe with the features filtered\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DatafFrame with the same index as the input with the features filtered\n    \"\"\"\n    return X.rolling(\n        self.window,\n        min_periods=self.min_periods,\n        center=self.center,\n    ).mean(numeric_only=True)\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.MedianFilter","title":"<code>MedianFilter</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Filter each feature using a rolling median filter</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>Size of the rolling window</p> required <code>min_periods</code> <code>int</code> <p>Minimum number of points of the rolling window, by default 15</p> <code>15</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>class MedianFilter(TransformerStep):\n    \"\"\"Filter each feature using a rolling median filter\n\n    Parameters:\n        window: Size of the rolling window\n        min_periods: Minimum number of points of the rolling window, by default 15\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(self, window: int, min_periods: int = 15, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.window = window\n        self.min_periods = min_periods\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Return a new dataframe with the features filtered\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DatafFrame with the same index as the input with the features filtered\n        \"\"\"\n        return X.rolling(self.window, min_periods=self.min_periods).median(\n            numeric_only=True\n        )\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.MedianFilter.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Return a new dataframe with the features filtered</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DatafFrame with the same index as the input with the features filtered</p> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new dataframe with the features filtered\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DatafFrame with the same index as the input with the features filtered\n    \"\"\"\n    return X.rolling(self.window, min_periods=self.min_periods).median(\n        numeric_only=True\n    )\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.MultiDimensionalKMeans","title":"<code>MultiDimensionalKMeans</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Clusterize data points and replace each feature with the centroid feature it belongs to</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters to obtain by default 5</p> <code>5</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>class MultiDimensionalKMeans(TransformerStep):\n    \"\"\"\n    Clusterize data points and replace each feature with the centroid feature it belongs to\n\n    Parameters:\n        n_clusters: Number of clusters to obtain by default 5\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(self, n_clusters: int = 5, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.n_clusters = n_clusters\n        self.clusters = MiniBatchKMeans(n_clusters=self.n_clusters, n_init=\"auto\")\n\n    def partial_fit(self, X: pd.DataFrame):\n        \"\"\"\n        Fit the model to the input data to obtain the clusters\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        self.clusters.partial_fit(X)\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"Transform the input life with the centroid information\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame in which each point was replaced by the centroid it belongs to\n        \"\"\"\n\n        X = X.copy()\n        X[:] = self.clusters.cluster_centers_[self.clusters.predict(X)]\n        return X\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.MultiDimensionalKMeans.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Fit the model to the input data to obtain the clusters</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame):\n    \"\"\"\n    Fit the model to the input data to obtain the clusters\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    self.clusters.partial_fit(X)\n    return self\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.MultiDimensionalKMeans.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the input life with the centroid information</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame in which each point was replaced by the centroid it belongs to</p> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"Transform the input life with the centroid information\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame in which each point was replaced by the centroid it belongs to\n    \"\"\"\n\n    X = X.copy()\n    X[:] = self.clusters.cluster_centers_[self.clusters.predict(X)]\n    return X\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.OneDimensionalKMeans","title":"<code>OneDimensionalKMeans</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Clusterize each feature into a number of clusters</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>Number of clusters, by default 5</p> <code>5</code> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>class OneDimensionalKMeans(TransformerStep):\n    \"\"\"\n    Clusterize each feature into a number of clusters\n\n    Parameters:\n        n_clusters: Number of clusters, by default 5\n\n    \"\"\"\n\n    def __init__(self, n_clusters: int = 5, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.clusters = {}\n        self.n_clusters = n_clusters\n\n    def partial_fit(self, X: pd.DataFrame):\n        \"\"\"\n        Fit the model to the input data to obtain the clusters\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        if len(self.clusters) == 0:\n            for c in X.columns:\n                self.clusters[c] = MiniBatchKMeans(\n                    n_clusters=self.n_clusters, n_init=\"auto\"\n                )\n\n        for c in X.columns:\n            self.clusters[c].partial_fit(np.atleast_2d(X[c]).T)\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input dataframe\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame with the same index as the input. Each feature is replaced with the clusters of each point\n        \"\"\"\n        X = X.copy()\n        for c in X.columns:\n            X[c] = self.clusters[c].cluster_centers_[\n                self.clusters[c].predict(np.atleast_2d(X[c]).T)\n            ]\n        return X\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.OneDimensionalKMeans.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Fit the model to the input data to obtain the clusters</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame):\n    \"\"\"\n    Fit the model to the input data to obtain the clusters\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    if len(self.clusters) == 0:\n        for c in X.columns:\n            self.clusters[c] = MiniBatchKMeans(\n                n_clusters=self.n_clusters, n_init=\"auto\"\n            )\n\n    for c in X.columns:\n        self.clusters[c].partial_fit(np.atleast_2d(X[c]).T)\n    return self\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.OneDimensionalKMeans.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the input dataframe</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input. Each feature is replaced with the clusters of each point</p> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input dataframe\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame with the same index as the input. Each feature is replaced with the clusters of each point\n    \"\"\"\n    X = X.copy()\n    for c in X.columns:\n        X[c] = self.clusters[c].cluster_centers_[\n            self.clusters[c].predict(np.atleast_2d(X[c]).T)\n        ]\n    return X\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.SavitzkyGolayTransformer","title":"<code>SavitzkyGolayTransformer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Filter each feature using LOESS</p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>window size of the filter</p> required <code>order</code> <code>int</code> <p>Order of the filter, by default 2</p> <code>2</code> <code>name</code> <code>Optional[str]</code> <p>Step name</p> <code>None</code> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>class SavitzkyGolayTransformer(TransformerStep):\n    \"\"\"Filter each feature using LOESS\n\n    Parameters:\n        window: window size of the filter\n        order:  Order of the filter, by default 2\n        name: Step name\n    \"\"\"\n\n    def __init__(self, window: int, order: int = 2, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.window = window\n        self.order = order\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Return a new dataframe with the features filtered\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DatafFrame with the same index as the input with the features filtered\n        \"\"\"\n        if X.shape[0] &gt; self.window:\n            return pd.DataFrame(\n                savgol_filter(X, self.window, self.order, axis=0),\n                columns=X.columns,\n                index=X.index,\n            )\n        else:\n            return X\n</code></pre>"},{"location":"transformation/features/denoising/#ceruleo.transformation.features.denoising.SavitzkyGolayTransformer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Return a new dataframe with the features filtered</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DatafFrame with the same index as the input with the features filtered</p> Source code in <code>ceruleo/transformation/features/denoising.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new dataframe with the features filtered\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DatafFrame with the same index as the input with the features filtered\n    \"\"\"\n    if X.shape[0] &gt; self.window:\n        return pd.DataFrame(\n            savgol_filter(X, self.window, self.order, axis=0),\n            columns=X.columns,\n            index=X.index,\n        )\n    else:\n        return X\n</code></pre>"},{"location":"transformation/features/entropy/","title":"Entropy","text":""},{"location":"transformation/features/entropy/#entropy","title":"Entropy","text":""},{"location":"transformation/features/entropy/#ceruleo.transformation.features.entropy.LocalEntropyMeasures","title":"<code>LocalEntropyMeasures</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute diverse entropy measures</p> <p>For each feature present in the life a number of feature will be computed for each time stamp</p> <p>The possible features are:</p> <ul> <li>Local Active Information</li> <li>Local Block Entropy</li> <li>Local Entropy Rate</li> </ul> <p>Parameters:</p> Name Type Description Default <code>min_points</code> <p>The minimun number of points of the expanding window, by default 2</p> required <code>to_compute</code> <code>List[str]</code> <p>List of the features to compute, by default None. Valid values are: 'local_active_information', 'local_block_entropy', 'local_entropy_rate'</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/entropy.py</code> <pre><code>class LocalEntropyMeasures(TransformerStep):\n    \"\"\"Compute diverse entropy measures\n\n    For each feature present in the life a number of feature will be computed for each time stamp\n\n    The possible features are:\n\n    - Local Active Information\n    - Local Block Entropy\n    - Local Entropy Rate\n\n\n    Parameters:\n        min_points: The minimun number of points of the expanding window, by default 2\n        to_compute: List of the features to compute, by default None. Valid values are: 'local_active_information', 'local_block_entropy', 'local_entropy_rate'\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(\n        self, window: int = 2, to_compute: List[str] = None, name: Optional[str] = None\n    ):\n        super().__init__(name=name)\n        self.window = window\n\n        if to_compute is None:\n            self.to_compute = copy(ENTROPY_MEASURES_VALID_STATS)\n        else:\n            for f in to_compute:\n                if f not in ENTROPY_MEASURES_VALID_STATS:\n                    raise ValueError(\n                        f\"Invalid feature to compute {f}. Valids are {ENTROPY_MEASURES_VALID_STATS}\"\n                    )\n            self.to_compute = to_compute\n\n    def partial_fit(self, X, y=None):\n        return self\n\n    def fit(self, X, y=None):\n        return self\n\n\n    def _local_active_information(self, s: pd.Series):\n        return active_info(s.values, self.window, local=True)\n\n    def _local_block_entropy(self, s: pd.Series):\n        return block_entropy(s.values, self.window, local=True)\n\n    def _local_entropy_rate(self, s: pd.Series):\n        return entropy_rate(s.values, self.window, local=True)\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n\n        \"\"\" \n        Return a new dataframe with Entropy features computed for each feature in the input\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame with the at maximum three times the number of columns as the input since three Entropy features are computed for each feature in the input\n        \"\"\"\n\n        X_new_n_columns = len(X.columns) * len(self.to_compute)\n        i = 0\n\n        columns = np.empty((X_new_n_columns,), dtype=object)\n        for c in X.columns:\n            for stats in self.to_compute:\n                columns[i] = f\"{c}_{stats}\"\n                i += 1\n\n\n\n\n        data = np.empty((len(X.index), len(columns),))\n        data[:] = np.nan\n        X_new = pd.DataFrame(data, index=X.index, columns=columns)\n\n        for c in X.columns:\n            for stats in self.to_compute:\n                data = np.squeeze(getattr(self, f\"_{stats}\")(X[c]))\n                X_new.loc[:, f\"{c}_{stats}\"].iloc[-len(data):] = data\n        X_new[np.isinf(X_new) | np.isnan(X_new)] = np.nan\n        return X_new\n</code></pre>"},{"location":"transformation/features/entropy/#ceruleo.transformation.features.entropy.LocalEntropyMeasures.transform","title":"<code>transform(X)</code>","text":"<p>Return a new dataframe with Entropy features computed for each feature in the input</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the at maximum three times the number of columns as the input since three Entropy features are computed for each feature in the input</p> Source code in <code>ceruleo/transformation/features/entropy.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n\n    \"\"\" \n    Return a new dataframe with Entropy features computed for each feature in the input\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame with the at maximum three times the number of columns as the input since three Entropy features are computed for each feature in the input\n    \"\"\"\n\n    X_new_n_columns = len(X.columns) * len(self.to_compute)\n    i = 0\n\n    columns = np.empty((X_new_n_columns,), dtype=object)\n    for c in X.columns:\n        for stats in self.to_compute:\n            columns[i] = f\"{c}_{stats}\"\n            i += 1\n\n\n\n\n    data = np.empty((len(X.index), len(columns),))\n    data[:] = np.nan\n    X_new = pd.DataFrame(data, index=X.index, columns=columns)\n\n    for c in X.columns:\n        for stats in self.to_compute:\n            data = np.squeeze(getattr(self, f\"_{stats}\")(X[c]))\n            X_new.loc[:, f\"{c}_{stats}\"].iloc[-len(data):] = data\n    X_new[np.isinf(X_new) | np.isnan(X_new)] = np.nan\n    return X_new\n</code></pre>"},{"location":"transformation/features/extraction/","title":"Extraction","text":""},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.ChangesDetector","title":"<code>ChangesDetector</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute how many changes there are in a categorical variable</p> <p>['a', 'a', 'b', 'c] -&gt; [0, 0, 1, 1]</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class ChangesDetector(TransformerStep):\n    \"\"\"Compute how many changes there are in a categorical variable\n\n\n    ['a', 'a', 'b', 'c] -&gt; [0, 0, 1, 1]\n    \"\"\"\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Appply the transformation to the input life\n\n        Parameters:\n            X: The input life \n\n        Returns:\n            A DataFrame with boolean values representing weather changes were applied to the input variable or not\n        \"\"\"\n        return X != X.shift(axis=0)\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.ChangesDetector.transform","title":"<code>transform(X)</code>","text":"<p>Appply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with boolean values representing weather changes were applied to the input variable or not</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Appply the transformation to the input life\n\n    Parameters:\n        X: The input life \n\n    Returns:\n        A DataFrame with boolean values representing weather changes were applied to the input variable or not\n    \"\"\"\n    return X != X.shift(axis=0)\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.ColumnWiseSum","title":"<code>ColumnWiseSum</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the column-wise sum each column</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Name of the unique column which is returned</p> required Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class ColumnWiseSum(TransformerStep):\n    \"\"\"\n    Compute the column-wise sum each column\n\n    Parameters:\n        column_name: Name of the unique column which is returned \n    \"\"\"\n\n    def __init__(self, column_name: str, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.column_name = column_name\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life \n\n        Parameters:\n            X: The input life \n\n        Returns:\n            A single-column DataFrame containing the column-wise sum for each input sample\n        \"\"\"\n        return pd.DataFrame(X.sum(axis=1), columns=[self.column_name])\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.ColumnWiseSum.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to the input life </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A single-column DataFrame containing the column-wise sum for each input sample</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life \n\n    Parameters:\n        X: The input life \n\n    Returns:\n        A single-column DataFrame containing the column-wise sum for each input sample\n    \"\"\"\n    return pd.DataFrame(X.sum(axis=1), columns=[self.column_name])\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.Difference","title":"<code>Difference</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the difference between two set of features</p> <p>Example:</p> <pre><code>X[features1] - X[features2]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>feature_set1</code> <code>List[str]</code> <p>Feature list of the first group to substract</p> required <code>feature_set2</code> <code>List[str]</code> <p>Feature list of the second group to substract</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class Difference(TransformerStep):\n    \"\"\"Compute the difference between two set of features\n\n    Example:\n\n        X[features1] - X[features2]\n\n    Parameters:\n        feature_set1: Feature list of the first group to substract\n        feature_set2:Feature list of the second group to substract\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(\n        self, *, feature_set1: List[str], feature_set2: List[str], name: Optional[str] = None\n    ):\n        super().__init__(name=name)\n        if len(feature_set1) != len(feature_set2):\n            raise ValueError(\n                \"Feature set 1 and feature set 2 must have the same length\"\n            )\n        self.feature_set1 = feature_set1\n        self.feature_set2 = feature_set2\n        self.feature_names_computed = False\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life \n\n        Parameters:\n            X: The input life\n\n        Returns: \n            A DataFrame with two columns containing the result of the differences between the two sets of input features \n        \"\"\"\n        if not self.feature_names_computed:\n            self.feature_set1 = [self.find_feature(X, c) for c in self.feature_set1]\n            self.feature_set2 = [self.find_feature(X, c) for c in self.feature_set2]\n            feature_names_computed = True\n        new_X = X[self.feature_set1].copy()\n        new_X = new_X - X[self.feature_set2].values\n        return new_X\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.Difference.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to the input life </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with two columns containing the result of the differences between the two sets of input features</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life \n\n    Parameters:\n        X: The input life\n\n    Returns: \n        A DataFrame with two columns containing the result of the differences between the two sets of input features \n    \"\"\"\n    if not self.feature_names_computed:\n        self.feature_set1 = [self.find_feature(X, c) for c in self.feature_set1]\n        self.feature_set2 = [self.find_feature(X, c) for c in self.feature_set2]\n        feature_names_computed = True\n    new_X = X[self.feature_set1].copy()\n    new_X = new_X - X[self.feature_set2].values\n    return new_X\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.EMD","title":"<code>EMD</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the empirical mode decomposition of each feature</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of modes to compute</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>'EMD'</code> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class EMD(TransformerStep):\n    \"\"\"Compute the empirical mode decomposition of each feature\n\n    Parameters:\n        n: Number of modes to compute\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(self, *, n: int, name: Optional[str] = \"EMD\"):\n        super().__init__(name=name)\n        self.n = n\n\n    def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" Apply transformation to the input life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A DataFrame where the number of columns is n times the one of the input life, since each features is substituted by the n modes of its EMD\n\n        \"\"\"\n        new_X = pd.DataFrame(index=X.index)\n        for c in X.columns:\n            try:\n                imf = emd.sift.sift(X[c].values, max_imfs=self.n)\n                for j in range(self.n):\n                    if j &lt; imf.shape[1]:\n                        new_X[f\"{c}_{j}\"] = imf[:, j]\n                    else:\n                        new_X[f\"{c}_{j}\"] = np.nan\n            except Exception as e:\n                for j in range(self.n):\n                    new_X[f\"{c}_{j}\"] = np.nan\n\n        return new_X\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.EMD.transform","title":"<code>transform(X)</code>","text":"<p>Apply transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where the number of columns is n times the one of the input life, since each features is substituted by the n modes of its EMD</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" Apply transformation to the input life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A DataFrame where the number of columns is n times the one of the input life, since each features is substituted by the n modes of its EMD\n\n    \"\"\"\n    new_X = pd.DataFrame(index=X.index)\n    for c in X.columns:\n        try:\n            imf = emd.sift.sift(X[c].values, max_imfs=self.n)\n            for j in range(self.n):\n                if j &lt; imf.shape[1]:\n                    new_X[f\"{c}_{j}\"] = imf[:, j]\n                else:\n                    new_X[f\"{c}_{j}\"] = np.nan\n        except Exception as e:\n            for j in range(self.n):\n                new_X[f\"{c}_{j}\"] = np.nan\n\n    return new_X\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.EMDFilter","title":"<code>EMDFilter</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Filter the signals using Empirical Mode decomposition</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of modes</p> required <code>min_imf</code> <code>int</code> <p>Min Intrinsic Mode Function</p> required <code>max_imf</code> <code>int</code> <p>Max Intrinsic Mode Function</p> required Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class EMDFilter(TransformerStep):\n    \"\"\"\n    Filter the signals using Empirical Mode decomposition\n\n    Parameters:\n        n: Number of modes\n        min_imf: Min Intrinsic Mode Function\n        max_imf: Max Intrinsic Mode Function\n    \"\"\"\n\n    def __init__(\n        self, *, n: int, min_imf: int, max_imf: int, name: Optional[str] = \"EMD\"\n    ):\n        super().__init__(name=name)\n        self.n = n\n        self.min_imf = min_imf\n        self.max_imf = max_imf\n\n    def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life\n\n        Parameters:\n            X: The input life \n\n        Returns: \n            A DataFrame with the same shape of the input life and with the result of the EMD Filter application\n        \"\"\"\n        new_X = pd.DataFrame(index=X.index)\n\n        for c in X.columns:\n            try:\n                imf = emd.sift.sift(X[c].values, max_imfs=self.n)\n                new_X[c] = np.sum(imf[:, self.min_imf : self.max_imf], axis=1)\n            except Exception as e:\n                new_X[c] = X[c]\n\n        return new_X\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.EMDFilter.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life </p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the same shape of the input life and with the result of the EMD Filter application</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life\n\n    Parameters:\n        X: The input life \n\n    Returns: \n        A DataFrame with the same shape of the input life and with the result of the EMD Filter application\n    \"\"\"\n    new_X = pd.DataFrame(index=X.index)\n\n    for c in X.columns:\n        try:\n            imf = emd.sift.sift(X[c].values, max_imfs=self.n)\n            new_X[c] = np.sum(imf[:, self.min_imf : self.max_imf], axis=1)\n        except Exception as e:\n            new_X[c] = X[c]\n\n    return new_X\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.ExpandingStatistics","title":"<code>ExpandingStatistics</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute diverse number of features using an expandign window</p> <p>For each feature present in the life a number of feature will be computed for each time stamp</p> <p>The possible features are:</p> <ul> <li>Kurtosis</li> <li>Skewness</li> <li>Max</li> <li>Min</li> <li>Std</li> <li>Peak</li> <li>Impulse</li> <li>Clearance</li> <li>RMS</li> <li>Shape</li> <li>Crest</li> <li>Hurst</li> </ul> <p>Parameters:</p> Name Type Description Default <code>min_points</code> <code>int</code> <p>The minimun number of points of the expanding window, by default 2</p> <code>2</code> <code>to_compute</code> <code>List[str]</code> <p>List of the features to compute, by default None. Valid values are: 'kurtosis', 'skewness', 'max', 'min', 'std', 'peak', 'impulse','clearance', 'rms', 'shape', 'crest', 'hurst'</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class ExpandingStatistics(TransformerStep):\n    \"\"\"Compute diverse number of features using an expandign window\n\n    For each feature present in the life a number of feature will be computed for each time stamp\n\n    The possible features are:\n\n    - Kurtosis\n    - Skewness\n    - Max\n    - Min\n    - Std\n    - Peak\n    - Impulse\n    - Clearance\n    - RMS\n    - Shape\n    - Crest\n    - Hurst\n\n\n    Parameters:\n        min_points: The minimun number of points of the expanding window, by default 2\n        to_compute: List of the features to compute, by default None. Valid values are: 'kurtosis', 'skewness', 'max', 'min', 'std', 'peak', 'impulse','clearance', 'rms', 'shape', 'crest', 'hurst'\n        name: Name of the step, by default None\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        min_points: int=2,\n        to_compute: List[str] = None,\n        specific: Optional[Dict[str, List[str]]] = None,\n        name: Optional[str] = None,\n    ):\n        super().__init__(name=name)\n        self.min_points = min_points\n        valid_stats = [\n            \"kurtosis\",\n            \"skewness\",\n            \"max\",\n            \"min\",\n            \"std\",\n            \"peak\",\n            \"impulse\",\n            \"clearance\",\n            \"rms\",\n            \"shape\",\n            \"crest\",\n            \"mean\",\n            \"deviance\",\n            \"std_atan\",\n            \"energy\",\n            \"std_acosh\",\n            \"std_asinh\",\n        ]\n        not_default = [\"energy\", \"deviance\"]\n        if to_compute is not None and specific is not None:\n            raise ValueError(\"Only one of to_compute or specific should be used\")\n        self.specific = specific\n        self.to_compute = to_compute\n        if to_compute is None:\n            if specific is None:\n                self.to_compute = list(set(valid_stats) - set(not_default))\n            else:\n                self.specific = specific\n        else:\n            for f in to_compute:\n                if f not in valid_stats:\n                    raise ValueError(\n                        f\"Invalid feature to compute {f}. Valids are {valid_stats}\"\n                    )\n            self.to_compute = to_compute\n\n    def partial_fit(self, X, y=None):\n        return self\n\n    def fit(self, X, y=None):\n        return self\n\n    def _std_asinh(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return x.apply(np.arcsinh).expanding(self.min_points).std(numeric_only=True)\n\n    def _std_acosh(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return x.apply(np.arccosh).expanding(self.min_points).std(numeric_only=True)\n\n    def _energy(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return x.pow(2).expanding(self.min_points).sum(numeric_only=True)\n\n    def _std_atan(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return x.apply(np.arctan).expanding(self.min_points).std(numeric_only=True)\n\n    def _kurtosis(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return s.kurt(numeric_only=True)\n\n    def _skewness(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return s.skew(numeric_only=True)\n\n    def _max(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return s.max(numeric_only=True)\n\n    def _min(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return s.min(numeric_only=True)\n\n    def _std(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return s.std(numeric_only=True)\n\n    def _peak(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return s.max(numeric_only=True) - s.min(numeric_only=True)\n\n    def _impulse(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return self._peak(x, s, s_abs, s_abs_sqrt, s_sq) / s_abs.mean()\n\n    def _deviance(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return (x - s.mean(numeric_only=True)) / (s.std(numeric_only=True) + 0.00000000001)\n\n    def _clearance(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return self._peak(x, s, s_abs, s_abs_sqrt, s_sq) / s_abs_sqrt.mean().pow(2)\n\n    # def _hurst(\n    #    self,\n    #    x: pd.Series,\n    #    s: Expanding,\n    #    s_abs: Expanding,\n    #    s_abs_sqrt: Expanding,\n    #    s_sq: Expanding,\n    # ):\n    #    return s.apply(lambda s: hurst_exponent(s, method=\"RS\"))\n\n    def _rms(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return s_sq.mean(numeric_only=True).pow(1 / 2.0)\n\n    def _mean(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return s.mean(numeric_only=True)\n\n    def _shape(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return self._rms(x, s, s_abs, s_abs_sqrt, s_sq) / s_abs.mean(numeric_only=True)\n\n    def _crest(\n        self,\n        x: pd.Series,\n        s: Expanding,\n        s_abs: Expanding,\n        s_abs_sqrt: Expanding,\n        s_sq: Expanding,\n    ):\n        return self._peak(x, s, s_abs, s_abs_sqrt, s_sq) / self._rms(\n            x, s, s_abs, s_abs_sqrt, s_sq\n        )\n\n    def _compute_column_names(self, X: pd.DataFrame):\n        columns = []\n        if self.to_compute is not None:\n            for stats in self.to_compute:\n                for c in X.columns:\n                    columns.append(f\"{c}_{stats}\")\n        else:\n            for c in self.specific.keys():\n                for stats in self.specific[c]:\n                    columns.append(f\"{c}_{stats}\")\n        return columns\n\n    def _transform_all_features(\n        self, X: pd.DataFrame, X_new: pd.DataFrame, expanding, s_abs, s_abs_sqrt, s_sq\n    ):\n        for stats in self.to_compute:\n            columns_to_assign = [f\"{c}_{stats}\" for c in X.columns]\n            out = getattr(self, f\"_{stats}\")(X, expanding, s_abs, s_abs_sqrt, s_sq)\n            X_new.loc[:, columns_to_assign] = out.values\n\n    def _transform_specific(\n        self, X: pd.DataFrame, X_new: pd.DataFrame, expanding, s_abs, s_abs_sqrt, s_sq\n    ):\n        for c in self.specific.keys():\n            for stats in self.specific[c]:\n                feature = f\"{c}_{stats}\"\n                out = getattr(self, f\"_{stats}\")(\n                    X[c], expanding[c], s_abs[c], s_abs_sqrt[c], s_sq[c]\n                )\n                X_new.loc[:, feature] = out.values\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute features from the given life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f,\n        \"\"\"\n\n        columns = self._compute_column_names(X)\n\n        X_new = pd.DataFrame(index=X.index, columns=columns)\n        expanding = X.expanding(self.min_points)\n        s_abs = X.abs().expanding(self.min_points)\n        s_abs_sqrt = X.abs().pow(1.0 / 2).expanding(self.min_points)\n        s_sq = X.pow(2).expanding(self.min_points)\n        if self.to_compute is not None:\n            self._transform_all_features(X, X_new, expanding, s_abs, s_abs_sqrt, s_sq)\n        else:\n            self._transform_specific(X, X_new, expanding, s_abs, s_abs_sqrt, s_sq)\n        return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.ExpandingStatistics.transform","title":"<code>transform(X)</code>","text":"<p>Compute features from the given life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f,</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute features from the given life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f,\n    \"\"\"\n\n    columns = self._compute_column_names(X)\n\n    X_new = pd.DataFrame(index=X.index, columns=columns)\n    expanding = X.expanding(self.min_points)\n    s_abs = X.abs().expanding(self.min_points)\n    s_abs_sqrt = X.abs().pow(1.0 / 2).expanding(self.min_points)\n    s_sq = X.pow(2).expanding(self.min_points)\n    if self.to_compute is not None:\n        self._transform_all_features(X, X_new, expanding, s_abs, s_abs_sqrt, s_sq)\n    else:\n        self._transform_specific(X, X_new, expanding, s_abs, s_abs_sqrt, s_sq)\n    return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.HashingEncodingCategorical","title":"<code>HashingEncodingCategorical</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute a simple numerical encoding for a given feature</p> <p>Parameters:</p> Name Type Description Default <code>nbins</code> <code>int</code> <p>Number of bins after the hash</p> required <code>feature</code> <code>Optional[str]</code> <p>Feature name from which compute the simple encoding</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Step name</p> <code>None</code> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class HashingEncodingCategorical(TransformerStep):\n    \"\"\"Compute a simple numerical encoding for a given feature\n\n    Parameters:\n        nbins: Number of bins after the hash\n        feature: Feature name from which compute the simple encoding\n        name: Step name\n    \"\"\"\n\n    def __init__(\n        self, *, nbins: int, feature: Optional[str] = None, name: Optional[str] = None\n    ):\n        super().__init__(name=name)\n        self.nbins = nbins\n        self.feature = feature\n        self.categories = set()\n        self.encoder = None\n\n    def transform(self, X: pd.DataFrame, y: Optional[type]=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Return a new DataFrame with the feature  encoded with integer numbers\n\n        Parameters;\n            X: The input life\n            y: [type], optional\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new dataframe with the same index as the input with 1 column containing the encoding of the input feature. \n        \"\"\"\n\n        def hash(x):\n            if isinstance(x, int):\n                x = x.to_bytes((x.bit_length() + 7) // 8, \"little\")\n            return (mmh3.hash(x) &amp; 0xFFFFFFFF) % self.nbins\n\n        if self.feature is None:\n            self.feature = X.columns[0]\n        X_new = pd.DataFrame(index=X.index)\n        X_new[\"encoding\"] = X[self.feature].map(hash)\n        return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.HashingEncodingCategorical.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Return a new DataFrame with the feature  encoded with integer numbers</p> <p>Parameters;     X: The input life     y: [type], optional</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new dataframe with the same index as the input with 1 column containing the encoding of the input feature.</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame, y: Optional[type]=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new DataFrame with the feature  encoded with integer numbers\n\n    Parameters;\n        X: The input life\n        y: [type], optional\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new dataframe with the same index as the input with 1 column containing the encoding of the input feature. \n    \"\"\"\n\n    def hash(x):\n        if isinstance(x, int):\n            x = x.to_bytes((x.bit_length() + 7) // 8, \"little\")\n        return (mmh3.hash(x) &amp; 0xFFFFFFFF) % self.nbins\n\n    if self.feature is None:\n        self.feature = X.columns[0]\n    X_new = pd.DataFrame(index=X.index)\n    X_new[\"encoding\"] = X[self.feature].map(hash)\n    return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.Interactions","title":"<code>Interactions</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute pairwise interactions between the features</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class Interactions(TransformerStep):\n    \"\"\"Compute pairwise interactions between the features\"\"\"\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to one life \n\n        Parameters:\n            X: The input life\n\n        Returns:\n            DataFrame containing the pairwise interaction values \n\n        \"\"\"\n        X_new = pd.DataFrame(index=X.index)\n        for c1, c2 in itertools.combinations(X.columns, 2):\n            X_new[f\"{c1}_{c2}\"] = X[c1] * X[c2]\n        return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.Interactions.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to one life </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the pairwise interaction values</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to one life \n\n    Parameters:\n        X: The input life\n\n    Returns:\n        DataFrame containing the pairwise interaction values \n\n    \"\"\"\n    X_new = pd.DataFrame(index=X.index)\n    for c1, c2 in itertools.combinations(X.columns, 2):\n        X_new[f\"{c1}_{c2}\"] = X[c1] * X[c2]\n    return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.LifeStatistics","title":"<code>LifeStatistics</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute diverse number of features for each life.</p> <p>Returns a 1 row with the statistics computed for every feature</p> <p>The possible features are:</p> <ul> <li>Kurtosis</li> <li>Skewness</li> <li>Max</li> <li>Min</li> <li>Std</li> <li>Peak</li> <li>Impulse</li> <li>Clearance</li> <li>RMS</li> <li>Shape</li> <li>Crest</li> <li>Hurst</li> </ul> <p>Parameters:</p> Name Type Description Default <code>to_compute</code> <code>Optional[List[str]]</code> <p>List of the features to compute, by default None. Valid values are:'kurtosis', 'skewness', 'max', 'min', 'std', 'peak', 'impulse','clearance', 'rms', 'shape', 'crest', 'hurst'</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class LifeStatistics(TransformerStep):\n    \"\"\"Compute diverse number of features for each life.\n\n    Returns a 1 row with the statistics computed for every feature\n\n\n    The possible features are:\n\n    - Kurtosis\n    - Skewness\n    - Max\n    - Min\n    - Std\n    - Peak\n    - Impulse\n    - Clearance\n    - RMS\n    - Shape\n    - Crest\n    - Hurst\n\n\n    Parameters:\n        to_compute: List of the features to compute, by default None. Valid values are:'kurtosis', 'skewness', 'max', 'min', 'std', 'peak', 'impulse','clearance', 'rms', 'shape', 'crest', 'hurst'\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(\n        self, *, to_compute: Optional[List[str]] = None, name: Optional[str] = None\n    ):\n        super().__init__(name=name)\n        valid_stats = [\n            \"kurtosis\",\n            \"skewness\",\n            \"max\",\n            \"min\",\n            \"std\",\n            \"peak\",\n            \"impulse\",\n            \"clearance\",\n            \"rms\",\n            \"shape\",\n            \"crest\",\n        ]\n        if to_compute is None:\n            self.to_compute = valid_stats\n        else:\n            for f in to_compute:\n                if f not in valid_stats:\n                    raise ValueError(\n                        f\"Invalid feature to compute {f}. Valids are {valid_stats}\"\n                    )\n            self.to_compute = to_compute\n\n    def partial_fit(self, X, y=None):\n        return self\n\n    def fit(self, X, y=None):\n        return self\n\n    def _kurtosis(self, s: pd.Series):\n        return s.kurt(skipna=True)\n\n    def _skewness(self, s: pd.Series):\n        return s.skew(skipna=True)\n\n    def _max(self, s: pd.Series):\n        return s.max(skipna=True)\n\n    def _min(self, s: pd.Series):\n        return s.min(skipna=True)\n\n    def _std(self, s: pd.Series):\n        return s.std(skipna=True)\n\n    def _peak(self, s: pd.Series):\n        return s.max(skipna=True) - s.min(skipna=True)\n\n    def _impulse(self, s: pd.Series):\n        m = s.abs().mean()\n        if m &gt; 0:\n            return self._peak(s) / m\n        else:\n            return 0\n\n    def _clearance(self, s: pd.Series):\n        m = s.abs().pow(1.0 / 2).mean()\n        if m &gt; 0:\n            return (self._peak(s) / m) ** 2\n        else:\n            return 0\n\n    def _rms(self, s: pd.Series):\n        return np.sqrt(s.pow(2).mean(skipna=True))\n\n    def _shape(self, s: pd.Series):\n        m = s.abs().mean(skipna=True)\n        if m &gt; 0:\n            return self._rms(s) / m\n        else:\n            return 0\n\n    def _crest(self, s: pd.Series):\n        m = self._rms(s)\n        if m &gt; 0:\n            return self._peak(s) / m\n        else:\n            return 0\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute features from the given life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f,\n        \"\"\"\n        X_new = pd.DataFrame(index=[0])\n        for c in X.columns:\n            for stats in self.to_compute:\n                X_new[f\"{c}_{stats}\"] = getattr(self, f\"_{stats}\")(X[c])\n        return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.LifeStatistics.transform","title":"<code>transform(X)</code>","text":"<p>Compute features from the given life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f,</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute features from the given life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f,\n    \"\"\"\n    X_new = pd.DataFrame(index=[0])\n    for c in X.columns:\n        for stats in self.to_compute:\n            X_new[f\"{c}_{stats}\"] = getattr(self, f\"_{stats}\")(X[c])\n    return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.OneHotCategorical","title":"<code>OneHotCategorical</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute a one-hot encoding for a given feature</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Optional[str]</code> <p>Feature name from which compute the one-hot encoding</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Step name, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class OneHotCategorical(TransformerStep):\n    \"\"\"Compute a one-hot encoding for a given feature\n\n    Parameters:\n        feature: Feature name from which compute the one-hot encoding\n        name: Step name, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        feature: Optional[str] = None,\n        categories: Optional[List[any]] = None,\n        name: Optional[str] = None,\n    ):\n        super().__init__(name=name)\n        self.feature = feature\n        self.categories = categories\n        self.fixed_categories = True\n        if self.categories is None:\n            self.categories = set()\n            self.fixed_categories = False\n        self.encoder = None\n\n    def partial_fit(self, X: pd.DataFrame, y=None):\n        if self.fixed_categories:\n            return self\n        if self.feature is None:\n            self.feature = X.columns[0]\n        self.categories.update(set(X[self.feature].unique()))\n        return self\n\n    def fit(self, X: pd.DataFrame, y=None):\n        if self.fixed_categories:\n            return self\n        if self.feature is None:\n            self.feature = X.columns[0]\n        self.categories.update(set(X[self.feature].unique()))\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply the transformation to the input life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A DataFrame with shape equal to (X.shape[0],n_unique(feature)) containin the One Hot Encoding for the input feature\n        \"\"\"\n        categories = sorted(list([c for c in self.categories if c is not None]))\n        d = pd.Categorical(X[self.feature], categories=categories)\n\n        df = pd.get_dummies(d)\n        df.index = X.index\n        return df\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.OneHotCategorical.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Apply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with shape equal to (X.shape[0],n_unique(feature)) containin the One Hot Encoding for the input feature</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply the transformation to the input life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A DataFrame with shape equal to (X.shape[0],n_unique(feature)) containin the One Hot Encoding for the input feature\n    \"\"\"\n    categories = sorted(list([c for c in self.categories if c is not None]))\n    d = pd.Categorical(X[self.feature], categories=categories)\n\n    df = pd.get_dummies(d)\n    df.index = X.index\n    return df\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.RollingStatistics","title":"<code>RollingStatistics</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute diverse number of features using an rolling window.</p> <p>For each feature present in the life a number of feature will be computed for each time stamp</p> <p>The possible features are:</p> <p>Time domain:</p> <ul> <li>Kurtosis</li> <li>Skewness</li> <li>Max</li> <li>Min</li> <li>Std</li> <li>Peak</li> <li>Impulse</li> <li>Clearance</li> <li>RMS</li> <li>Shape</li> <li>Crest</li> </ul> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>Size of the rolling window, by default 15</p> <code>15</code> <code>min_points</code> <p>The minimun number of points of the expanding window</p> <code>2</code> <code>to_compute</code> <code>Optional[List[str]]</code> <p>Name of features to compute. Possible values are: 'kurtosis', 'skewness', 'max', 'min', 'std', 'peak', 'impulse', 'clearance', 'rms', 'shape', 'crest'</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class RollingStatistics(TransformerStep):\n    \"\"\"Compute diverse number of features using an rolling window.\n\n    For each feature present in the life a number of feature will be computed for each time stamp\n\n    The possible features are:\n\n    Time domain:\n\n    - Kurtosis\n    - Skewness\n    - Max\n    - Min\n    - Std\n    - Peak\n    - Impulse\n    - Clearance\n    - RMS\n    - Shape\n    - Crest\n\n    Parameters:\n        window: Size of the rolling window, by default 15\n        min_points: The minimun number of points of the expanding window\n        to_compute: Name of features to compute. Possible values are: 'kurtosis', 'skewness', 'max', 'min', 'std', 'peak', 'impulse', 'clearance', 'rms', 'shape', 'crest'\n        name: Name of the step, by default None\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        window: int = 15,\n        min_points=2,\n        to_compute: Optional[List[str]] = None,\n        specific: Optional[Dict[str, List[str]]] = None,\n        name: Optional[str] = None,\n    ):\n        super().__init__(name=name)\n        self.window = window\n        self.min_points = min_points\n        valid_stats = [\n            \"mean\",\n            \"kurtosis\",\n            \"skewness\",\n            \"max\",\n            \"min\",\n            \"std\",\n            \"peak\",\n            \"impulse\",\n            \"clearance\",\n            \"rms\",\n            \"shape\",\n            \"crest\",\n            \"deviance\",\n            \"std_atan\",\n            \"std_acosh\",\n            \"std_asinh\",\n            \"energy\",\n        ]\n\n        if to_compute is not None and specific is not None:\n            raise ValueError(\"Only one of to_compute or specific should be used\")\n        self.specific = specific\n        self.to_compute = to_compute\n        if to_compute is None:\n            if specific is None:\n                self.to_compute = valid_stats\n            else:\n                self.specific = specific\n        else:\n            for f in to_compute:\n                if f not in valid_stats:\n                    raise ValueError(\n                        f\"Invalid feature to compute {f}. Valids are {valid_stats}\"\n                    )\n            self.to_compute = to_compute\n\n    def partial_fit(self, X, y=None):\n        return self\n\n    def fit(self, X, y=None):\n        return self\n\n    def _std_asinh(self, X, rolling, abs_rolling):\n        return (\n            X.apply(np.arcsinh).rolling(self.window, self.min_points).std(numeric_only=True)\n        )\n\n    def _std_acosh(self, X, rolling, abs_rolling):\n        return (\n            X.apply(np.arccosh).rolling(self.window, self.min_points).std(numeric_only=True)\n        )\n\n    def _energy(self, X, rolling, abs_rolling):\n        return X.pow(2).rolling(self.window, self.min_points).sum()\n\n    def _std_atan(self, X, rolling, abs_rolling):\n        return (\n            X.apply(np.arctan)\n            .rolling(self.window, self.min_points)\n            .std(numeric_only=True)\n        )\n\n    def _mean(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return rolling.mean(numeric_only=True)\n\n    def _kurtosis(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return rolling.kurt(numeric_only=True)\n\n    def _skewness(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return rolling.skew(numeric_only=True)\n\n    def _max(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return rolling.max(numeric_only=True)\n\n    def _min(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return rolling.min(numeric_only=True)\n\n    def _std(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return rolling.std(numeric_only=True)\n\n    def _peak(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return rolling.max(numeric_only=True) - rolling.min(numeric_only=True)\n\n    def _impulse(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return self._peak(X, rolling, abs_rolling) / abs_rolling.mean()\n\n    def _deviance(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return (X - rolling.mean()) / rolling.std()\n\n    def _clearance(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return self._peak(X, rolling, abs_rolling) / X.abs().pow(1.0 / 2).rolling(\n            self.window, self.min_points\n        ).mean().pow(2)\n\n    def _rms(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return (\n            X.pow(2)\n            .rolling(self.window, self.min_points)\n            .mean(numeric_only=True)\n            .pow(1 / 2.0)\n        )\n\n    def _shape(self, X, rolling: Rolling, abs_rolling: Rolling):\n        return self._rms(X, rolling, abs_rolling) / abs_rolling.mean(numeric_only=True)\n\n    def _crest(self, X, rolling, abs_rolling):\n        return self._peak(X, rolling, abs_rolling) / self._rms(X, rolling, abs_rolling)\n\n    def _compute_column_names(self, X: pd.DataFrame):\n        columns = []\n        if self.to_compute is not None:\n            for stats in self.to_compute:\n                for c in X.columns:\n                    columns.append(f\"{c}_{stats}\")\n        else:\n            for c in self.specific.keys():\n                for stats in self.specific[c]:\n                    columns.append(f\"{c}_{stats}\")\n        return columns\n\n    def _transform_all_features(\n        self, X: pd.DataFrame, X_new: pd.DataFrame, rolling, abs_rolling\n    ):\n        for stats in self.to_compute:\n            columns_to_assign = [f\"{c}_{stats}\" for c in X.columns]\n            out = getattr(self, f\"_{stats}\")(X, rolling, abs_rolling)\n            X_new.loc[:, columns_to_assign] = out.values\n\n    def _transform_specific(\n        self, X: pd.DataFrame, X_new: pd.DataFrame, rolling, abs_rolling\n    ):\n        for c in self.specific.keys():\n            for stats in self.specific[c]:\n                feature = f\"{c}_{stats}\"\n                out = getattr(self, f\"_{stats}\")(X[c], rolling[c], abs_rolling[c])\n                X_new.loc[:, feature] = out.values\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute features from the given life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f\n        \"\"\"\n        columns = self._compute_column_names(X)\n\n        X_new = pd.DataFrame(index=X.index, columns=columns)\n        rolling = X.rolling(self.window, self.min_points)\n        abs_rolling = X.abs().rolling(self.window, self.min_points)\n        if self.to_compute is not None:\n            self._transform_all_features(X, X_new, rolling, abs_rolling)\n        else:\n            self._transform_specific(X, X_new, rolling, abs_rolling)\n        return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.RollingStatistics.transform","title":"<code>transform(X)</code>","text":"<p>Compute features from the given life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute features from the given life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new DataFrame with one row and with n columns. Let m be the number of features of the life and f the len(to_compute), then n = m x f\n    \"\"\"\n    columns = self._compute_column_names(X)\n\n    X_new = pd.DataFrame(index=X.index, columns=columns)\n    rolling = X.rolling(self.window, self.min_points)\n    abs_rolling = X.abs().rolling(self.window, self.min_points)\n    if self.to_compute is not None:\n        self._transform_all_features(X, X_new, rolling, abs_rolling)\n    else:\n        self._transform_specific(X, X_new, rolling, abs_rolling)\n    return X_new\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SampleNumber","title":"<code>SampleNumber</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Return a column with increasing number</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class SampleNumber(TransformerStep):\n    \"\"\"Return a column with increasing number\"\"\"\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A DataFrame with increasing sample indexes. \n        \"\"\"\n        df = pd.DataFrame(index=X.index)\n        df[\"sample_number\"] = list(range(X.shape[0]))\n        return df\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SampleNumber.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with increasing sample indexes.</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A DataFrame with increasing sample indexes. \n    \"\"\"\n    df = pd.DataFrame(index=X.index)\n    df[\"sample_number\"] = list(range(X.shape[0]))\n    return df\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SimpleEncodingCategorical","title":"<code>SimpleEncodingCategorical</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute a simple numerical encoding for a given feature</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Optional[str]</code> <p>Feature name from which compute the simple encoding</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Step name, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class SimpleEncodingCategorical(TransformerStep):\n    \"\"\"Compute a simple numerical encoding for a given feature\n\n    Parameters:\n        feature: Feature name from which compute the simple encoding\n        name: Step name, by default None\n    \"\"\"\n\n    def __init__(self, *, feature: Optional[str] = None, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.feature = feature\n        self.categories = set()\n        self.encoder = None\n\n    def partial_fit(self, X: pd.DataFrame, y=None) -&gt; \"SimpleEncodingCategorical\":\n        \"\"\"Compute incrementally the set of possible categories\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            Instance of class SimpleEncodingCategorical\n        \"\"\"\n        if self.feature is None:\n            self.feature = X.columns[0]\n\n        self.categories.update(set(X[self.feature].unique()))\n\n        return self\n\n    def fit(self, X: pd.DataFrame, y=None) -&gt; \"SimpleEncodingCategorical\":\n        \"\"\"\n        Compute the set of possible categories\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            Instance of class SimpleEncodingCategorical\n        \"\"\"\n        if self.feature is None:\n            self.feature = X.columns[0]\n\n        self.categories.update(set(X[self.feature].unique()))\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"Return a new DataFrame with the feature  encoded with integer numbers\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new dataframe with the same index as the input with 1 column with the Simple Encoding of the input feature. \n        \"\"\"\n        categories = sorted(list([c for c in self.categories if c is not None]))\n        d = pd.Categorical(X[self.feature], categories=categories)\n        return pd.DataFrame({\"encoding\": d.codes}, index=X.index)\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SimpleEncodingCategorical.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the set of possible categories</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>SimpleEncodingCategorical</code> <p>Instance of class SimpleEncodingCategorical</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def fit(self, X: pd.DataFrame, y=None) -&gt; \"SimpleEncodingCategorical\":\n    \"\"\"\n    Compute the set of possible categories\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        Instance of class SimpleEncodingCategorical\n    \"\"\"\n    if self.feature is None:\n        self.feature = X.columns[0]\n\n    self.categories.update(set(X[self.feature].unique()))\n    return self\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SimpleEncodingCategorical.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Compute incrementally the set of possible categories</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>SimpleEncodingCategorical</code> <p>Instance of class SimpleEncodingCategorical</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame, y=None) -&gt; \"SimpleEncodingCategorical\":\n    \"\"\"Compute incrementally the set of possible categories\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        Instance of class SimpleEncodingCategorical\n    \"\"\"\n    if self.feature is None:\n        self.feature = X.columns[0]\n\n    self.categories.update(set(X[self.feature].unique()))\n\n    return self\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SimpleEncodingCategorical.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Return a new DataFrame with the feature  encoded with integer numbers</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new dataframe with the same index as the input with 1 column with the Simple Encoding of the input feature.</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"Return a new DataFrame with the feature  encoded with integer numbers\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new dataframe with the same index as the input with 1 column with the Simple Encoding of the input feature. \n    \"\"\"\n    categories = sorted(list([c for c in self.categories if c is not None]))\n    d = pd.Categorical(X[self.feature], categories=categories)\n    return pd.DataFrame({\"encoding\": d.codes}, index=X.index)\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SlidingNonOverlappingWaveletDecomposition","title":"<code>SlidingNonOverlappingWaveletDecomposition</code>","text":"<p>               Bases: <code>TransformerStep</code></p>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SlidingNonOverlappingWaveletDecomposition--todo-test","title":"TODO TEST","text":"<p>X = signal coeffs = pywt.wavedec(X, 'db1', level=level) A4 = wrcoef(X, 'a', coeffs, 'db1', level) D4 = wrcoef(X, 'd', coeffs, 'db1', level) D3 = wrcoef(X, 'd', coeffs, 'db1', 3) D2 = wrcoef(X, 'd', coeffs, 'db1', 2) D1 = wrcoef(X, 'd', coeffs, 'db1', 1) r = A4 + D4 + D3 + D2 + D1 assert(np.mean(r-X) &lt; 0.00000)</p>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.SlidingNonOverlappingWaveletDecomposition--parameters","title":"Parameters","text":"<p>TransformerStep : [type]     [description]</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class SlidingNonOverlappingWaveletDecomposition(TransformerStep):\n    \"\"\"\n\n    # TODO TEST\n    X = signal\n    coeffs = pywt.wavedec(X, 'db1', level=level)\n    A4 = wrcoef(X, 'a', coeffs, 'db1', level)\n    D4 = wrcoef(X, 'd', coeffs, 'db1', level)\n    D3 = wrcoef(X, 'd', coeffs, 'db1', 3)\n    D2 = wrcoef(X, 'd', coeffs, 'db1', 2)\n    D1 = wrcoef(X, 'd', coeffs, 'db1', 1)\n    r = A4 + D4 + D3 + D2 + D1\n    assert(np.mean(r-X) &lt; 0.00000)\n\n    Parameters\n    ----------\n    TransformerStep : [type]\n        [description]\n    \"\"\"\n\n    def __init__(\n        self, *, window_size: int, level: int, wavelet: str, keep: List[str], **kwargs\n    ):\n        super().__init__(*kwargs)\n        self.wavelet = wavelet\n        self.level = level\n        self.keep = keep\n        self.window_size = window_size\n        self.strides = window_size\n\n    def transform(self, X: pd.DataFrame):\n        def _wavelet(values: np.ndarray):\n            coeffs = pywt.wavedec(values, self.wavelet, level=self.level)\n            out = np.zeros((values.shape[0], len(self.keep)))\n            for i, s in enumerate(self.keep):\n                part, level = s\n                out[:, i] = wrcoef(\n                    values, part.lower(), coeffs, self.wavelet, int(level)\n                )\n            return out\n\n        column_list = []\n        for c in X.columns:\n            for name in self.keep:\n                column_list.append(f\"wavelet_{name}_{c}\")\n        out = pd.DataFrame(index=X.index, columns=column_list, dtype=np.float32)\n        for c in X.columns:\n            wv_computed = apply_rolling_data(\n                X[c].values, _wavelet, self.window_size, self.strides\n            )\n\n            out.loc[:, [f\"wavelet_{name}_{c}\" for name in self.keep]] = wv_computed\n        return out\n</code></pre>"},{"location":"transformation/features/extraction/#ceruleo.transformation.features.extraction.TimeToPreviousBinaryValue","title":"<code>TimeToPreviousBinaryValue</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Return a column with increasing number</p> Source code in <code>ceruleo/transformation/features/extraction.py</code> <pre><code>class TimeToPreviousBinaryValue(TransformerStep):\n    \"\"\"Return a column with increasing number\"\"\"\n\n    def time_to_previous_event(self, X: pd.DataFrame, c: str):\n        def min_idex(group):\n            if group.iloc[0, 0] == 0:\n                return np.nan\n            else:\n                return np.min(group.index)\n\n        X_c_cumsum = X[[c]].cumsum()\n        min_index = X_c_cumsum.groupby(c).apply(min_idex)\n        X_merged = X_c_cumsum.merge(\n            pd.DataFrame(min_index, columns=[\"start\"]), left_on=c, right_index=True\n        )\n        return X_merged.index - X_merged[\"start\"]\n\n    def transform(self, X: pd.DataFrame):\n        new_X = pd.DataFrame(index=X.index)\n        for c in X.columns:\n            new_X[f\"ttp_{c}\"] = self.time_to_previous_event(X, c)\n        return new_X\n</code></pre>"},{"location":"transformation/features/extraction_frequency/","title":"Extraction Frequency","text":""},{"location":"transformation/features/extraction_frequency/#extraction-frequency","title":"Extraction Frequency","text":""},{"location":"transformation/features/extraction_frequency/#ceruleo.transformation.features.extraction_frequency.compute_frequency_features","title":"<code>compute_frequency_features(x)</code>","text":"<p>Returns the spectral centroid (mean), variance, skew, and kurtosis of the absolute fourier transform spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>the time series on which the spectral features are calculated</p> required <p>Returns:</p> Type Description <code>array</code> <p>An array containig the spectral centroid, variance, skew, and kurtosis of the absolute fourier transform spectrum.</p> Source code in <code>ceruleo/transformation/features/extraction_frequency.py</code> <pre><code>@jit(debug=True, nopython=True, error_model='numpy')\ndef compute_frequency_features(x: np.array) -&gt; np.array:\n    \"\"\"\n    Returns the spectral centroid (mean), variance, skew, and kurtosis of the absolute fourier transform spectrum.\n\n    Parameters:\n        x: the time series on which the spectral features are calculated\n\n    Returns:\n        An array containig the spectral centroid, variance, skew, and kurtosis of the absolute fourier transform spectrum.\n    \"\"\"\n\n    with objmode(fft_abs='float64[:]'):\n        fft_abs = np.abs(fft.rfft(x))\n\n    ps = fft_abs**2\n\n    data = np.zeros(8, dtype=np.float32)\n    data[0] = get_centroid(fft_abs)\n    data[1] = get_variance(fft_abs)\n    data[2] = get_skew(fft_abs)\n    data[3] = get_kurtosis(fft_abs)\n    data[4] = get_centroid(ps)\n    data[5] = get_variance(ps)\n    data[6] = get_skew(ps)\n    data[7] = get_kurtosis(ps)\n    return data\n</code></pre>"},{"location":"transformation/features/extraction_frequency/#ceruleo.transformation.features.extraction_frequency.get_centroid","title":"<code>get_centroid(y)</code>","text":"<p>Compute the centroid of the input distribution (aka distribution mean, first moment)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array</code> <p>the discrete distribution from which one wants to calculate the centroid</p> required <p>Returns:</p> Type Description <code>float32</code> <p>The centroid of distribution y</p> Source code in <code>ceruleo/transformation/features/extraction_frequency.py</code> <pre><code>@jit(nopython=True, error_model='numpy')\ndef get_centroid(y: np.array) -&gt; np.float32:\n    \"\"\"\n    Compute the centroid of the input distribution (aka distribution mean, first moment)\n\n    Parameters:\n        y: the discrete distribution from which one wants to calculate the centroid\n\n    Returns:\n        The centroid of distribution y\n    \"\"\"\n    return get_moment(y, 1)\n</code></pre>"},{"location":"transformation/features/extraction_frequency/#ceruleo.transformation.features.extraction_frequency.get_kurtosis","title":"<code>get_kurtosis(y)</code>","text":"<p>Calculates the kurtosis as the fourth standardized moment. (Ref: https://en.wikipedia.org/wiki/Kurtosis#Pearson_moments)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array</code> <p>the discrete distribution from which one wants to calculate the kurtosis</p> required <p>Returns:</p> Type Description <code>float32</code> <p>The kurtosis of distribution y</p> Source code in <code>ceruleo/transformation/features/extraction_frequency.py</code> <pre><code>@jit(nopython=True, error_model='numpy')\ndef get_kurtosis(y: np.array) -&gt; np.float32:\n    \"\"\"\n    Calculates the kurtosis as the fourth standardized moment. (Ref: https://en.wikipedia.org/wiki/Kurtosis#Pearson_moments)\n\n    Parameters:\n        y: the discrete distribution from which one wants to calculate the kurtosis\n\n    Returns:\n        The kurtosis of distribution y\n    \"\"\"\n\n    variance = get_variance(y)\n    # In the limit of a dirac delta, kurtosis should be 3 and variance 0.  However, in the discrete limit,\n    # the kurtosis blows up as variance --&gt; 0, hence return nan when variance is smaller than a resolution of 0.5:\n    if variance &lt; 0.5:\n        return np.nan\n    else:\n        return (\n            get_moment(y, 4) - 4 * get_centroid(y) * get_moment(y, 3)\n            + 6 * get_moment(y, 2) * get_centroid(y)**2 -\n            3 * get_centroid(y)\n        ) / get_variance(y)**2\n</code></pre>"},{"location":"transformation/features/extraction_frequency/#ceruleo.transformation.features.extraction_frequency.get_moment","title":"<code>get_moment(y, moment)</code>","text":"<p>Compute the (non centered) moment of the input distribution </p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array</code> <p>the discrete distribution from which one wants to calculate the moment</p> required <code>moment</code> <code>int</code> <p>the moment one wants to calculate (choose 1,2,3, ... )</p> required <p>Returns:</p> Type Description <code>float32</code> <p>The moment requested</p> Source code in <code>ceruleo/transformation/features/extraction_frequency.py</code> <pre><code>@jit(nopython=True, error_model='numpy')\ndef get_moment(y: np.array, moment: int) -&gt; np.float32:\n    \"\"\"\n    Compute the (non centered) moment of the input distribution \n\n    Parameters:\n        y: the discrete distribution from which one wants to calculate the moment\n        moment: the moment one wants to calculate (choose 1,2,3, ... )\n\n    Returns:\n        The moment requested\n    \"\"\"\n    return y.dot(np.arange(len(y), dtype=np.float64)**moment) / y.sum()\n</code></pre>"},{"location":"transformation/features/extraction_frequency/#ceruleo.transformation.features.extraction_frequency.get_skew","title":"<code>get_skew(y)</code>","text":"<p>Calculates the skew as the third standardized moment. (Ref: https://en.wikipedia.org/wiki/Skewness#Definition)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array</code> <p>the discrete distribution from which one wants to calculate the skew</p> required <p>Returns:</p> Type Description <code>float32</code> <p>The skew of distribution y</p> Source code in <code>ceruleo/transformation/features/extraction_frequency.py</code> <pre><code>@jit(nopython=True, error_model='numpy')\ndef get_skew(y: np.array) -&gt; np.float32:\n    \"\"\"\n    Calculates the skew as the third standardized moment. (Ref: https://en.wikipedia.org/wiki/Skewness#Definition)\n\n    Parameters:\n        y: the discrete distribution from which one wants to calculate the skew\n\n    Returns:\n        The skew of distribution y\n    \"\"\"\n\n    variance = get_variance(y)\n    # In the limit of a dirac delta, skew should be 0 and variance 0.  However, in the discrete limit,\n    # the skew blows up as variance --&gt; 0, hence return nan when variance is smaller than a resolution of 0.5:\n    if variance &lt; 0.5:\n        return np.nan\n    else:\n        return (\n            get_moment(y, 3) - 3 * get_centroid(y) *\n            variance - get_centroid(y)**3\n        ) / get_variance(y)**(1.5)\n</code></pre>"},{"location":"transformation/features/extraction_frequency/#ceruleo.transformation.features.extraction_frequency.get_variance","title":"<code>get_variance(y)</code>","text":"<p>Compute the variance of the input distribution (aka distribution second central moment)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array</code> <p>the discrete distribution from which one wants to calculate the variance</p> required <p>Returns:</p> Type Description <code>float32</code> <p>The variance of distribution y</p> Source code in <code>ceruleo/transformation/features/extraction_frequency.py</code> <pre><code>@jit(nopython=True, error_model='numpy')\ndef get_variance(y: np.array) -&gt; np.float32:\n    \"\"\"\n    Compute the variance of the input distribution (aka distribution second central moment)\n\n    Parameters:\n        y: the discrete distribution from which one wants to calculate the variance\n\n    Returns:\n        The variance of distribution y\n    \"\"\"\n    # Here we are implementing the formula var(X) = E[X**2] (second moment) - E[X]**2 (first moment squared)\n    return get_moment(y, 2) - get_centroid(y) ** 2\n</code></pre>"},{"location":"transformation/features/hurst/","title":"Hurst Exponent Estimator","text":""},{"location":"transformation/features/hurst/#hurst-exponent-estimator","title":"Hurst Exponent Estimator","text":""},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_dma","title":"<code>hurst_dma(prices, min_chunksize=8, max_chunksize=200, num_chunksize=5)</code>","text":"<p>Estimate the Hurst exponent using the DMA method.</p> <p>Estimates the Hurst (H) exponent using the DMA method from the time series. The DMA method consists on calculate the moving average of size <code>series_len</code> and subtract it to the original series and calculating the standard deviation of that result. This repeats the process for several <code>series_len</code> values and adjusts data regression to obtain the H. <code>series_len</code> will take values between <code>min_chunksize</code> and <code>max_chunksize</code>, the step size from <code>min_chunksize</code> to <code>max_chunksize</code> can be controlled through the parameter <code>step_chunksize</code>.</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>array</code> <p>A time series to calculate hurst exponent, must have more elements than <code>min_chunksize</code> and <code>max_chunksize</code>.</p> required <code>min_chunksize</code> <code>int</code> <p>This parameter allow you control the minimum window size.</p> <code>8</code> <code>max_chunksize</code> <code>int</code> <p>This parameter allow you control the maximum window size.</p> <code>200</code> <code>num_chunksize</code> <code>int</code> <p>This parameter allow you control the size of the step from minimum to maximum window size. Bigger step means fewer calculations.</p> <code>5</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimation of hurst exponent.</p>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_dma--references","title":"References","text":"<pre><code>Alessio, E., Carbone, A., Castelli, G. et al. Eur. Phys. J. B (2002) 27:\n197. http://dx.doi.org/10.1140/epjb/e20020150\n</code></pre> Source code in <code>ceruleo/transformation/features/hurst.py</code> <pre><code>def hurst_dma(prices: np.array, min_chunksize: int=8, max_chunksize: int=200, num_chunksize: int=5) -&gt; float:\n    \"\"\"\n    Estimate the Hurst exponent using the DMA method.\n\n    Estimates the Hurst (H) exponent using the DMA method from the time series.\n    The DMA method consists on calculate the moving average of size `series_len`\n    and subtract it to the original series and calculating the standard\n    deviation of that result. This repeats the process for several `series_len`\n    values and adjusts data regression to obtain the H. `series_len` will take\n    values between `min_chunksize` and `max_chunksize`, the step size from\n    `min_chunksize` to `max_chunksize` can be controlled through the parameter\n    `step_chunksize`.\n\n    Parameters:\n        prices: A time series to calculate hurst exponent, must have more elements than `min_chunksize` and `max_chunksize`.\n        min_chunksize: This parameter allow you control the minimum window size.\n        max_chunksize: This parameter allow you control the maximum window size.\n        num_chunksize: This parameter allow you control the size of the step from minimum to maximum window size. Bigger step means fewer calculations.\n\n    Returns:\n        Estimation of hurst exponent.\n\n\n    References\n    ----------\n        Alessio, E., Carbone, A., Castelli, G. et al. Eur. Phys. J. B (2002) 27:\n        197. http://dx.doi.org/10.1140/epjb/e20020150\n\n    \"\"\"\n    max_chunksize += 1\n    N = len(prices)\n    n_list = np.arange(min_chunksize, max_chunksize, num_chunksize, dtype=np.int64)\n    dma_list = np.empty(len(n_list))\n    factor = 1 / (N - max_chunksize)\n    # sweeping n_list\n    for i, n in enumerate(n_list):\n        b = np.divide([n - 1] + (n - 1) * [-1], n)  # do the same as:  y - y_ma_n\n        noise = np.power(signal.lfilter(b, 1, prices)[max_chunksize:], 2)\n        dma_list[i] = np.sqrt(factor * np.sum(noise))\n\n    H, const = np.linalg.lstsq(\n        a=np.vstack([np.log10(n_list), np.ones(len(n_list))]).T,\n        b=np.log10(dma_list)\n    )[0]\n    return H\n</code></pre>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_dsod","title":"<code>hurst_dsod(x)</code>","text":"<p>Estimate Hurst exponent on data timeseries using the DSOD (Discrete Second Order Derivative).</p> <p>The estimation is based on the discrete second order derivative. Consists on get two different noise of the original series and calculate the standard deviation and calculate the slope of two point with that values. source: https://gist.github.com/wmvanvliet/d883c3fe1402c7ced6fc</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>Time series to estimate the Hurst exponent for.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Estimation of the Hurst exponent for the given time series.</p>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_dsod--references","title":"References","text":"<pre><code>Istas, J.; G. Lang (1994), \u201cQuadratic variations and estimation of the local\nH\u00f6lder index of data Gaussian process,\u201d Ann. Inst. Poincar\u00e9, 33, pp. 407\u2013436.\n</code></pre>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_dsod--notes","title":"Notes","text":"<pre><code>This hurst_ets is data literal traduction of wfbmesti.m of waveleet toolbox from matlab.\n</code></pre> Source code in <code>ceruleo/transformation/features/hurst.py</code> <pre><code>def hurst_dsod(x: np.array) -&gt; float:\n    \"\"\"\n    Estimate Hurst exponent on data timeseries using the DSOD (Discrete Second Order Derivative).\n\n    The estimation is based on the discrete second order derivative. Consists on\n    get two different noise of the original series and calculate the standard\n    deviation and calculate the slope of two point with that values.\n    source: https://gist.github.com/wmvanvliet/d883c3fe1402c7ced6fc\n\n    Parameters:\n        x: Time series to estimate the Hurst exponent for.\n\n\n    Returns:\n       Estimation of the Hurst exponent for the given time series.\n\n    References\n    ----------\n        Istas, J.; G. Lang (1994), \u201cQuadratic variations and estimation of the local\n        H\u00f6lder index of data Gaussian process,\u201d Ann. Inst. Poincar\u00e9, 33, pp. 407\u2013436.\n\n\n    Notes\n    ----------\n        This hurst_ets is data literal traduction of wfbmesti.m of waveleet toolbox from matlab.\n    \"\"\"\n    y = np.cumsum(np.diff(x, axis=0), axis=0)\n\n    # second order derivative\n    b1 = [1, -2, 1]\n    y1 = signal.lfilter(b1, 1, y, axis=0)\n    y1 = y1[len(b1) - 1:]  # first values contain filter artifacts\n\n    # wider second order derivative\n    b2 = [1,  0, -2, 0, 1]\n    y2 = signal.lfilter(b2, 1, y, axis=0)\n    y2 = y2[len(b2) - 1:]  # first values contain filter artifacts\n\n    s1 = np.mean(y1 ** 2, axis=0)\n    s2 = np.mean(y2 ** 2, axis=0)\n\n    return 0.5 * np.log2(s2 / s1)\n</code></pre>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_exponent","title":"<code>hurst_exponent(prices, min_chunksize=8, max_chunksize=200, num_chunksize=5, method='RS')</code>","text":"<p>Estimates Hurst Exponent.</p> <p>Estimate the hurst exponent following one of 3 methods. Each method</p> <p>Parameters:</p> Name Type Description Default <code>prices</code> <code>Option[ndarray, Series, DataFrame]</code> <p>A time series to estimate hurst exponent.</p> required <code>min_chunksize</code> <p>int, optional Minimum chunk  size of the original series. This parameter doesn't have any effect with DSOD method, by default 8</p> <code>8</code> <code>max_chunksize</code> <p>int, optional Maximum chunk size of the original series. This parameter doesn't have any effect with DSOD method. by default 200.</p> <code>200</code> <code>num_chunksize</code> <p>int, optional Step used to select next the chunk size which divide the original series. This parameter doesn't have any effect with DSOD method, by default 5.</p> <code>5</code> <code>method</code> <p>{'RS', 'DMA', 'DSOD', 'all'}. Valid values are: RS : rescaled range, DMA : deviation moving average, DSOD : discrete second order derivative.</p> <code>'RS'</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimation of hurst_exponent according to the method selected.</p>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_exponent--references","title":"References","text":"<pre><code>RS: Hurst, H. E. (1951). Long term storage capacity of reservoirs. ASCE\n    Transactions, 116(776), 770-808.\nDMA: Alessio, E., Carbone, A., Castelli, G. et al. Eur. Phys. J. B (2002)\n    27: 197. http://dx.doi.org/10.1140/epjb/e20020150\nDSOD: Istas, J.; G. Lang (1994), \u201cQuadratic variations and estimation of\n    the local H\u00f6lder index of data Gaussian process,\u201d Ann. Inst. Poincar\u00e9,\n    33, pp. 407\u2013436.\n</code></pre>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_exponent--notes","title":"Notes","text":"<pre><code>The hurst exponent is an estimation which is important because there is no\ndata closed equation for it instead we have some methods to estimate it with\nhigh variations among them.\n</code></pre>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_exponent--see-also","title":"See Also","text":"<pre><code>hurst_rs, hurst_dma, hurst_dsod\n</code></pre> Source code in <code>ceruleo/transformation/features/hurst.py</code> <pre><code>def hurst_exponent(prices: Option[np.ndarray, pd.Series, pd.DataFrame], min_chunksize: int =8, max_chunksize: int =200, num_chunksize: int =5,\n                   method: str ='RS') -&gt; float:\n    \"\"\"\n    Estimates Hurst Exponent.\n\n    Estimate the hurst exponent following one of 3 methods. Each method\n\n    Parameters:\n        prices: A time series to estimate hurst exponent.\n        min_chunksize : int, optional\n            Minimum chunk  size of the original series. This parameter doesn't have any effect with DSOD method, by default 8\n        max_chunksize : int, optional\n            Maximum chunk size of the original series. This parameter doesn't have any effect with DSOD method. by default 200.\n        num_chunksize : int, optional\n            Step used to select next the chunk size which divide the original series. This parameter doesn't have any effect with DSOD method, by default 5.\n        method : {'RS', 'DMA', 'DSOD', 'all'}. Valid values are: RS : rescaled range, DMA : deviation moving average, DSOD : discrete second order derivative.\n\n\n    Returns:\n        Estimation of hurst_exponent according to the method selected.\n\n\n    References\n    ----------\n        RS: Hurst, H. E. (1951). Long term storage capacity of reservoirs. ASCE\n            Transactions, 116(776), 770-808.\n        DMA: Alessio, E., Carbone, A., Castelli, G. et al. Eur. Phys. J. B (2002)\n            27: 197. http://dx.doi.org/10.1140/epjb/e20020150\n        DSOD: Istas, J.; G. Lang (1994), \u201cQuadratic variations and estimation of\n            the local H\u00f6lder index of data Gaussian process,\u201d Ann. Inst. Poincar\u00e9,\n            33, pp. 407\u2013436.\n\n    Notes\n    ----------\n        The hurst exponent is an estimation which is important because there is no\n        data closed equation for it instead we have some methods to estimate it with\n        high variations among them.\n\n    See Also\n    ----------\n        hurst_rs, hurst_dma, hurst_dsod\n    \"\"\"\n    if len(prices) == 0:\n        return np.nan\n    # extract array\n    arr = prices.__array__()\n    # choose data method\n    if method == 'RS':\n        if prices.ndim &gt; 1:\n            h = hurst_rs(np.diff(arr, axis=0).T, min_chunksize, max_chunksize,\n                         num_chunksize)\n        else:\n            h = hurst_rs(np.diff(arr), min_chunksize, max_chunksize,\n                         num_chunksize)\n    elif method == 'DMA':\n        h = hurst_dma(arr, min_chunksize, max_chunksize, num_chunksize)\n    elif method == 'DSOD':\n        h = hurst_dsod(arr)\n    elif method == 'all':\n        return [\n            hurst_exponent(arr, min_chunksize, max_chunksize, num_chunksize, 'RS'),\n            hurst_exponent(arr, min_chunksize, max_chunksize, num_chunksize, 'DMA'),\n            hurst_exponent(arr, min_chunksize, max_chunksize, num_chunksize, 'DSOD')\n        ]\n    else:\n        raise NotImplementedError('The method choose is not implemented.')\n\n    return h\n</code></pre>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_rs","title":"<code>hurst_rs(x, min_chunksize, max_chunksize, num_chunksize, out)</code>","text":"<p>Estimate the Hurst exponent using R/S method.</p> <p>Estimates the Hurst (H) exponent using the R/S method from the time series. The R/S method consists of dividing the series into pieces of equal size series_len and calculating the rescaled range. This repeats the process for several <code>series_len</code> values and adjusts data regression to obtain the H. <code>series_len</code> will take values between <code>min_chunksize</code> and <code>max_chunksize</code>, the step size from <code>min_chunksize</code> to <code>max_chunksize</code> can be controlled through the parameter <code>step_chunksize</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>A time series to calculate hurst exponent, must have more elements than <code>min_chunksize</code> and <code>max_chunksize</code>.</p> required <code>min_chunksize</code> <code>int</code> <p>This parameter allow you control the minimum window size.</p> required <code>max_chunksize</code> <code>int</code> <p>This parameter allow you control the maximum window size.</p> required <code>num_chunksize</code> <code>int</code> <p>This parameter allow you control the size of the step from minimum to maximum window size. Bigger step means fewer calculations.</p> required <code>out</code> <code>array</code> <p>One element array to store the output.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Estimation of Hurst exponent.</p>"},{"location":"transformation/features/hurst/#ceruleo.transformation.features.hurst.hurst_rs--references","title":"References","text":"<pre><code>Hurst, H. E. (1951). Long term storage capacity of reservoirs. ASCE\nTransactions, 116(776), 770-808.\nAlessio, E., Carbone, A., Castelli, G. et al. Eur. Phys. J. B (2002) 27:\n197. http://dx.doi.org/10.1140/epjb/e20020150\n</code></pre> Source code in <code>ceruleo/transformation/features/hurst.py</code> <pre><code>@guvectorize(\"float64[:], int64, int64, int64, float64[:]\", \"(m),(),(),()-&gt;()\",\n             cache=True, nopython=True)\ndef hurst_rs(x: np.array, min_chunksize: int, max_chunksize: int, num_chunksize: int, out: np.array) -&gt; float:\n    \"\"\"\n    Estimate the Hurst exponent using R/S method.\n\n    Estimates the Hurst (H) exponent using the R/S method from the time series.\n    The R/S method consists of dividing the series into pieces of equal size\n    series_len and calculating the rescaled range. This repeats the process\n    for several `series_len` values and adjusts data regression to obtain the H.\n    `series_len` will take values between `min_chunksize` and `max_chunksize`,\n    the step size from `min_chunksize` to `max_chunksize` can be controlled\n    through the parameter `step_chunksize`.\n\n    Parameters:\n        x: A time series to calculate hurst exponent, must have more elements than `min_chunksize` and `max_chunksize`.\n        min_chunksize: This parameter allow you control the minimum window size.\n        max_chunksize: This parameter allow you control the maximum window size.\n        num_chunksize: This parameter allow you control the size of the step from minimum to maximum window size. Bigger step means fewer calculations.\n        out: One element array to store the output.\n\n\n    Returns:\n        Estimation of Hurst exponent.\n\n    References\n    ----------\n        Hurst, H. E. (1951). Long term storage capacity of reservoirs. ASCE\n        Transactions, 116(776), 770-808.\n        Alessio, E., Carbone, A., Castelli, G. et al. Eur. Phys. J. B (2002) 27:\n        197. http://dx.doi.org/10.1140/epjb/e20020150\n    \"\"\"\n    N = len(x)\n    max_chunksize += 1\n    rs_tmp = np.empty(N, dtype=np.float64)\n    chunk_size_list = np.linspace(min_chunksize, max_chunksize, num_chunksize)\\\n                        .astype(np.int64)\n    rs_values_list = np.empty(num_chunksize, dtype=np.float64)\n\n    # 1. The series is divided into chunks of chunk_size_list size\n    for i in range(num_chunksize):\n        chunk_size = chunk_size_list[i]\n\n        # 2. it iterates on the indices of the first observation of each chunk\n        number_of_chunks = int(len(x) / chunk_size)\n\n        for idx in range(number_of_chunks):\n            # next means no overlapping\n            # convert index to index selection of each chunk\n            ini = idx * chunk_size\n            end = ini + chunk_size\n            chunk = x[ini:end]\n\n            # 2.1 Calculate the RS (chunk_size)\n            z = np.cumsum(chunk - np.mean(chunk))\n            rs_tmp[idx] = np.divide(\n                np.max(z) - np.min(z),  # range\n                np.nanstd(chunk)  # standar deviation\n            )\n\n        # 3. Average of RS(chunk_size)\n        rs_values_list[i] = np.nanmean(rs_tmp[:idx + 1])\n\n    # 4. calculate the Hurst exponent.\n    H, c = np.linalg.lstsq(\n        a=np.vstack((np.log(chunk_size_list), np.ones(num_chunksize))).T,\n        b=np.log(rs_values_list)\n    )[0]\n\n    out[0] = H\n</code></pre>"},{"location":"transformation/features/imputers/","title":"Imputers","text":""},{"location":"transformation/features/imputers/#imputers","title":"Imputers","text":""},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.ApplyRollingImputer","title":"<code>ApplyRollingImputer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Impute missing values using a function over a rolling window</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Window size of the rolling window</p> required <code>func</code> <code>callable</code> <p>The function to call in each window</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class ApplyRollingImputer(TransformerStep):\n    \"\"\"\n    Impute missing values using a function over a rolling window\n\n    Parameters:\n        window_size: Window size of the rolling window\n        func: The function to call in each window\n\n    \"\"\"\n\n    def __init__(self, *, window_size: int, func: callable, **kwargs):\n        super().__init__(**kwargs)\n        self.window_size = window_size\n        self.function = func\n        self.mean_value_list = []\n        self.sum = None\n\n    def partial_fit(self, X: pd.DataFrame):\n        \"\"\"\n        Compute incrementally the mean value to use as default value to impute\n\n        Parameters:\n            X: The input life\n        \"\"\"\n        if self.sum is None:\n            self.sum = X.sum(axis=0)\n            self.counts = X.shape[0]\n        else:\n            self.sum += X.sum(axis=0)\n            self.counts += X.shape[0]\n        self.default_value = (self.sum / self.counts).to_dict()\n        return self\n\n    def fit(self, X: pd.DataFrame):\n        \"\"\"\n        Compute a default value in case there are not valid values in the rolling window\n\n        Parameters:\n            X: The input life\n        \"\"\"\n        self.default_value = np.mean(X, axis=0)\n        self.default_value[~np.isfinite(self.default_value)] = 0\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new life with the same index as the input with the missing values replaced by the output of the function supplied\n        \"\"\"\n        X = X.copy()\n\n        row, features = np.where(~np.isfinite(X))\n        min_limit = np.maximum(row - self.window_size, 0)\n        max_limit = np.minimum(row + self.window_size, X.shape[0])\n        for r, min_r, max_r, f in zip(row, min_limit, max_limit, features):\n            X.iloc[r, f] = self.function(X.iloc[min_r:max_r, f].values)\n            if ~np.isfinite(X.iloc[r, f]):\n                X.iloc[r, f] = self.default_value[X.columns[f]]\n        return X\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.ApplyRollingImputer.fit","title":"<code>fit(X)</code>","text":"<p>Compute a default value in case there are not valid values in the rolling window</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def fit(self, X: pd.DataFrame):\n    \"\"\"\n    Compute a default value in case there are not valid values in the rolling window\n\n    Parameters:\n        X: The input life\n    \"\"\"\n    self.default_value = np.mean(X, axis=0)\n    self.default_value[~np.isfinite(self.default_value)] = 0\n    return self\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.ApplyRollingImputer.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Compute incrementally the mean value to use as default value to impute</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame):\n    \"\"\"\n    Compute incrementally the mean value to use as default value to impute\n\n    Parameters:\n        X: The input life\n    \"\"\"\n    if self.sum is None:\n        self.sum = X.sum(axis=0)\n        self.counts = X.shape[0]\n    else:\n        self.sum += X.sum(axis=0)\n        self.counts += X.shape[0]\n    self.default_value = (self.sum / self.counts).to_dict()\n    return self\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.ApplyRollingImputer.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new life with the same index as the input with the missing values replaced by the output of the function supplied</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new life with the same index as the input with the missing values replaced by the output of the function supplied\n    \"\"\"\n    X = X.copy()\n\n    row, features = np.where(~np.isfinite(X))\n    min_limit = np.maximum(row - self.window_size, 0)\n    max_limit = np.minimum(row + self.window_size, X.shape[0])\n    for r, min_r, max_r, f in zip(row, min_limit, max_limit, features):\n        X.iloc[r, f] = self.function(X.iloc[min_r:max_r, f].values)\n        if ~np.isfinite(X.iloc[r, f]):\n            X.iloc[r, f] = self.default_value[X.columns[f]]\n    return X\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.BackwardFillImputer","title":"<code>BackwardFillImputer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Impute forward filling the values</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class BackwardFillImputer(TransformerStep):\n    \"\"\"Impute forward filling the values\"\"\"\n\n    def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new life with the same index as the input with the missing values replaced by the value in the previous timestamp \n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input array must be a data frame\")\n        return X.bfill()\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.BackwardFillImputer.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new life with the same index as the input with the missing values replaced by the value in the previous timestamp</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new life with the same index as the input with the missing values replaced by the value in the previous timestamp \n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input array must be a data frame\")\n    return X.bfill()\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.FillImputer","title":"<code>FillImputer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class FillImputer(TransformerStep):\n    def __init__(self, *, value: float, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.value = value\n\n    \"\"\"Impute substituting the missing values with a value specified in the input \n    \"\"\"\n    def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new life with the same index as the input with the missing values replaced by the value specified in the input \n        \"\"\"\n        return X.fillna(value=self.value)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.FillImputer.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new life with the same index as the input with the missing values replaced by the value specified in the input</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new life with the same index as the input with the missing values replaced by the value specified in the input \n    \"\"\"\n    return X.fillna(value=self.value)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.ForwardFillImputer","title":"<code>ForwardFillImputer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Impute forward filling the values</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class ForwardFillImputer(TransformerStep):\n    \"\"\"Impute forward filling the values\"\"\"\n\n    def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new life with the same index as the input with the missing values replaced by the value in the succesive timestamp \n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input array must be a data frame\")\n        return X.ffill()\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.ForwardFillImputer.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new life with the same index as the input with the missing values replaced by the value in the succesive timestamp</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new life with the same index as the input with the missing values replaced by the value in the succesive timestamp \n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input array must be a data frame\")\n    return X.ffill()\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.MeanImputer","title":"<code>MeanImputer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Impute missing values with the mean value of the training set</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the step</p> <code>None</code> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class MeanImputer(TransformerStep):\n    \"\"\"Impute missing values with the mean value of the training set\n\n    Parameters:\n        name: The name of the step\n\n    \"\"\"\n\n    def __init__(self, *, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.sum = None\n\n    def partial_fit(self, X:pd.DataFrame, y=None):\n        \"\"\"Compute the mean value incrementally\n\n        Parameters:\n            X: The input life  \n        \"\"\"\n        if self.sum is None:\n            self.sum = X.sum(axis=0)\n            self.counts = X.shape[0]\n        else:\n            self.sum += X.sum(axis=0)\n            self.counts += X.shape[0]\n        self.mean = (self.sum / self.counts).to_dict()\n        return self\n\n    def fit(self, X:pd.DataFrame, y=None):\n        \"\"\"Compute the mean value \n\n        Parameters:\n            X: The input life  \n        \"\"\"\n        self.mean = X.mean(axis=0).to_dict()\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"Return a new dataframe with the missing values replaced by the fitted mean\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new DataFrame with the same index as the input with the Na values replaced by the fitted mean\n        \"\"\"\n        return X.fillna(value=self.mean)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.MeanImputer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the mean value </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def fit(self, X:pd.DataFrame, y=None):\n    \"\"\"Compute the mean value \n\n    Parameters:\n        X: The input life  \n    \"\"\"\n    self.mean = X.mean(axis=0).to_dict()\n    return self\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.MeanImputer.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Compute the mean value incrementally</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def partial_fit(self, X:pd.DataFrame, y=None):\n    \"\"\"Compute the mean value incrementally\n\n    Parameters:\n        X: The input life  \n    \"\"\"\n    if self.sum is None:\n        self.sum = X.sum(axis=0)\n        self.counts = X.shape[0]\n    else:\n        self.sum += X.sum(axis=0)\n        self.counts += X.shape[0]\n    self.mean = (self.sum / self.counts).to_dict()\n    return self\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.MeanImputer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Return a new dataframe with the missing values replaced by the fitted mean</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the Na values replaced by the fitted mean</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"Return a new dataframe with the missing values replaced by the fitted mean\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new DataFrame with the same index as the input with the Na values replaced by the fitted mean\n    \"\"\"\n    return X.fillna(value=self.mean)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.MedianImputer","title":"<code>MedianImputer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Impute missing values with the median value of the training set</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>The name of the step</p> <code>None</code> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class MedianImputer(TransformerStep):\n    \"\"\"\n    Impute missing values with the median value of the training set\n\n    Parameters:\n        name: The name of the step\n\n    \"\"\"\n\n    def __init__(self, *, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.tdigest_dict = None\n\n    def fit(self, X:pd.DataFrame, y=None):\n        \"\"\"Compute the median value\n\n        Parameters:\n            X: The input life\n        \"\"\"\n        self.median = X.median(axis=0).to_dict()\n        return self\n\n    def partial_fit(self, X:pd.DataFrame):\n        \"\"\"Compute the median value incrementally\n\n         Parameters:\n            X: The input life\n        \"\"\"\n        if self.tdigest_dict is None:\n            self.tdigest_dict = {c: TDigest() for c in X.columns}\n        for c in X.columns:\n            self.tdigest_dict[c].batch_update(X[c].values)\n\n        self.median = {\n            c: self.tdigest_dict[c].percentile(50) for c in self.tdigest_dict.keys()\n        }\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Return a new dataframe with the missing values replaced by the fitted median\n\n        Parameters:\n            X: The input life\n\n\n        Returns:\n            A new DataFrame with the same index as the input with the Na values replaced by the fitted median\n        \"\"\"\n        return X.fillna(value=self.median)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.MedianImputer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the median value</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def fit(self, X:pd.DataFrame, y=None):\n    \"\"\"Compute the median value\n\n    Parameters:\n        X: The input life\n    \"\"\"\n    self.median = X.median(axis=0).to_dict()\n    return self\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.MedianImputer.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Compute the median value incrementally</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def partial_fit(self, X:pd.DataFrame):\n    \"\"\"Compute the median value incrementally\n\n     Parameters:\n        X: The input life\n    \"\"\"\n    if self.tdigest_dict is None:\n        self.tdigest_dict = {c: TDigest() for c in X.columns}\n    for c in X.columns:\n        self.tdigest_dict[c].batch_update(X[c].values)\n\n    self.median = {\n        c: self.tdigest_dict[c].percentile(50) for c in self.tdigest_dict.keys()\n    }\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.MedianImputer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Return a new dataframe with the missing values replaced by the fitted median</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the Na values replaced by the fitted median</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new dataframe with the missing values replaced by the fitted median\n\n    Parameters:\n        X: The input life\n\n\n    Returns:\n        A new DataFrame with the same index as the input with the Na values replaced by the fitted median\n    \"\"\"\n    return X.fillna(value=self.median)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.NaNtoInf","title":"<code>NaNtoInf</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Replace NaN for inf</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class NaNtoInf(TransformerStep):\n    \"\"\"Replace NaN for inf\"\"\"\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life replacing Nan for inf\n\n        Parameters:\n            X: Input Dataframe to be transformed\n\n        Returns:\n            A dataframe with she same index as the input with the NaN values replaced with inf\n        \"\"\"\n        return X.replace([np.inf, -np.inf], np.nan)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.NaNtoInf.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the input life replacing Nan for inf</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input Dataframe to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with she same index as the input with the NaN values replaced with inf</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life replacing Nan for inf\n\n    Parameters:\n        X: Input Dataframe to be transformed\n\n    Returns:\n        A dataframe with she same index as the input with the NaN values replaced with inf\n    \"\"\"\n    return X.replace([np.inf, -np.inf], np.nan)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.PerColumnImputer","title":"<code>PerColumnImputer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Impute the values of each column following a simple rule</p> The imputing is made following this rule <ul> <li>-np.inf -&gt; min</li> <li>np.inf -&gt; max</li> <li>nan -&gt; median</li> </ul> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Step name, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class PerColumnImputer(TransformerStep):\n    \"\"\"Impute the values of each column following a simple rule\n\n    The imputing is made following this rule:\n        * -np.inf -&gt; min\n        * np.inf -&gt; max\n        * nan -&gt; median\n\n    Parameters:\n        name: Step name, by default None\n\n    \"\"\"\n\n    def __init__(self, *, name: Optional[str] = None):\n        super().__init__(name=name, prefer_partial_fit=False)\n        self.data_min = None\n        self.data_max = None\n        self.data_median = None\n\n    def partial_fit(self, X:pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\" \n        Fit the transformation incrementally\n\n        Parameters:\n            X: The input life\n\n        \"\"\"\n        X = X.replace([np.inf, -np.inf], np.nan)\n        col_to_max = X.max()\n        col_to_min = X.max()\n        col_to_median = X.median()\n        if self.data_min is None:\n            self.data_min = col_to_min\n            self.data_max = col_to_max\n            self.data_median = col_to_median\n        else:\n            self.data_min = pd.concat([self.data_min, col_to_min], axis=1).min(axis=1)\n            self.data_max = pd.concat([self.data_max, col_to_max], axis=1).max(axis=1)\n            self.data_median = pd.concat([self.data_max, col_to_median], axis=1).median(\n                axis=1\n            )\n        self._remove_na()\n\n    def _remove_na(self):\n        self.data_max.fillna(0, inplace=True)\n        self.data_min.fillna(0, inplace=True)\n        self.data_median.fillna(0, inplace=True)\n\n    def fit(self, X:pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\" \n        Fit the transformation \n\n        Parameters:\n            X: The input life\n\n        \"\"\"\n        X = X.replace([np.inf, -np.inf], np.nan)\n        col_to_max = X.max()\n        col_to_min = X.max()\n        col_to_median = X.median()\n\n        self.data_min = col_to_min\n        self.data_max = col_to_max\n        self.data_median = col_to_median\n\n        self._remove_na()\n        return self\n\n    def transform(self, X:pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life\n\n        Parameters:\n            X: The input life\n\n        \"\"\"\n        X_new = X.copy()\n        for c in X_new.columns:\n            X_new[c].replace([np.inf], self.data_max[c], inplace=True)\n            X_new[c].replace([-np.inf], self.data_min[c], inplace=True)\n            X_new[c].replace([np.nan], self.data_median[c], inplace=True)\n        return X_new\n\n    def description(self) -&gt; tuple:\n        \"\"\" \n        Transformation's Description\n\n        Returns:\n            A tuple with the transformation name and the Max, Min and Median values for each feature. \n\n        \"\"\"\n        name = super().description()\n        data = []\n        for k in self.data_max.index:\n            data.append(\n                (\n                    k,\n                    {\n                        \"Max\": self.data_max[k],\n                        \"Min\": self.data_max[k],\n                        \"Median\": self.data_max[k],\n                    },\n                )\n            )\n        return (name, data)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.PerColumnImputer.description","title":"<code>description()</code>","text":"<p>Transformation's Description</p> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple with the transformation name and the Max, Min and Median values for each feature.</p> Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def description(self) -&gt; tuple:\n    \"\"\" \n    Transformation's Description\n\n    Returns:\n        A tuple with the transformation name and the Max, Min and Median values for each feature. \n\n    \"\"\"\n    name = super().description()\n    data = []\n    for k in self.data_max.index:\n        data.append(\n            (\n                k,\n                {\n                    \"Max\": self.data_max[k],\n                    \"Min\": self.data_max[k],\n                    \"Median\": self.data_max[k],\n                },\n            )\n        )\n    return (name, data)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.PerColumnImputer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformation </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def fit(self, X:pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\" \n    Fit the transformation \n\n    Parameters:\n        X: The input life\n\n    \"\"\"\n    X = X.replace([np.inf, -np.inf], np.nan)\n    col_to_max = X.max()\n    col_to_min = X.max()\n    col_to_median = X.median()\n\n    self.data_min = col_to_min\n    self.data_max = col_to_max\n    self.data_median = col_to_median\n\n    self._remove_na()\n    return self\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.PerColumnImputer.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Fit the transformation incrementally</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def partial_fit(self, X:pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\" \n    Fit the transformation incrementally\n\n    Parameters:\n        X: The input life\n\n    \"\"\"\n    X = X.replace([np.inf, -np.inf], np.nan)\n    col_to_max = X.max()\n    col_to_min = X.max()\n    col_to_median = X.median()\n    if self.data_min is None:\n        self.data_min = col_to_min\n        self.data_max = col_to_max\n        self.data_median = col_to_median\n    else:\n        self.data_min = pd.concat([self.data_min, col_to_min], axis=1).min(axis=1)\n        self.data_max = pd.concat([self.data_max, col_to_max], axis=1).max(axis=1)\n        self.data_median = pd.concat([self.data_max, col_to_median], axis=1).median(\n            axis=1\n        )\n    self._remove_na()\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.PerColumnImputer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Apply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>def transform(self, X:pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life\n\n    Parameters:\n        X: The input life\n\n    \"\"\"\n    X_new = X.copy()\n    for c in X_new.columns:\n        X_new[c].replace([np.inf], self.data_max[c], inplace=True)\n        X_new[c].replace([-np.inf], self.data_min[c], inplace=True)\n        X_new[c].replace([np.nan], self.data_median[c], inplace=True)\n    return X_new\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.RollingMeanImputer","title":"<code>RollingMeanImputer</code>","text":"<p>               Bases: <code>ApplyRollingImputer</code></p> <p>Impute missing values with the mean value on a rolling window</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Window size of the rolling window</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class RollingMeanImputer(ApplyRollingImputer):\n    \"\"\"Impute missing values with the mean value on a rolling window\n\n    Parameters:\n        window_size: Window size of the rolling window\n\n    \"\"\"\n\n    def __init__(self, *, window_size: int, name:str=None):\n        super().__init__(window_size=window_size, func=np.mean, name=name)\n</code></pre>"},{"location":"transformation/features/imputers/#ceruleo.transformation.features.imputers.RollingMedianImputer","title":"<code>RollingMedianImputer</code>","text":"<p>               Bases: <code>ApplyRollingImputer</code></p> <p>Impute missing values with the median value on a rolling window</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Window size of the rolling window</p> required Source code in <code>ceruleo/transformation/features/imputers.py</code> <pre><code>class RollingMedianImputer(ApplyRollingImputer):\n    \"\"\"Impute missing values with the median value on a rolling window\n\n    Parameters:\n        window_size: Window size of the rolling window\n    \"\"\"\n\n    def __init__(self, *, window_size: int, name:str=None):\n        super().__init__(window_size, func=np.median, name=name)\n</code></pre>"},{"location":"transformation/features/operations/","title":"Operations","text":""},{"location":"transformation/features/operations/#operations","title":"Operations","text":""},{"location":"transformation/features/operations/#ceruleo.transformation.features.operations.Divide","title":"<code>Divide</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Divide multiple run-to-failure cycles vertically</p> Source code in <code>ceruleo/transformation/features/operations.py</code> <pre><code>class Divide(TransformerStep):\n    \"\"\"\n    Divide multiple run-to-failure cycles vertically\n    \"\"\"\n    def transform(self, X: List[pd.DataFrame]) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply the division\n\n        Parameters:\n            X: List of run-to-failure cycles to divide\n\n        Returns:\n            A dataframe with the divided run-to-failure cycles\n        \"\"\"\n        return reduce(lambda x, y: x.divide(y, fill_value=0), X)\n</code></pre>"},{"location":"transformation/features/operations/#ceruleo.transformation.features.operations.Divide.transform","title":"<code>transform(X)</code>","text":"<p>Apply the division</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>List[DataFrame]</code> <p>List of run-to-failure cycles to divide</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the divided run-to-failure cycles</p> Source code in <code>ceruleo/transformation/features/operations.py</code> <pre><code>def transform(self, X: List[pd.DataFrame]) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply the division\n\n    Parameters:\n        X: List of run-to-failure cycles to divide\n\n    Returns:\n        A dataframe with the divided run-to-failure cycles\n    \"\"\"\n    return reduce(lambda x, y: x.divide(y, fill_value=0), X)\n</code></pre>"},{"location":"transformation/features/operations/#ceruleo.transformation.features.operations.Sum","title":"<code>Sum</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Concatenate multiple run-to-failure cycles vertically</p> Source code in <code>ceruleo/transformation/features/operations.py</code> <pre><code>class Sum(TransformerStep):\n    \"\"\" \n    Concatenate multiple run-to-failure cycles vertically\n    \"\"\"\n    def transform(self, X: List[pd.DataFrame]) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply the concatenation \n\n        Parameters:\n            X: List of run-to-failure cycles to concatenate\n\n        Returns:\n            A dataframe with the concatenated run-to-failure cycles\n        \"\"\"\n        return reduce(lambda x, y: x.add(y, fill_value=0), X)\n</code></pre>"},{"location":"transformation/features/operations/#ceruleo.transformation.features.operations.Sum.transform","title":"<code>transform(X)</code>","text":"<p>Apply the concatenation </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>List[DataFrame]</code> <p>List of run-to-failure cycles to concatenate</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the concatenated run-to-failure cycles</p> Source code in <code>ceruleo/transformation/features/operations.py</code> <pre><code>def transform(self, X: List[pd.DataFrame]) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply the concatenation \n\n    Parameters:\n        X: List of run-to-failure cycles to concatenate\n\n    Returns:\n        A dataframe with the concatenated run-to-failure cycles\n    \"\"\"\n    return reduce(lambda x, y: x.add(y, fill_value=0), X)\n</code></pre>"},{"location":"transformation/features/outliers/","title":"Outliers","text":""},{"location":"transformation/features/outliers/#outliers","title":"Outliers","text":""},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.BeyondQuartileOutlierRemover","title":"<code>BeyondQuartileOutlierRemover</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Remove values outside (Q1, Q3)</p> <p>If clip is True the values will be clipped between the range, otherwise the values are going to be replaced by inf and -inf</p> <p>Parameters:</p> Name Type Description Default <code>lower_quantile</code> <code>float</code> <p>Lower quantile threshold for the non-anomalous values, by default 0.25</p> <code>0.25</code> <code>upper_quantile</code> <code>float</code> <p>Upper quantile threshold for the non-anomalous values, by default 0.75</p> <code>0.75</code> <code>clip</code> <code>bool</code> <p>Wether to clip the values outside the range, by default False</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>class BeyondQuartileOutlierRemover(TransformerStep):\n    \"\"\"\n    Remove values outside (Q1, Q3)\n\n    If clip is True the values will be clipped between the range,\n    otherwise the values are going to be replaced by inf and -inf\n\n    Parameters:\n        lower_quantile:  Lower quantile threshold for the non-anomalous values, by default 0.25\n        upper_quantile: Upper quantile threshold for the non-anomalous values, by default 0.75\n        clip: Wether to clip the values outside the range, by default False\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        lower_quantile: float = 0.25,\n        upper_quantile: float = 0.75,\n        subsample: float = 1.0,\n        clip: bool = False,\n        name: Optional[str] = None,\n        prefer_partial_fit:bool=False\n    ):\n\n        super().__init__(name=name, prefer_partial_fit=prefer_partial_fit)\n        self.tdigest_dict = None\n        self.lower_quantile = lower_quantile\n        self.upper_quantile = upper_quantile\n        self.clip = clip\n        self.subsample= subsample\n        self.Q1 = None\n        self.Q3 = None\n        self.quantile_estimator = None\n\n    def partial_fit(self, X: pd.DataFrame):\n        \"\"\"\n        Compute the quantiles of the data incrementally\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        if X.shape[0] == 1:\n            return self\n        if self.quantile_estimator is None:\n            self.quantile_estimator = QuantileEstimator(\n               tdigest_size=100, subsample=self.subsample\n            )\n\n        self.quantile_estimator.update(X.select_dtypes(include=\"number\"))\n        return self\n\n    def fit(self, X: pd.DataFrame):\n        \"\"\"\n        Compute the quantiles of the data\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        if self.subsample &lt; 1:\n\n            sampled_points = np.random.choice(\n                X.shape[0], int(X.shape[0] * self.subsample), replace=False\n            )\n            X = X.iloc[sampled_points, :]\n        self.Q1 = X.quantile(self.lower_quantile)\n        self.Q3 = X.quantile(self.upper_quantile)\n\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Remove the outliers from the input life.\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame with the outliers removed\n        \"\"\"\n\n        if self.Q1 is None:\n            self.Q1 = self.quantile_estimator.estimate_quantile(self.lower_quantile)\n            self.Q3 = self.quantile_estimator.estimate_quantile(self.upper_quantile)\n\n        new_X = X.copy()\n\n\n        if self.clip:\n            new_X.clip(lower=self.Q1, upper=self.Q3, inplace=True, axis=1)\n        else:\n            new_X[new_X &lt; self.Q1] = -np.inf\n            new_X[new_X &gt; self.Q3] = np.inf            \n        return new_X\n\n    def description(self):\n        name = super().description()\n        data = []\n        for k in self.Q1.keys():\n            data.append((k, {\"Q1\": self.Q1[k], \"Q3\": self.Q3[k], \"IQR\": self.IQR[k]}))\n        return (name, data)\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.BeyondQuartileOutlierRemover.fit","title":"<code>fit(X)</code>","text":"<p>Compute the quantiles of the data</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def fit(self, X: pd.DataFrame):\n    \"\"\"\n    Compute the quantiles of the data\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    if self.subsample &lt; 1:\n\n        sampled_points = np.random.choice(\n            X.shape[0], int(X.shape[0] * self.subsample), replace=False\n        )\n        X = X.iloc[sampled_points, :]\n    self.Q1 = X.quantile(self.lower_quantile)\n    self.Q3 = X.quantile(self.upper_quantile)\n\n    return self\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.BeyondQuartileOutlierRemover.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Compute the quantiles of the data incrementally</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame):\n    \"\"\"\n    Compute the quantiles of the data incrementally\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    if X.shape[0] == 1:\n        return self\n    if self.quantile_estimator is None:\n        self.quantile_estimator = QuantileEstimator(\n           tdigest_size=100, subsample=self.subsample\n        )\n\n    self.quantile_estimator.update(X.select_dtypes(include=\"number\"))\n    return self\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.BeyondQuartileOutlierRemover.transform","title":"<code>transform(X)</code>","text":"<p>Remove the outliers from the input life.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the outliers removed</p> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Remove the outliers from the input life.\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame with the outliers removed\n    \"\"\"\n\n    if self.Q1 is None:\n        self.Q1 = self.quantile_estimator.estimate_quantile(self.lower_quantile)\n        self.Q3 = self.quantile_estimator.estimate_quantile(self.upper_quantile)\n\n    new_X = X.copy()\n\n\n    if self.clip:\n        new_X.clip(lower=self.Q1, upper=self.Q3, inplace=True, axis=1)\n    else:\n        new_X[new_X &lt; self.Q1] = -np.inf\n        new_X[new_X &gt; self.Q3] = np.inf            \n    return new_X\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.EWMAOutOfRange","title":"<code>EWMAOutOfRange</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the EWMA limits  and mark as NaN points outside UCL and LCL</p> <p>Parameters:</p> Name Type Description Default <code>lambda_</code> <code>float</code> <p>Parameter for the EWMA, by default 0.5</p> <code>0.5</code> <code>return_mask</code> <code>bool</code> <p>Wether to return a mask with the outliers or the original data with the outliers marked as NaN, by default False</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>class EWMAOutOfRange(TransformerStep):\n    \"\"\"\n    Compute the EWMA limits  and mark as NaN points outside UCL and LCL\n\n    Parameters:\n        lambda_: Parameter for the EWMA, by default 0.5\n        return_mask: Wether to return a mask with the outliers or the original data with the outliers marked as NaN, by default False\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        lambda_ : float=0.5,\n        return_mask: bool = False,\n        name: Optional[str] = None,\n        prefer_partial_fit: bool = False,\n    ):\n        super().__init__(name=name, prefer_partial_fit=prefer_partial_fit)\n        self.lambda_ = lambda_\n        self.UCL = None\n        self.LCL = None\n        self.columns = None\n        self.return_mask = return_mask\n\n    def partial_fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Compute the EWMA limits incrementally\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        if self.columns is None:\n            self.columns = X.columns.values\n        else:\n            self.columns = [c for c in self.columns if c in X.columns]\n        if self.LCL is not None:\n            self.LCL = self.LCL.loc[self.columns].copy()\n            self.UCL = self.UCL.loc[self.columns].copy()\n        LCL, UCL = self._compute_limits(X[self.columns].copy())\n        self.LCL = np.minimum(LCL, self.LCL) if self.LCL is not None else LCL\n        self.UCL = np.maximum(UCL, self.UCL) if self.UCL is not None else UCL\n        return self\n\n    def _compute_limits(self, X):\n\n        mean = np.nanmean(X, axis=0)\n        s = np.sqrt(self.lambda_ / (2 - self.lambda_)) * np.nanstd(X, axis=0)\n        UCL = mean + 3 * s\n        LCL = mean - 3 * s\n        return (pd.Series(LCL, index=self.columns), pd.Series(UCL, index=self.columns))\n\n    def fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Compute the EWMA limits\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        self.columns = X.columns\n        LCL, UCL = self._compute_limits(X)\n        self.LCL = LCL\n        self.UCL = UCL\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Remove the outliers from the input life.\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame with the outliers removed\n        \"\"\"\n        mask = (X[self.columns] &lt; (self.LCL)) | (X[self.columns] &gt; (self.UCL))\n        if self.return_mask:\n            return mask.astype(\"int\")\n        else:\n            X = X.copy()\n            X[mask] = np.nan\n            return X\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.EWMAOutOfRange.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the EWMA limits</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Compute the EWMA limits\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    self.columns = X.columns\n    LCL, UCL = self._compute_limits(X)\n    self.LCL = LCL\n    self.UCL = UCL\n    return self\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.EWMAOutOfRange.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Compute the EWMA limits incrementally</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Compute the EWMA limits incrementally\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    if self.columns is None:\n        self.columns = X.columns.values\n    else:\n        self.columns = [c for c in self.columns if c in X.columns]\n    if self.LCL is not None:\n        self.LCL = self.LCL.loc[self.columns].copy()\n        self.UCL = self.UCL.loc[self.columns].copy()\n    LCL, UCL = self._compute_limits(X[self.columns].copy())\n    self.LCL = np.minimum(LCL, self.LCL) if self.LCL is not None else LCL\n    self.UCL = np.maximum(UCL, self.UCL) if self.UCL is not None else UCL\n    return self\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.EWMAOutOfRange.transform","title":"<code>transform(X)</code>","text":"<p>Remove the outliers from the input life.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the outliers removed</p> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove the outliers from the input life.\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame with the outliers removed\n    \"\"\"\n    mask = (X[self.columns] &lt; (self.LCL)) | (X[self.columns] &gt; (self.UCL))\n    if self.return_mask:\n        return mask.astype(\"int\")\n    else:\n        X = X.copy()\n        X[mask] = np.nan\n        return X\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.IQROutlierRemover","title":"<code>IQROutlierRemover</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Remove values outside (Q1 - marginIQR, Q2 + marginIQR)</p> <p>If clip is True the values will be clipped between the range, otherwise the values are going to be replaced by inf and -inf</p> <p>Parameters:</p> Name Type Description Default <code>lower_quantile</code> <code>float</code> <p>Lower quantile threshold for the non-anomalous values, by feault 0.25</p> <code>0.25</code> <code>upper_quantile</code> <code>float</code> <p>Upper quantile threshold for the non-anomalous values, by feault 0.75</p> <code>0.75</code> <code>margin</code> <code>float</code> <p>How many times the IQR gets multiplied, by default 0.75</p> <code>1.5</code> <code>proportion_to_sample</code> <code>float</code> <p>If you want to compute the quantiles in an smaller proportion of data you can specify it,by default 1.0</p> <code>1.0</code> <code>clip</code> <code>bool</code> <p>Wether to clip the values outside the range, by default False</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>class IQROutlierRemover(TransformerStep):\n    \"\"\"\n    Remove values outside (Q1 - margin*IQR, Q2 + margin*IQR)\n\n    If clip is True the values will be clipped between the range,\n    otherwise the values are going to be replaced by inf and -inf\n\n    Parameters:\n        lower_quantile: Lower quantile threshold for the non-anomalous values, by feault 0.25\n        upper_quantile: Upper quantile threshold for the non-anomalous values, by feault 0.75\n        margin: How many times the IQR gets multiplied, by default 0.75\n        proportion_to_sample: If you want to compute the quantiles in an smaller proportion of data\n            you can specify it,by default 1.0\n        clip: Wether to clip the values outside the range, by default False\n        name: Name of the step, by default None\n\n    \"\"\"\n\n    def __init__(\n        self,\n        lower_quantile: float = 0.25,\n        upper_quantile: float = 0.75,\n        margin: float=1.5,\n        proportion_to_sample: float=1.0,\n        clip: bool = False,\n        name: Optional[str] = None,\n        prefer_partial_fit: bool = False,\n    ):\n\n        super().__init__(name=name, prefer_partial_fit=prefer_partial_fit)\n        self.margin = margin\n        self.proportion_to_sample = proportion_to_sample\n        self.tdigest_dict = None\n        self.lower_quantile = lower_quantile\n        self.upper_quantile = upper_quantile\n        self.clip = clip\n\n    def partial_fit(self, X: pd.DataFrame):\n        \"\"\"\n        Compute the quantiles of the data and the interquartile range incrementally\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        if X.shape[0] == 1:\n            return self\n        if self.proportion_to_sample &lt; 1:\n            sampled_points = np.random.choice(\n                X.shape[0], int(X.shape[0] * self.proportion_to_sample), replace=False\n            )\n            X = X.iloc[sampled_points, :]\n        if self.tdigest_dict is None:\n            self.tdigest_dict = {c: TDigest(100) for c in X.columns}\n        for c in X.columns:\n            self.tdigest_dict[c] = self.tdigest_dict[c].merge_unsorted(X[c].values)\n\n        self.Q1 = {\n            c: self.tdigest_dict[c].estimate_quantile(self.lower_quantile)\n            for c in self.tdigest_dict.keys()\n        }\n\n        self.Q3 = {\n            c: self.tdigest_dict[c].estimate_quantile(self.upper_quantile)\n            for c in self.tdigest_dict.keys()\n        }\n\n        self.IQR = {c: self.Q3[c] - self.Q1[c] for c in self.Q1.keys()}\n        return self\n\n    def fit(self, X: pd.DataFrame):\n        \"\"\"\n        Compute the quantiles of the data and the interquartile range incrementally\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        if self.proportion_to_sample &lt; 1:\n            sampled_points = np.random.choice(\n                X.shape[0], int(X.shape[0] * self.proportion_to_sample), replace=False\n            )\n            X = X.iloc[sampled_points, :]\n        self.Q1 = X.quantile(self.lower_quantile)\n        self.Q3 = X.quantile(self.upper_quantile)\n        self.IQR = (self.Q3 - self.Q1).to_dict()\n        self.Q1 = self.Q1.to_dict()\n        self.Q3 = self.Q3.to_dict()\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Remove the outliers from the input life. \n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame with the outliers removed\n        \"\"\"\n        X = X.copy()\n        check_is_fitted(self, \"Q1\")\n        check_is_fitted(self, \"Q3\")\n        check_is_fitted(self, \"IQR\")\n        for c in X.columns:\n            min_value = self.Q1[c] - self.margin * self.IQR[c]\n            mask = X[c] &lt; min_value\n            if not self.clip:\n                X.loc[mask, c] = -np.inf\n            else:\n                X.loc[mask, c] = min_value\n            max_value = self.Q3[c] + self.margin * self.IQR[c]\n            mask = X[c] &gt; (max_value)\n            if not self.clip:\n                X.loc[mask, c] = np.inf\n            else:\n                X.loc[mask, c] = max_value\n        return X\n\n    def description(self):\n        name = super().description()\n        data = []\n        for k in self.Q1.keys():\n            data.append((k, {\"Q1\": self.Q1[k], \"Q3\": self.Q3[k], \"IQR\": self.IQR[k]}))\n        return (name, data)\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.IQROutlierRemover.fit","title":"<code>fit(X)</code>","text":"<p>Compute the quantiles of the data and the interquartile range incrementally</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def fit(self, X: pd.DataFrame):\n    \"\"\"\n    Compute the quantiles of the data and the interquartile range incrementally\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    if self.proportion_to_sample &lt; 1:\n        sampled_points = np.random.choice(\n            X.shape[0], int(X.shape[0] * self.proportion_to_sample), replace=False\n        )\n        X = X.iloc[sampled_points, :]\n    self.Q1 = X.quantile(self.lower_quantile)\n    self.Q3 = X.quantile(self.upper_quantile)\n    self.IQR = (self.Q3 - self.Q1).to_dict()\n    self.Q1 = self.Q1.to_dict()\n    self.Q3 = self.Q3.to_dict()\n    return self\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.IQROutlierRemover.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Compute the quantiles of the data and the interquartile range incrementally</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame):\n    \"\"\"\n    Compute the quantiles of the data and the interquartile range incrementally\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    if X.shape[0] == 1:\n        return self\n    if self.proportion_to_sample &lt; 1:\n        sampled_points = np.random.choice(\n            X.shape[0], int(X.shape[0] * self.proportion_to_sample), replace=False\n        )\n        X = X.iloc[sampled_points, :]\n    if self.tdigest_dict is None:\n        self.tdigest_dict = {c: TDigest(100) for c in X.columns}\n    for c in X.columns:\n        self.tdigest_dict[c] = self.tdigest_dict[c].merge_unsorted(X[c].values)\n\n    self.Q1 = {\n        c: self.tdigest_dict[c].estimate_quantile(self.lower_quantile)\n        for c in self.tdigest_dict.keys()\n    }\n\n    self.Q3 = {\n        c: self.tdigest_dict[c].estimate_quantile(self.upper_quantile)\n        for c in self.tdigest_dict.keys()\n    }\n\n    self.IQR = {c: self.Q3[c] - self.Q1[c] for c in self.Q1.keys()}\n    return self\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.IQROutlierRemover.transform","title":"<code>transform(X)</code>","text":"<p>Remove the outliers from the input life. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the outliers removed</p> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove the outliers from the input life. \n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame with the outliers removed\n    \"\"\"\n    X = X.copy()\n    check_is_fitted(self, \"Q1\")\n    check_is_fitted(self, \"Q3\")\n    check_is_fitted(self, \"IQR\")\n    for c in X.columns:\n        min_value = self.Q1[c] - self.margin * self.IQR[c]\n        mask = X[c] &lt; min_value\n        if not self.clip:\n            X.loc[mask, c] = -np.inf\n        else:\n            X.loc[mask, c] = min_value\n        max_value = self.Q3[c] + self.margin * self.IQR[c]\n        mask = X[c] &gt; (max_value)\n        if not self.clip:\n            X.loc[mask, c] = np.inf\n        else:\n            X.loc[mask, c] = max_value\n    return X\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.IsolationForestOutlierRemover","title":"<code>IsolationForestOutlierRemover</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Remove outliers using Isolation Forests to detect them.</p> <p>Parameters:</p> Name Type Description Default <code>n_estimators</code> <p>Number of trees in the forest, by default 100</p> <code>100</code> <code>name</code> <p>Name of the step, by default None</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>class IsolationForestOutlierRemover(TransformerStep):\n    \"\"\" \n    Remove outliers using Isolation Forests to detect them.\n\n    Parameters:\n        n_estimators: Number of trees in the forest, by default 100\n        name: Name of the step, by default None\n    \"\"\"\n    def __init__(self, *, n_estimators=100, **kwargs):\n        super().__init__(prefer_partial_fit=False, **kwargs)\n        self.n_estimators = n_estimators\n        self.forests = {}\n\n    def fit(self, X: pd.DataFrame):\n        \"\"\"\n        Fit the Isolation Forest model to the data\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        for c in X.columns:\n            self.forests[c] = IsolationForest(n_estimators=self.n_estimators).fit(X[c].values.reshape(-1, 1) )\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Remove the outliers from the input life.\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame with the outliers removed\n        \"\"\"\n        X_new = X.copy()\n        for c in X.columns:\n            r = self.forests[c].predict(X[c].values.reshape(-1, 1) )\n            X_new.loc[r == -1, c] = np.nan\n        return X_new\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.IsolationForestOutlierRemover.fit","title":"<code>fit(X)</code>","text":"<p>Fit the Isolation Forest model to the data</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def fit(self, X: pd.DataFrame):\n    \"\"\"\n    Fit the Isolation Forest model to the data\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    for c in X.columns:\n        self.forests[c] = IsolationForest(n_estimators=self.n_estimators).fit(X[c].values.reshape(-1, 1) )\n    return self\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.IsolationForestOutlierRemover.transform","title":"<code>transform(X)</code>","text":"<p>Remove the outliers from the input life.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the outliers removed</p> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove the outliers from the input life.\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame with the outliers removed\n    \"\"\"\n    X_new = X.copy()\n    for c in X.columns:\n        r = self.forests[c].predict(X[c].values.reshape(-1, 1) )\n        X_new.loc[r == -1, c] = np.nan\n    return X_new\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.RollingMeanOutlierRemover","title":"<code>RollingMeanOutlierRemover</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the rolling mean and use it to compute the upper and lower bound to define outliers </p> <p>Parameters:</p> Name Type Description Default <code>window</code> <code>int</code> <p>Window for the rolling mean, by default 15</p> <code>15</code> <code>lambda_</code> <code>float</code> <p>Multiplier of the std used to define the bounds, by default 3</p> <code>3</code> <code>return_mask</code> <code>bool</code> <p>Wether to return a mask with the outliers or the original data with the outliers marked as NaN, by default False</p> <code>False</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>class RollingMeanOutlierRemover(TransformerStep):\n    \"\"\"\n    Compute the rolling mean and use it to compute the upper and lower bound to define outliers \n\n    Parameters:\n        window: Window for the rolling mean, by default 15\n        lambda_: Multiplier of the std used to define the bounds, by default 3\n        return_mask: Wether to return a mask with the outliers or the original data with the outliers marked as NaN, by default False\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        window: int = 15,\n        lambda_: float = 3,\n        return_mask: bool = False,\n        name: Optional[str] = None,\n    ):\n        super().__init__(name=name)\n        self.window = window\n        self.lambda_ = lambda_\n        self.return_mask = return_mask\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Remove the outliers from the input life.\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame with the outliers removed\n        \"\"\"\n        r = X.rolling(self.window, min_periods=1)\n        std = r.quantile(0.75) -  r.quantile(0.25)\n        upper = r.median() + (self.lambda_ * std)\n        lower = r.median() - (self.lambda_ * std)\n        mask = (X &gt; upper) | (X &lt; lower)\n        if self.return_mask:\n            return mask.astype(\"int\")\n        else:\n            X = X.copy()\n            X[(X &gt; upper)] = np.minimum(upper.values, X) \n            X[(X &lt; upper)] = np.maximum(lower.values, X) \n\n            #X[mask] = np.nan\n            return X\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.RollingMeanOutlierRemover.transform","title":"<code>transform(X)</code>","text":"<p>Remove the outliers from the input life.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the outliers removed</p> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove the outliers from the input life.\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame with the outliers removed\n    \"\"\"\n    r = X.rolling(self.window, min_periods=1)\n    std = r.quantile(0.75) -  r.quantile(0.25)\n    upper = r.median() + (self.lambda_ * std)\n    lower = r.median() - (self.lambda_ * std)\n    mask = (X &gt; upper) | (X &lt; lower)\n    if self.return_mask:\n        return mask.astype(\"int\")\n    else:\n        X = X.copy()\n        X[(X &gt; upper)] = np.minimum(upper.values, X) \n        X[(X &lt; upper)] = np.maximum(lower.values, X) \n\n        #X[mask] = np.nan\n        return X\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.ZScoreOutlierRemover","title":"<code>ZScoreOutlierRemover</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Remove values outside (mean - number_of_std_allowedstd, mean + number_of_std_allowedstd). The outliers are set to NaN</p> <p>Parameters:</p> Name Type Description Default <code>number_of_std_allowed</code> <p>Number of standard deviations to consider a point an outlier</p> required <code>name</code> <code>str</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>class ZScoreOutlierRemover(TransformerStep):\n    \"\"\"\n    Remove values outside (mean - number_of_std_allowed*std, mean + number_of_std_allowed*std). The outliers are set to NaN\n\n    Parameters:\n        number_of_std_allowed: Number of standard deviations to consider a point an outlier\n        name: Name of the step, by default None\n    \"\"\"\n    #X = np.random.rand(500, 5) * np.random.randn(500, 5) * 15\n    #imput = ZScoreImputer(1.5)\n    #imput.fit(X)\n    #X_t = imput.transform(X)\n\n    def __init__(\n        self,\n        *,\n        number_of_std_allowed,\n        name: str = None,\n        prefer_partial_fit: bool = False,\n    ):\n        super().__init__(name=name, prefer_partial_fit=prefer_partial_fit)\n        self.number_of_std_allowed = number_of_std_allowed\n        self.scaler = StandardScaler()\n\n    def fit(self, X: pd.DataFrame):\n        \"\"\"\n        Fit a StandardScaler to the data\n\n        Parameters:\n            X: Input life\n        \"\"\"\n        self.scaler.fit(X)\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Remove the outliers from the input life.\n\n        Parameters:\n            X: Input life\n\n        Returns:\n            A new DataFrame with the outliers removed\n        \"\"\"\n        X_new = self.scaler.transform(X)\n        X_new[np.abs(X_new) &gt; self.number_of_std_allowed] = np.nan\n        return pd.DataFrame(X_new, columns=X.columns, index=X.index)\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.ZScoreOutlierRemover.fit","title":"<code>fit(X)</code>","text":"<p>Fit a StandardScaler to the data</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def fit(self, X: pd.DataFrame):\n    \"\"\"\n    Fit a StandardScaler to the data\n\n    Parameters:\n        X: Input life\n    \"\"\"\n    self.scaler.fit(X)\n    return self\n</code></pre>"},{"location":"transformation/features/outliers/#ceruleo.transformation.features.outliers.ZScoreOutlierRemover.transform","title":"<code>transform(X)</code>","text":"<p>Remove the outliers from the input life.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the outliers removed</p> Source code in <code>ceruleo/transformation/features/outliers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove the outliers from the input life.\n\n    Parameters:\n        X: Input life\n\n    Returns:\n        A new DataFrame with the outliers removed\n    \"\"\"\n    X_new = self.scaler.transform(X)\n    X_new[np.abs(X_new) &gt; self.number_of_std_allowed] = np.nan\n    return pd.DataFrame(X_new, columns=X.columns, index=X.index)\n</code></pre>"},{"location":"transformation/features/resamplers/","title":"Resamplers","text":""},{"location":"transformation/features/resamplers/#resamplers","title":"Resamplers","text":""},{"location":"transformation/features/resamplers/#ceruleo.transformation.features.resamplers.IndexMeanResampler","title":"<code>IndexMeanResampler</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Resample  When the index of the run-to-failure cycle is a time feature</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>str</code> <p>Time frequency or rule according to which the data should be resampled</p> required Source code in <code>ceruleo/transformation/features/resamplers.py</code> <pre><code>class IndexMeanResampler(TransformerStep):\n    \"\"\"Resample \n    When the index of the run-to-failure cycle is a time feature\n\n    Parameters:\n        rule: Time frequency or rule according to which the data should be resampled\n    \"\"\"\n    def __init__(self, *, rule: str,  **kwargs):\n        super().__init__(**kwargs)\n        self.rule = rule\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life\n\n        Parameters:\n            X: The input life\n\n        Returns: \n            The resampled DataFrame\n        \"\"\"\n        return X.resample(self.rule).mean().dropna()\n</code></pre>"},{"location":"transformation/features/resamplers/#ceruleo.transformation.features.resamplers.IndexMeanResampler.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The resampled DataFrame</p> Source code in <code>ceruleo/transformation/features/resamplers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life\n\n    Parameters:\n        X: The input life\n\n    Returns: \n        The resampled DataFrame\n    \"\"\"\n    return X.resample(self.rule).mean().dropna()\n</code></pre>"},{"location":"transformation/features/resamplers/#ceruleo.transformation.features.resamplers.IntegerIndexResamplerTransformer","title":"<code>IntegerIndexResamplerTransformer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Resample the time series with an integer index and interpolate linearly the values</p> <p>Parameters:</p> Name Type Description Default <code>time_feature</code> <code>str</code> <p>Time feature</p> required <code>steps</code> <code>int</code> <p>Number of steps</p> required <code>drop_time_feature</code> <code>bool</code> <p>Drop the time feature</p> required Source code in <code>ceruleo/transformation/features/resamplers.py</code> <pre><code>class IntegerIndexResamplerTransformer(TransformerStep):\n    \"\"\"\n    Resample the time series with an integer index and interpolate linearly the values\n\n    Parameters:\n        time_feature: Time feature\n        steps: Number of steps\n        drop_time_feature: Drop the time feature\n    \"\"\"\n\n    def __init__(self, *args, time_feature: str, steps: int, drop_time_feature: bool):\n        super().__init__(*args)\n        self._time_feature_name = time_feature\n        self._time_feature = None\n        self.steps = steps\n        self.drop_time_feature = drop_time_feature\n\n    def partial_fit(self, X: pd.DataFrame):\n        \"\"\"\n        Obtain the name of the feature used as time\n\n        Parameters:\n            X: The current time-series to be fitted\n\n        Returns:\n            Instance of class IntegerIndexResamplerTransformer\n        \"\"\"\n        if self._time_feature is None:\n            self._time_feature = self.find_feature(X, self._time_feature_name)\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life\n\n        Parameters:\n            X: The input life\n\n        Returns: \n            The resampled DataFrame\n        \"\"\"\n        Told = X[self._time_feature].values\n        Tnew = np.arange(Told.min(), Told.max(), self.steps)\n        new_columns = X.columns.values\n        if self.drop_time_feature:\n            new_columns = np.delete(\n                new_columns, np.where(new_columns == self._time_feature)\n            )\n\n        new_X = pd.DataFrame(index=Tnew, columns=new_columns)\n        for c in new_columns:\n            try:\n                F = interp1d(Told, X[c])\n                new_X[c] = F(Tnew)\n            except ValueError:\n                # https://stackoverflow.com/questions/62015823/interpolating-categorical-data-in-python-nearest-previous-value/62016097#62016097\n                y = X[c].values\n                f = interp1d(\n                    Told,\n                    range(len(y)),\n                    kind=\"nearest\",\n                    fill_value=(0, len(y) - 1),\n                    bounds_error=False,\n                )\n                y_idx = f(Tnew)\n                new_X[c] = [y[int(i)] for i in y_idx]\n        return new_X\n</code></pre>"},{"location":"transformation/features/resamplers/#ceruleo.transformation.features.resamplers.IntegerIndexResamplerTransformer.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Obtain the name of the feature used as time</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The current time-series to be fitted</p> required <p>Returns:</p> Type Description <p>Instance of class IntegerIndexResamplerTransformer</p> Source code in <code>ceruleo/transformation/features/resamplers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame):\n    \"\"\"\n    Obtain the name of the feature used as time\n\n    Parameters:\n        X: The current time-series to be fitted\n\n    Returns:\n        Instance of class IntegerIndexResamplerTransformer\n    \"\"\"\n    if self._time_feature is None:\n        self._time_feature = self.find_feature(X, self._time_feature_name)\n    return self\n</code></pre>"},{"location":"transformation/features/resamplers/#ceruleo.transformation.features.resamplers.IntegerIndexResamplerTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The resampled DataFrame</p> Source code in <code>ceruleo/transformation/features/resamplers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life\n\n    Parameters:\n        X: The input life\n\n    Returns: \n        The resampled DataFrame\n    \"\"\"\n    Told = X[self._time_feature].values\n    Tnew = np.arange(Told.min(), Told.max(), self.steps)\n    new_columns = X.columns.values\n    if self.drop_time_feature:\n        new_columns = np.delete(\n            new_columns, np.where(new_columns == self._time_feature)\n        )\n\n    new_X = pd.DataFrame(index=Tnew, columns=new_columns)\n    for c in new_columns:\n        try:\n            F = interp1d(Told, X[c])\n            new_X[c] = F(Tnew)\n        except ValueError:\n            # https://stackoverflow.com/questions/62015823/interpolating-categorical-data-in-python-nearest-previous-value/62016097#62016097\n            y = X[c].values\n            f = interp1d(\n                Told,\n                range(len(y)),\n                kind=\"nearest\",\n                fill_value=(0, len(y) - 1),\n                bounds_error=False,\n            )\n            y_idx = f(Tnew)\n            new_X[c] = [y[int(i)] for i in y_idx]\n    return new_X\n</code></pre>"},{"location":"transformation/features/resamplers/#ceruleo.transformation.features.resamplers.SubsamplerTransformer","title":"<code>SubsamplerTransformer</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>IntegerIndexResamplerTransformer</p> <p>Resample the time series with an integer index and interpolate linearly the values</p> <p>Parameters:</p> Name Type Description Default <code>time_feature</code> <code>str</code> <p>Time feature</p> required <code>steps</code> <code>int</code> <p>Number of steps</p> required <code>drop_time_feature</code> <code>bool</code> <p>Drop the time feature</p> required Source code in <code>ceruleo/transformation/features/resamplers.py</code> <pre><code>class SubsamplerTransformer(TransformerStep):\n    \"\"\"IntegerIndexResamplerTransformer\n\n    Resample the time series with an integer index and interpolate linearly the values\n\n    Parameters:\n        time_feature: Time feature\n        steps:  Number of steps\n        drop_time_feature: Drop the time feature\n    \"\"\"\n\n    def __init__(self, *args, time_feature: str, steps: int, drop_time_feature: bool):\n        super().__init__(*args)\n        self._time_feature_name = time_feature\n        self._time_feature = None\n        self.steps = steps\n        self.drop_time_feature = drop_time_feature\n\n    def partial_fit(self, X: pd.DataFrame):\n        \"\"\"Obtain the name of the feature used as time\n\n        Parameters:\n            X: The current time-series to be fitted\n\n        Returns:\n            Instance of class IntegerIndexResamplerTransformer\n\n        \"\"\"\n        if self._time_feature is None:\n            self._time_feature = self.find_feature(X, self._time_feature_name)\n            if self._time_feature is None:\n                raise ValueError(\"Time feature not found\")\n\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life\n\n        Parameters:\n            X: The input life\n\n        Returns: \n            The resampled DataFrame\n        \"\"\"\n        X = X.groupby(X[self._time_feature] // self.steps, sort=False).mean()\n        if self.drop_time_feature:\n            X = X.drop(columns=[self._time_feature])\n\n        return X\n</code></pre>"},{"location":"transformation/features/resamplers/#ceruleo.transformation.features.resamplers.SubsamplerTransformer.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Obtain the name of the feature used as time</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The current time-series to be fitted</p> required <p>Returns:</p> Type Description <p>Instance of class IntegerIndexResamplerTransformer</p> Source code in <code>ceruleo/transformation/features/resamplers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame):\n    \"\"\"Obtain the name of the feature used as time\n\n    Parameters:\n        X: The current time-series to be fitted\n\n    Returns:\n        Instance of class IntegerIndexResamplerTransformer\n\n    \"\"\"\n    if self._time_feature is None:\n        self._time_feature = self.find_feature(X, self._time_feature_name)\n        if self._time_feature is None:\n            raise ValueError(\"Time feature not found\")\n\n    return self\n</code></pre>"},{"location":"transformation/features/resamplers/#ceruleo.transformation.features.resamplers.SubsamplerTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The resampled DataFrame</p> Source code in <code>ceruleo/transformation/features/resamplers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life\n\n    Parameters:\n        X: The input life\n\n    Returns: \n        The resampled DataFrame\n    \"\"\"\n    X = X.groupby(X[self._time_feature] // self.steps, sort=False).mean()\n    if self.drop_time_feature:\n        X = X.drop(columns=[self._time_feature])\n\n    return X\n</code></pre>"},{"location":"transformation/features/rolling_windows/","title":"Rolling Windows","text":""},{"location":"transformation/features/rolling_windows/#rolling-windows","title":"Rolling Windows","text":""},{"location":"transformation/features/rolling_windows/#ceruleo.transformation.features.rolling_windows.apply_rolling_data","title":"<code>apply_rolling_data(values, function, window, step=1)</code>","text":"<p>Perform a rolling window analysis at the column <code>col</code> from <code>data</code></p> <p>Given a dataframe <code>data</code> with time series, call <code>function</code> at sections of length <code>window</code> at the data of column <code>col</code>. Append the results to <code>data</code> at a new columns with name <code>label</code>.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>1-D Time series of data</p> required <code>function</code> <code>Callable[[ndarray], ndarray]</code> <p>Function to be called to calculate the rolling window analysis, the function must receive as input an array or pandas series. Its output must be either a number or a pandas series</p> required <code>window</code> <code>int</code> <p>Length of the window to perform the analysis</p> required <code>step</code> <code>int</code> <p>Step to take between two consecutive windows, by default 1</p> <code>1</code> <p>Returns:</p> Name Type Description <code>data</code> <code>array</code> <p>Columns generated by the function applied</p> Source code in <code>ceruleo/transformation/features/rolling_windows.py</code> <pre><code>def apply_rolling_data(values : np.ndarray, function: Callable[[np.ndarray], np.ndarray], window: int, step: int =1) -&gt; np.array:\n    \"\"\"\n    Perform a rolling window analysis at the column `col` from `data`\n\n    Given a dataframe `data` with time series, call `function` at sections of length `window` at the data of column `col`. Append the results to `data` at a new columns with name `label`.\n\n    Parameters:\n        values: 1-D Time series of data\n        function: Function to be called to calculate the rolling window analysis, the function must receive as input an array or pandas series. Its output must be either a number or a pandas series\n        window: Length of the window to perform the analysis\n        step: Step to take between two consecutive windows, by default 1\n\n    Returns:\n        data: Columns generated by the function applied\n    \"\"\"\n\n    x = _strided_app(values, window, step)\n\n    return np.vstack([function(np.array(b)) for b in x])\n</code></pre>"},{"location":"transformation/features/scalers/","title":"Scalers","text":""},{"location":"transformation/features/scalers/#scaling","title":"Scaling","text":""},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.MinMaxScaler","title":"<code>MinMaxScaler</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Transform features by scaling each feature to a given range.</p> <p>This transformer scales and translates each feature individually such that it is in the given range on the training set.</p> <p>Parameters:</p> Name Type Description Default <code>range</code> <code>tuple</code> <p>Desired range of transformed data.</p> required <code>clip</code> <code>bool</code> <p>Set to True to clip transformed values of held-out data to provided, by default True</p> <code>True</code> <code>fillna</code> <code>Optional[float]</code> <p>Wheter to fill NaN with a value</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>class MinMaxScaler(TransformerStep):\n    \"\"\"\n    Transform features by scaling each feature to a given range.\n\n    This transformer scales and translates each feature individually\n    such that it is in the given range on the training set.\n\n    Parameters:\n        range: Desired range of transformed data.\n        clip: Set to True to clip transformed values of held-out data to provided, by default True\n        fillna: Wheter to fill NaN with a value\n        name: Name of the step, by default None\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        range: tuple,\n        clip: bool = True,\n        fillna: Optional[float] = None,\n        name: Optional[str] = None,\n    ):\n        super().__init__(name=name)\n        self.range = range\n        self.min = range[0]\n        self.max = range[1]\n        self.data_min = None\n        self.data_max = None\n        self.clip = clip\n        self.fillna = fillna\n\n    def partial_fit(self, df: pd.DataFrame, y=None):\n        \"\"\"\n        Compute the dataset's bounds\n\n        Parameters:\n            df: The input dataset\n        \"\"\"\n        partial_data_min = df.min(skipna=True)\n        partial_data_max = df.max(skipna=True)\n        if self.data_min is None:\n            self.data_min = partial_data_min\n            self.data_max = partial_data_max\n        else:\n            self.data_min = pd.concat([self.data_min, partial_data_min], axis=1).min(\n                axis=1, skipna=True\n            )\n            self.data_max = pd.concat([self.data_max, partial_data_max], axis=1).max(\n                axis=1, skipna=True\n            )\n        return self\n\n    def fit(self, df: pd.DataFrame, y=None):\n        \"\"\"\n        Compute the dataset's bounds\n\n        Parameters:\n            df: The input dataset\n        \"\"\"\n        self.data_min = df.min(skipna=True)\n        self.data_max = df.max(skipna=True)\n\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Scale the input dataset\n\n        Parameters:\n            X: The input dataset\n\n        Returns:\n            A new DataFrame with the same index as the input with the data scaled in the range inserted in input\n        \"\"\"\n        try:\n            divisor = self.data_max - self.data_min\n\n            mask = np.abs((divisor)) &gt; 1e-25\n            X = X.astype(float)\n            X.loc[:, mask] = (\n                (X.loc[:, mask] - self.data_min[mask])\n                / divisor[mask]\n                * (self.max - self.min)\n            ) + self.min\n            if self.fillna is not None:\n                X.loc[:, ~mask] = self.fillna\n        except:\n            raise\n        if self.clip:\n            X.clip(lower=self.min, upper=self.max, inplace=True)\n        return X\n\n    def description(self):\n        data = super().description()\n        return (data, {\"Min\": self.data_min, \"Max\": self.data_max})\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.MinMaxScaler.fit","title":"<code>fit(df, y=None)</code>","text":"<p>Compute the dataset's bounds</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def fit(self, df: pd.DataFrame, y=None):\n    \"\"\"\n    Compute the dataset's bounds\n\n    Parameters:\n        df: The input dataset\n    \"\"\"\n    self.data_min = df.min(skipna=True)\n    self.data_max = df.max(skipna=True)\n\n    return self\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.MinMaxScaler.partial_fit","title":"<code>partial_fit(df, y=None)</code>","text":"<p>Compute the dataset's bounds</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def partial_fit(self, df: pd.DataFrame, y=None):\n    \"\"\"\n    Compute the dataset's bounds\n\n    Parameters:\n        df: The input dataset\n    \"\"\"\n    partial_data_min = df.min(skipna=True)\n    partial_data_max = df.max(skipna=True)\n    if self.data_min is None:\n        self.data_min = partial_data_min\n        self.data_max = partial_data_max\n    else:\n        self.data_min = pd.concat([self.data_min, partial_data_min], axis=1).min(\n            axis=1, skipna=True\n        )\n        self.data_max = pd.concat([self.data_max, partial_data_max], axis=1).max(\n            axis=1, skipna=True\n        )\n    return self\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.MinMaxScaler.transform","title":"<code>transform(X)</code>","text":"<p>Scale the input dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the data scaled in the range inserted in input</p> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Scale the input dataset\n\n    Parameters:\n        X: The input dataset\n\n    Returns:\n        A new DataFrame with the same index as the input with the data scaled in the range inserted in input\n    \"\"\"\n    try:\n        divisor = self.data_max - self.data_min\n\n        mask = np.abs((divisor)) &gt; 1e-25\n        X = X.astype(float)\n        X.loc[:, mask] = (\n            (X.loc[:, mask] - self.data_min[mask])\n            / divisor[mask]\n            * (self.max - self.min)\n        ) + self.min\n        if self.fillna is not None:\n            X.loc[:, ~mask] = self.fillna\n    except:\n        raise\n    if self.clip:\n        X.clip(lower=self.min, upper=self.max, inplace=True)\n    return X\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.PerCategoricalMinMaxScaler","title":"<code>PerCategoricalMinMaxScaler</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Performs a minmax scaler partition of the data trough some categorical feature</p> <p>Usually, different execution configurations lead to different scales in the features. Therefore, sometimes it is useful to scale the data based on a categorical feature, to reflect the difference in the execution parameters.</p> <p>Parameters:</p> Name Type Description Default <code>categorical_feature</code> <code>str</code> <p>str The name of the categorical feature whose values are going to be used to split each time-series</p> required <code>scaler</code> <code>Optional[Union[MinMaxScaler, RobustMinMaxScaler]]</code> <p>The scaler to use when scaling the data, by default MinMaxScaler</p> <code>MinMaxScaler</code> <code>scaler_params</code> <code>dict</code> <p>Parameters used when constructing the scaler, by default {}</p> <code>{}</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>class PerCategoricalMinMaxScaler(TransformerStep):\n    \"\"\"\n    Performs a minmax scaler partition of the data trough some categorical feature\n\n    Usually, different execution configurations lead to different scales in the features.\n    Therefore, sometimes it is useful to scale the data based on a categorical feature,\n    to reflect the difference in the execution parameters.\n\n    Parameters:\n        categorical_feature: str\n            The name of the categorical feature whose values are going to be used to split each time-series\n        scaler: The scaler to use when scaling the data, by default MinMaxScaler\n        scaler_params: Parameters used when constructing the scaler, by default {}\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        categorical_feature: str,\n        scaler: Optional[Union[MinMaxScaler, RobustMinMaxScaler]] = MinMaxScaler,\n        scaler_params: dict = {},\n        name: Optional[str] = None,\n    ):\n        super().__init__(name=name)\n        self.categorical_feature = categorical_feature\n        self.categorical_feature_name = None\n        self.scaler = scaler\n        self.scaler_params = scaler_params\n\n        self.scalers = {\"default\": self.scaler(**self.scaler_params)}\n\n    def partial_fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Fit the scaler\n\n        Parameters:\n            X: The input dataset\n        \"\"\"\n        if self.categorical_feature_name is None:\n            self.categorical_feature_name = self.find_feature(\n                X, self.categorical_feature\n            )\n        for category, data in X.groupby(self.categorical_feature_name):\n            data = data.drop(columns=[self.categorical_feature_name])\n            if category not in self.scalers:\n                self.scalers[category] = self.scaler(**self.scaler_params)\n            self.scalers[category].partial_fit(data)\n            self.scalers[\"default\"].partial_fit(data)\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Scale the input dataset using the appropriate scaler for each category\n\n        Parameters:\n            X: The input dataset\n\n        Returns:\n            A new DataFrame with the same index as the input with the data scaled with respect to the categorical feature\n        \"\"\"\n        X_new = X.drop(columns=[self.categorical_feature_name])\n\n        for category, data in X.groupby(self.categorical_feature_name):\n\n            data = data.drop(columns=[self.categorical_feature_name])\n            scaler = (\n                self.scalers[category]\n                if category in self.scalers\n                else self.scalers[\"default\"]\n            )  # Use a defaultdict\n            X_new.loc[data.index, :] = scaler.transform(data)\n        return X_new\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.PerCategoricalMinMaxScaler.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Fit the scaler</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Fit the scaler\n\n    Parameters:\n        X: The input dataset\n    \"\"\"\n    if self.categorical_feature_name is None:\n        self.categorical_feature_name = self.find_feature(\n            X, self.categorical_feature\n        )\n    for category, data in X.groupby(self.categorical_feature_name):\n        data = data.drop(columns=[self.categorical_feature_name])\n        if category not in self.scalers:\n            self.scalers[category] = self.scaler(**self.scaler_params)\n        self.scalers[category].partial_fit(data)\n        self.scalers[\"default\"].partial_fit(data)\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.PerCategoricalMinMaxScaler.transform","title":"<code>transform(X)</code>","text":"<p>Scale the input dataset using the appropriate scaler for each category</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the data scaled with respect to the categorical feature</p> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Scale the input dataset using the appropriate scaler for each category\n\n    Parameters:\n        X: The input dataset\n\n    Returns:\n        A new DataFrame with the same index as the input with the data scaled with respect to the categorical feature\n    \"\"\"\n    X_new = X.drop(columns=[self.categorical_feature_name])\n\n    for category, data in X.groupby(self.categorical_feature_name):\n\n        data = data.drop(columns=[self.categorical_feature_name])\n        scaler = (\n            self.scalers[category]\n            if category in self.scalers\n            else self.scalers[\"default\"]\n        )  # Use a defaultdict\n        X_new.loc[data.index, :] = scaler.transform(data)\n    return X_new\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.RobustMinMaxScaler","title":"<code>RobustMinMaxScaler</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Scale features using statistics that are robust to outliers.</p> <p>This Scaler scales the data according to the quantile range. The IQR is the range between the limits provided, by default, 1st quartile (25th quantile) and the 3rd quartile (75th quantile).</p> <p>The quantiles are approximated using tdigest</p> <p>Parameters:</p> Name Type Description Default <code>range</code> <code>tuple</code> <p>Desired range of transformed data.</p> required <code>clip</code> <code>bool</code> <p>Set to True to clip transformed values of held-out data to provided, by default True</p> <code>True</code> <code>lower_quantile</code> <code>float</code> <p>Lower limit of the quantile range to compute the scale, by default 0.25</p> <code>0.25</code> <code>upper_quantile</code> <code>float</code> <p>Upper limit of the quantile range to compute the scale, by default 0.75</p> <code>0.75</code> <code>tdigest_size</code> <p>Size of the t-digest structure, by default 100</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>class RobustMinMaxScaler(TransformerStep):\n    \"\"\"\n    Scale features using statistics that are robust to outliers.\n\n    This Scaler scales the data according to the quantile range.\n    The IQR is the range between the limits provided, by default,\n    1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n\n    The quantiles are approximated using tdigest\n\n    Parameters:\n        range: Desired range of transformed data.\n        clip: Set to True to clip transformed values of held-out data to provided, by default True\n        lower_quantile: Lower limit of the quantile range to compute the scale, by default 0.25\n        upper_quantile: Upper limit of the quantile range to compute the scale, by default 0.75\n        tdigest_size: Size of the t-digest structure, by default 100\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        range: tuple,\n        clip: bool = True,\n        lower_quantile: float = 0.25,\n        upper_quantile: float = 0.75,\n        max_workers: int = 1,\n        subsample: Optional[Union[int, float]] = None,\n        name: Optional[str] = None,\n        prefer_partial_fit:bool = False\n    ):\n\n        super().__init__(name=name, prefer_partial_fit=prefer_partial_fit)\n        self.range = range\n        self.Q1 = None\n        self.Q3 = None\n        self.clip = clip\n        self.quantile_estimator = QuantileEstimator(\n            tdigest_size=50, subsample=subsample, max_workers=max_workers\n        )\n        self.lower_quantile = lower_quantile\n        self.upper_quantile = upper_quantile\n\n    def _compute_quantiles(self):\n        self.Q1 = self.quantile_estimator.quantile(self.lower_quantile)\n        self.Q3 = self.quantile_estimator.quantile(self.upper_quantile)\n\n        self.IQR = self.Q3 - self.Q1\n\n        self.valid_mask = self.IQR.abs() &gt; 0.000000000001\n\n\n    def partial_fit(self, df: pd.DataFrame, y=None):\n        \"\"\" \n        Compute the quantiles of the dataset\n\n        Parameters:\n            df: The input dataset\n        \"\"\"\n        self.quantile_estimator.update(df)\n        return self\n\n    def fit(self, df: pd.DataFrame, y=None):\n        \"\"\" \n        Compute the quantiles of the dataset\n\n        Parameters:\n            df: The input dataset\n        \"\"\"\n        self.quantile_estimator.update(df)\n        self._compute_quantiles()\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Scale the input dataset\n\n        Parameters:\n            X: The input dataset\n\n        Returns:\n            A new DataFrame with the same index as the input with the\n            data scaled with respect to the quantiles of the fiited dataset\n        \"\"\"\n        if self.Q1 is None:\n            self._compute_quantiles()\n\n\n        new_X = X.copy()\n        X_std = (X.loc[:, self.valid_mask] - self.Q1[self.valid_mask]) / (\n            self.IQR[self.valid_mask]\n        )\n        new_X.loc[:, self.valid_mask] = (\n            X_std * (self.range[1] - self.range[0]) + self.range[0]\n        )\n        new_X.loc[:, ~self.valid_mask] = 0\n        if self.clip:\n            return new_X.clip(self.range[0], self.range[1])\n        else:\n            return new_X\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.RobustMinMaxScaler.fit","title":"<code>fit(df, y=None)</code>","text":"<p>Compute the quantiles of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def fit(self, df: pd.DataFrame, y=None):\n    \"\"\" \n    Compute the quantiles of the dataset\n\n    Parameters:\n        df: The input dataset\n    \"\"\"\n    self.quantile_estimator.update(df)\n    self._compute_quantiles()\n    return self\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.RobustMinMaxScaler.partial_fit","title":"<code>partial_fit(df, y=None)</code>","text":"<p>Compute the quantiles of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def partial_fit(self, df: pd.DataFrame, y=None):\n    \"\"\" \n    Compute the quantiles of the dataset\n\n    Parameters:\n        df: The input dataset\n    \"\"\"\n    self.quantile_estimator.update(df)\n    return self\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.RobustMinMaxScaler.transform","title":"<code>transform(X)</code>","text":"<p>Scale the input dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the</p> <code>DataFrame</code> <p>data scaled with respect to the quantiles of the fiited dataset</p> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Scale the input dataset\n\n    Parameters:\n        X: The input dataset\n\n    Returns:\n        A new DataFrame with the same index as the input with the\n        data scaled with respect to the quantiles of the fiited dataset\n    \"\"\"\n    if self.Q1 is None:\n        self._compute_quantiles()\n\n\n    new_X = X.copy()\n    X_std = (X.loc[:, self.valid_mask] - self.Q1[self.valid_mask]) / (\n        self.IQR[self.valid_mask]\n    )\n    new_X.loc[:, self.valid_mask] = (\n        X_std * (self.range[1] - self.range[0]) + self.range[0]\n    )\n    new_X.loc[:, ~self.valid_mask] = 0\n    if self.clip:\n        return new_X.clip(self.range[0], self.range[1])\n    else:\n        return new_X\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.RobustStandardScaler","title":"<code>RobustStandardScaler</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Scale features using statistics that are robust to outliers.</p> <p>Parameters:</p> Name Type Description Default <code>quantile_range</code> <code>tuple</code> <p>Desired quantile range of transformed data, by defualt (0.25,0.75)</p> <code>(0.25, 0.75)</code> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>class RobustStandardScaler(TransformerStep):\n    \"\"\"\n    Scale features using statistics that are robust to outliers.\n\n    Parameters:\n        quantile_range: Desired quantile range of transformed data, by defualt (0.25,0.75)\n    \"\"\"\n\n    def __init__(self, *, quantile_range: tuple=(0.25, 0.75), prefer_partial_fit:bool = False, **kwargs):\n        super().__init__( **kwargs,prefer_partial_fit=prefer_partial_fit)\n        self.quantile_range = quantile_range\n        self.quantile_estimator = QuantileEstimator()\n        self.IQR = None\n        self.median = None\n\n    def fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Compute the mean of the dataset\n\n        Parameters:\n            X: the input dataset   \n        \"\"\"\n        Q1 = X.quantile(self.quantile_range[0])\n        Q3 = X.quantile(self.quantile_range[1])\n        self.IQR = Q3 - Q1\n        self.median = X.median()\n\n    def partial_fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Compute incrementally the mean of the dataset\n\n        Parameters:\n            X: the input dataset   \n        \"\"\"\n        if X.shape[0] &lt; 2:\n            return self\n\n        self.quantile_estimator.update(X)\n\n        return self\n\n    def _compute_quantiles(self):\n        self.Q1 = self.quantile_estimator.quantile(self.quantile_range[0])\n        self.Q3 = self.quantile_estimator.quantile(self.quantile_range[1])\n\n        self.IQR = self.Q3 - self.Q1\n\n        self.median = self.quantile_estimator.quantile(0.5)\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Center the input life\n\n        Parameters:\n        X: pd.DataFrame\n            The input life\n\n        Returns:\n            A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset\n        \"\"\"\n        if self.IQR is None:\n            self._compute_quantiles()\n        return (X - self.median) / self.IQR\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.RobustStandardScaler.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the mean of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>the input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Compute the mean of the dataset\n\n    Parameters:\n        X: the input dataset   \n    \"\"\"\n    Q1 = X.quantile(self.quantile_range[0])\n    Q3 = X.quantile(self.quantile_range[1])\n    self.IQR = Q3 - Q1\n    self.median = X.median()\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.RobustStandardScaler.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Compute incrementally the mean of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>the input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Compute incrementally the mean of the dataset\n\n    Parameters:\n        X: the input dataset   \n    \"\"\"\n    if X.shape[0] &lt; 2:\n        return self\n\n    self.quantile_estimator.update(X)\n\n    return self\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.RobustStandardScaler.transform","title":"<code>transform(X)</code>","text":"<p>Center the input life</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset</p> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Center the input life\n\n    Parameters:\n    X: pd.DataFrame\n        The input life\n\n    Returns:\n        A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset\n    \"\"\"\n    if self.IQR is None:\n        self._compute_quantiles()\n    return (X - self.median) / self.IQR\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.ScaleInvRUL","title":"<code>ScaleInvRUL</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Scale binary columns according to the inverse of the RUL.Usually this will be used before a CumSum transformation</p> <p>Parameters:</p> Name Type Description Default <code>rul_column</code> <code>str</code> <p>Column with the RUL</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>class ScaleInvRUL(TransformerStep):\n    \"\"\"\n    Scale binary columns according to the inverse of the RUL.Usually this will be used before a CumSum transformation\n\n    Parameters:\n        rul_column: Column with the RUL\n    \"\"\"\n\n    def __init__(self, *,rul_column: str, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.RUL_list_per_column = {}\n        self.penalty = {}\n        self.rul_column_in = rul_column\n        self.rul_column = None\n\n    def partial_fit(self, X: pd.DataFrame):\n        \"\"\"\n        Fit the scaler\n\n        Parameters:\n            X: The input dataset\n        \"\"\"\n        if self.rul_column is None:\n            self.rul_column = self.column_name(X, self.rul_column_in)\n        columns = [c for c in X.columns if c != self.rul_column]\n        for c in columns:\n            mask = X[X[c] &gt; 0].index\n            if len(mask) &gt; 0:\n                RUL_list = self.RUL_list_per_column.setdefault(c, [])\n                RUL_list.extend(\n                    (\n                        1\n                        + (\n                            X[self.rul_column].loc[mask].values\n                            / X[self.rul_column].max()\n                        )\n                    ).tolist()\n                )\n\n        for k in self.RUL_list_per_column.keys():\n\n            self.penalty[k] = 1 / np.median(self.RUL_list_per_column[k])\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Scale the input dataset\n\n        Parameters:\n            X: The input dataset\n\n        Returns:\n            A new DataFrame with the same index as the input with the data scaled with respect to the RUL\n        \"\"\"\n        columns = [c for c in X.columns if c != self.rul_column]\n        X_new = pd.DataFrame(index=X.index)\n        for c in columns:\n            if c in self.penalty:\n                X_new[c] = X[c] * self.penalty[c]\n        return X_new\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.ScaleInvRUL.partial_fit","title":"<code>partial_fit(X)</code>","text":"<p>Fit the scaler</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame):\n    \"\"\"\n    Fit the scaler\n\n    Parameters:\n        X: The input dataset\n    \"\"\"\n    if self.rul_column is None:\n        self.rul_column = self.column_name(X, self.rul_column_in)\n    columns = [c for c in X.columns if c != self.rul_column]\n    for c in columns:\n        mask = X[X[c] &gt; 0].index\n        if len(mask) &gt; 0:\n            RUL_list = self.RUL_list_per_column.setdefault(c, [])\n            RUL_list.extend(\n                (\n                    1\n                    + (\n                        X[self.rul_column].loc[mask].values\n                        / X[self.rul_column].max()\n                    )\n                ).tolist()\n            )\n\n    for k in self.RUL_list_per_column.keys():\n\n        self.penalty[k] = 1 / np.median(self.RUL_list_per_column[k])\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.ScaleInvRUL.transform","title":"<code>transform(X)</code>","text":"<p>Scale the input dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the data scaled with respect to the RUL</p> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Scale the input dataset\n\n    Parameters:\n        X: The input dataset\n\n    Returns:\n        A new DataFrame with the same index as the input with the data scaled with respect to the RUL\n    \"\"\"\n    columns = [c for c in X.columns if c != self.rul_column]\n    X_new = pd.DataFrame(index=X.index)\n    for c in columns:\n        if c in self.penalty:\n            X_new[c] = X[c] * self.penalty[c]\n    return X_new\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.StandardScaler","title":"<code>StandardScaler</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Standardize features by removing the mean and scaling to unit variance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>class StandardScaler(TransformerStep):\n    \"\"\"\n    Standardize features by removing the mean and scaling to unit variance.\n\n    Parameters:\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(self, *, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.std = None\n        self.mean = None\n\n    def partial_fit(self, df: pd.DataFrame, y=None):\n        \"\"\"\n        Compute mean and std of the dataset\n\n        Parameters:\n            df: The input dataset\n        \"\"\"\n        if df.shape[0] &lt; 15:\n            return self\n        partial_data_mean = df.mean()\n        partial_data_std = df.std()\n        if self.mean is None:\n            self.mean = partial_data_mean\n            self.std = partial_data_std\n        else:\n            self.mean = pd.concat([self.mean, partial_data_mean], axis=1).mean(axis=1)\n            self.std = pd.concat([self.std, partial_data_std], axis=1).mean(axis=1)\n        return self\n\n    def fit(self, df: pd.DataFrame, y=None):\n        \"\"\"\n        Compute mean and std of the dataset\n\n        Parameters:\n            df: The input dataset\n        \"\"\"\n        self.mean = df.mean()\n        self.std = df.std()\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Scale the input dataset\n\n        Parameters:\n            X: The input dataset\n\n        Returns:\n            A new DataFrame with the same index as the input with the data scaled to have null mean and unit variance\n        \"\"\"\n        return (X - self.mean) / (self.std)\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.StandardScaler.fit","title":"<code>fit(df, y=None)</code>","text":"<p>Compute mean and std of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def fit(self, df: pd.DataFrame, y=None):\n    \"\"\"\n    Compute mean and std of the dataset\n\n    Parameters:\n        df: The input dataset\n    \"\"\"\n    self.mean = df.mean()\n    self.std = df.std()\n    return self\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.StandardScaler.partial_fit","title":"<code>partial_fit(df, y=None)</code>","text":"<p>Compute mean and std of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def partial_fit(self, df: pd.DataFrame, y=None):\n    \"\"\"\n    Compute mean and std of the dataset\n\n    Parameters:\n        df: The input dataset\n    \"\"\"\n    if df.shape[0] &lt; 15:\n        return self\n    partial_data_mean = df.mean()\n    partial_data_std = df.std()\n    if self.mean is None:\n        self.mean = partial_data_mean\n        self.std = partial_data_std\n    else:\n        self.mean = pd.concat([self.mean, partial_data_mean], axis=1).mean(axis=1)\n        self.std = pd.concat([self.std, partial_data_std], axis=1).mean(axis=1)\n    return self\n</code></pre>"},{"location":"transformation/features/scalers/#ceruleo.transformation.features.scalers.StandardScaler.transform","title":"<code>transform(X)</code>","text":"<p>Scale the input dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the data scaled to have null mean and unit variance</p> Source code in <code>ceruleo/transformation/features/scalers.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Scale the input dataset\n\n    Parameters:\n        X: The input dataset\n\n    Returns:\n        A new DataFrame with the same index as the input with the data scaled to have null mean and unit variance\n    \"\"\"\n    return (X - self.mean) / (self.std)\n</code></pre>"},{"location":"transformation/features/selection/","title":"Selectors","text":""},{"location":"transformation/features/selection/#selectors","title":"Selectors","text":""},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.ByNameFeatureSelector","title":"<code>ByNameFeatureSelector</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Select a subset of feature by name</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Union[str, List[str]]</code> <p>Feature name or List of features name to select</p> <code>[]</code> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>class ByNameFeatureSelector(TransformerStep):\n    \"\"\"Select a subset of feature by name\n\n    Parameters:\n            features: Feature name or List of features name to select\n    \"\"\"\n    def __init__(self, *, features:Union[str, List[str]]= [], name: Optional[str] = None):\n        super().__init__(name=name)\n        if isinstance(features, str):\n            features = [features]\n        self.features = features\n        self.features_indices = None\n        self.features_computed_ = []\n\n    def partial_fit(self, df, y=None):\n        if len(self.features) &gt; 0:\n            features = [f for f in self.features if f in set(df.columns)]\n        else:\n            features = list(set(df.columns))\n\n        if len(self.features_computed_) == 0:\n            self.features_computed_ = features\n        else:\n            self.features_computed_ = [\n                f for f in self.features_computed_ if f in features\n            ]\n        return self\n\n    def fit(self, df:pd.DataFrame, y=None):\n        \"\"\" \n        Find the indices of the features to select\n\n        Parameters:\n            df: DataFrame containing the input life\n        \"\"\"\n        if len(self.features) &gt; 0:\n            features = [f for f in self.features if f in set(df.columns)]\n        else:\n            features = list(set(df.columns))\n        self.features_computed_ = sorted(features)\n        return self\n        return X.loc[:, self.features_computed_].copy()\n\n    def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new DataFrame containing only the selected features\n        \"\"\"\n        return X.loc[:, self.features_computed_].copy()\n\n    @property\n    def n_features(self):\n        return len(self.features_computed_)\n\n    def description(self):\n        name = super().description()\n        return (name, self.features_computed_)\n\n    def __str__(self):\n        name, f = self.description()\n        features = ', '.join(f)[:10]\n        return f'{name} : [{features}]'\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.ByNameFeatureSelector.fit","title":"<code>fit(df, y=None)</code>","text":"<p>Find the indices of the features to select</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the input life</p> required Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def fit(self, df:pd.DataFrame, y=None):\n    \"\"\" \n    Find the indices of the features to select\n\n    Parameters:\n        df: DataFrame containing the input life\n    \"\"\"\n    if len(self.features) &gt; 0:\n        features = [f for f in self.features if f in set(df.columns)]\n    else:\n        features = list(set(df.columns))\n    self.features_computed_ = sorted(features)\n    return self\n    return X.loc[:, self.features_computed_].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.ByNameFeatureSelector.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame containing only the selected features</p> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new DataFrame containing only the selected features\n    \"\"\"\n    return X.loc[:, self.features_computed_].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.ByTypeFeatureSelector","title":"<code>ByTypeFeatureSelector</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Select a subset of feature by type</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>Union[str, List]</code> <p>Data type to be selected, by default []</p> <code>[]</code> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>class ByTypeFeatureSelector(TransformerStep):\n    \"\"\"Select a subset of feature by type\n\n    Parameters:\n            type_: Data type to be selected, by default []\n    \"\"\"\n    def __init__(self, *, type_:Union[str, List]= [], name: Optional[str] = None):\n        super().__init__(name=name)\n\n        self.features = []\n        self.type = type_\n\n    def partial_fit(self, df, y=None):\n        if len(self.features) == 0:            \n            self.features = df.select_dtypes(include=self.type).columns      \n        return self\n\n    def fit(self, df, y=None):\n        if len(self.features) == 0:\n            self.features = df.select_dtypes(include=self.type).columns\n\n        return self\n\n    def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new DataFrame containing only the features of the selected type\n        \"\"\"\n        return X.loc[:, self.features].copy()\n\n    @property\n    def n_features(self):\n        return len(self.features)\n\n    def description(self):\n        name = super().description()\n        return (name, self.features)\n\n    def __str__(self):\n        name, f = self.description()\n        features = ', '.join(f)[:10]\n        return f'{name} : [{features}]'\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.ByTypeFeatureSelector.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame containing only the features of the selected type</p> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new DataFrame containing only the features of the selected type\n    \"\"\"\n    return X.loc[:, self.features].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.DiscardByNameFeatureSelector","title":"<code>DiscardByNameFeatureSelector</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Remove a list of features from the input life</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List</code> <p>List of features to discard</p> <code>[]</code> <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>class DiscardByNameFeatureSelector(TransformerStep):\n    \"\"\"\n    Remove a list of features from the input life\n\n    Parameters:\n        features: List of features to discard\n        name: Name of the step, by default None\n    \"\"\"\n    def __init__(self, *, features: List=[], name: Optional[str] = None):\n        super().__init__(name=name)\n        self.features = features\n        self.features_indices = None\n\n    def fit(self, df:pd.DataFrame, y=None):\n        \"\"\"\n        Find the indices of the features to discard\n\n        Parameters:\n            df: DataFrame containing the set of features to discard\n        \"\"\"\n        self.feature_columns = [f for f in df.columns if f not in self.features]\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new DataFrame containing only the features not in the list of features to discard\n        \"\"\"\n        return X.loc[:, self.feature_columns]\n\n    @property\n    def n_features(self):\n        return len(self.features)\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.DiscardByNameFeatureSelector.fit","title":"<code>fit(df, y=None)</code>","text":"<p>Find the indices of the features to discard</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the set of features to discard</p> required Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def fit(self, df:pd.DataFrame, y=None):\n    \"\"\"\n    Find the indices of the features to discard\n\n    Parameters:\n        df: DataFrame containing the set of features to discard\n    \"\"\"\n    self.feature_columns = [f for f in df.columns if f not in self.features]\n    return self\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.DiscardByNameFeatureSelector.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame containing only the features not in the list of features to discard</p> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new DataFrame containing only the features not in the list of features to discard\n    \"\"\"\n    return X.loc[:, self.feature_columns]\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.MatchFeatureSelector","title":"<code>MatchFeatureSelector</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Select all the features that match a pattern</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Pattern to match</p> required Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>class  MatchFeatureSelector(TransformerStep):\n    \"\"\"Select all the features that match a pattern\n\n    Parameters:\n        pattern: Pattern to match\n    \"\"\"\n    def __init__(self, *, pattern:str, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.pattern = pattern\n        self.selected_columns_ = None\n\n\n    def partial_fit(self, df: pd.DataFrame, y=None):\n\n        \"\"\" \n        Find the features matching the pattern\n\n        Parameters:\n            df: DataFrame containing the entire set of features \n        \"\"\"\n\n        if self.selected_columns_ is None:\n            self.selected_columns_ = [f for f in df.columns if self.pattern in f ]\n\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new life with the same index as the input with the missing values replaced by the value in the succesive timestamp \n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input array must be a data frame\")\n        return X[self.selected_columns_].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.MatchFeatureSelector.partial_fit","title":"<code>partial_fit(df, y=None)</code>","text":"<p>Find the features matching the pattern</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the entire set of features</p> required Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def partial_fit(self, df: pd.DataFrame, y=None):\n\n    \"\"\" \n    Find the features matching the pattern\n\n    Parameters:\n        df: DataFrame containing the entire set of features \n    \"\"\"\n\n    if self.selected_columns_ is None:\n        self.selected_columns_ = [f for f in df.columns if self.pattern in f ]\n\n    return self\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.MatchFeatureSelector.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new life with the same index as the input with the missing values replaced by the value in the succesive timestamp</p> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new life with the same index as the input with the missing values replaced by the value in the succesive timestamp \n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input array must be a data frame\")\n    return X[self.selected_columns_].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.NullProportionSelector","title":"<code>NullProportionSelector</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Remove features with null proportion higher than a threshold inserted in input</p> <p>Parameters:</p> Name Type Description Default <code>max_null_proportion</code> <code>float</code> <p>Maximum null proportion threshold</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>class NullProportionSelector(TransformerStep):\n    \"\"\" \n    Remove features with null proportion higher than a threshold inserted in input\n\n    Parameters:\n        max_null_proportion: Maximum null proportion threshold\n        name: Name of the step, by default None\n    \"\"\"\n    def __init__(self, *, max_null_proportion: float, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.max_null_proportion = max_null_proportion\n        self.selected_columns_ = None\n\n    def partial_fit(self, X:pd.DataFrame, y=None):\n        \"\"\" \n        Find the indexes of the features with null proportion lower than the threshold\n\n        Parameters:\n            X: DataFrame containing the input life\n        \"\"\"\n        null_proportion = X.isnull().mean()\n\n        partial_selected_columns_ = X.columns[\n            null_proportion &lt; self.max_null_proportion\n        ]\n        if (\n            self.selected_columns_ is not None\n            and len(partial_selected_columns_) &lt; len(self.selected_columns_) * 0.5\n        ):\n            logger.warning(type(self).__name__)\n\n        if self.selected_columns_ is None:\n            self.selected_columns_ = partial_selected_columns_\n        else:\n            self.selected_columns_ = [\n                f for f in self.selected_columns_ if f in partial_selected_columns_\n            ]\n        if len(self.selected_columns_) == 0:\n            logger.warning(type(self).__name__)\n            logger.warning(\"All features were removed\")\n        return self\n\n    def fit(self, X:pd.DataFrame, y=None):\n        \"\"\"\n        Find the indexes of the features with null proportion lower than the threshold\n\n        Parameters:\n            X: DataFrame containing the input life\n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input array must be a data frame\")\n        self.null_proportion = X.isnull().mean()\n        self.selected_columns_ = X.columns[\n            self.null_proportion &lt; self.max_null_proportion\n        ]\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new life containing only the features with null proportion lower than the threshold\n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input array must be a data frame\")\n        return X[self.selected_columns_].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.NullProportionSelector.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Find the indexes of the features with null proportion lower than the threshold</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>DataFrame containing the input life</p> required Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def fit(self, X:pd.DataFrame, y=None):\n    \"\"\"\n    Find the indexes of the features with null proportion lower than the threshold\n\n    Parameters:\n        X: DataFrame containing the input life\n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input array must be a data frame\")\n    self.null_proportion = X.isnull().mean()\n    self.selected_columns_ = X.columns[\n        self.null_proportion &lt; self.max_null_proportion\n    ]\n    return self\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.NullProportionSelector.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Find the indexes of the features with null proportion lower than the threshold</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>DataFrame containing the input life</p> required Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def partial_fit(self, X:pd.DataFrame, y=None):\n    \"\"\" \n    Find the indexes of the features with null proportion lower than the threshold\n\n    Parameters:\n        X: DataFrame containing the input life\n    \"\"\"\n    null_proportion = X.isnull().mean()\n\n    partial_selected_columns_ = X.columns[\n        null_proportion &lt; self.max_null_proportion\n    ]\n    if (\n        self.selected_columns_ is not None\n        and len(partial_selected_columns_) &lt; len(self.selected_columns_) * 0.5\n    ):\n        logger.warning(type(self).__name__)\n\n    if self.selected_columns_ is None:\n        self.selected_columns_ = partial_selected_columns_\n    else:\n        self.selected_columns_ = [\n            f for f in self.selected_columns_ if f in partial_selected_columns_\n        ]\n    if len(self.selected_columns_) == 0:\n        logger.warning(type(self).__name__)\n        logger.warning(\"All features were removed\")\n    return self\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.NullProportionSelector.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new life containing only the features with null proportion lower than the threshold</p> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new life containing only the features with null proportion lower than the threshold\n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input array must be a data frame\")\n    return X[self.selected_columns_].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.PandasVarianceThreshold","title":"<code>PandasVarianceThreshold</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Remove features with variance lower than a variance threshold inserted in input</p> <p>Parameters:</p> Name Type Description Default <code>min_variance</code> <code>float</code> <p>Minimum variance threshold</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>class PandasVarianceThreshold(TransformerStep):\n    \"\"\" \n    Remove features with variance lower than a variance threshold inserted in input\n\n    Parameters:\n        min_variance: Minimum variance threshold\n        name: Name of the step, by default None\n    \"\"\"\n    def __init__(self, *, min_variance: float, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.min_variance = min_variance\n        self.selected_columns_ = None\n\n    def partial_fit(self, X:pd.DataFrame, y=None):\n        \"\"\" \n        Find the indexes of the features with variance higher than the threshold\n\n        Parameters:\n            X: DataFrame containing the input life\n        \"\"\"\n        variances_ = X.var(skipna=True)\n        partial_selected_columns_ = X.columns[variances_ &gt; self.min_variance]\n        if (\n            self.selected_columns_ is not None\n            and len(partial_selected_columns_) &lt; len(self.selected_columns_) * 0.5\n        ):\n            logger.warning(type(self).__name__)\n            logger.warning(\n                f\"Life removed more than a half of the columns. Shape {X.shape}\"\n            )\n            logger.warning(\n                f\"Current: {len(self.selected_columns_)}. New ones: {len(partial_selected_columns_)}\"\n            )\n        if self.selected_columns_ is None:\n            self.selected_columns_ = partial_selected_columns_\n        else:\n            self.selected_columns_ = [\n                f for f in self.selected_columns_ if f in partial_selected_columns_\n            ]\n        if len(self.selected_columns_) == 0:\n            logger.warning(type(self).__name__)\n            logger.warning(\"All features were removed\")\n        return self\n\n    def fit(self, X:pd.DataFrame, y=None):\n        \"\"\" \n        Find the indexes of the features with variance higher than the threshold\n\n        Parameters:\n            X: DataFrame containing the input life\n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input array must be a data frame\")\n        self.variances_ = X.var(skipna=True)\n        self.selected_columns_ = X.columns[self.variances_ &gt; self.min_variance]\n        logger.debug(\n            f\"Dropped columns {[c for c in X.columns if c not in self.selected_columns_]}\"\n        )\n        return self\n\n    def transform(self, X:pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\" \n        Transform the input life\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new life containing only the features with variance higher than the threshold\n        \"\"\"\n        if not isinstance(X, pd.DataFrame):\n            raise ValueError(\"Input array must be a data frame\")\n        return X[self.selected_columns_].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.PandasVarianceThreshold.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Find the indexes of the features with variance higher than the threshold</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>DataFrame containing the input life</p> required Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def fit(self, X:pd.DataFrame, y=None):\n    \"\"\" \n    Find the indexes of the features with variance higher than the threshold\n\n    Parameters:\n        X: DataFrame containing the input life\n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input array must be a data frame\")\n    self.variances_ = X.var(skipna=True)\n    self.selected_columns_ = X.columns[self.variances_ &gt; self.min_variance]\n    logger.debug(\n        f\"Dropped columns {[c for c in X.columns if c not in self.selected_columns_]}\"\n    )\n    return self\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.PandasVarianceThreshold.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Find the indexes of the features with variance higher than the threshold</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>DataFrame containing the input life</p> required Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def partial_fit(self, X:pd.DataFrame, y=None):\n    \"\"\" \n    Find the indexes of the features with variance higher than the threshold\n\n    Parameters:\n        X: DataFrame containing the input life\n    \"\"\"\n    variances_ = X.var(skipna=True)\n    partial_selected_columns_ = X.columns[variances_ &gt; self.min_variance]\n    if (\n        self.selected_columns_ is not None\n        and len(partial_selected_columns_) &lt; len(self.selected_columns_) * 0.5\n    ):\n        logger.warning(type(self).__name__)\n        logger.warning(\n            f\"Life removed more than a half of the columns. Shape {X.shape}\"\n        )\n        logger.warning(\n            f\"Current: {len(self.selected_columns_)}. New ones: {len(partial_selected_columns_)}\"\n        )\n    if self.selected_columns_ is None:\n        self.selected_columns_ = partial_selected_columns_\n    else:\n        self.selected_columns_ = [\n            f for f in self.selected_columns_ if f in partial_selected_columns_\n        ]\n    if len(self.selected_columns_) == 0:\n        logger.warning(type(self).__name__)\n        logger.warning(\"All features were removed\")\n    return self\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.PandasVarianceThreshold.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new life containing only the features with variance higher than the threshold</p> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def transform(self, X:pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\" \n    Transform the input life\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new life containing only the features with variance higher than the threshold\n    \"\"\"\n    if not isinstance(X, pd.DataFrame):\n        raise ValueError(\"Input array must be a data frame\")\n    return X[self.selected_columns_].copy()\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.PositionFeatures","title":"<code>PositionFeatures</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Reorder the features of the input life</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>dict</code> <p>Dictionary containing the features to reorder and their new position</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>class PositionFeatures(TransformerStep):\n    \"\"\"\n    Reorder the features of the input life\n\n    Parameters:\n        features: Dictionary containing the features to reorder and their new position\n        name: Name of the step, by default None\n    \"\"\"\n    def __init__(self, *, features: dict, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.features = features\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Transform the input life by reordering the features\n\n        Parameters:\n            X: The input life to be transformed\n\n        Returns:\n            A new DataFrame containing the features in the order specified in the constructor\n        \"\"\"\n        cols = list(X.columns)\n        for name, pos in self.features.items():\n            a, b = cols.index(name), pos\n            cols[b], cols[a] = cols[a], cols[b]\n            X = X[cols]\n        return X\n</code></pre>"},{"location":"transformation/features/selection/#ceruleo.transformation.features.selection.PositionFeatures.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life by reordering the features</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame containing the features in the order specified in the constructor</p> Source code in <code>ceruleo/transformation/features/selection.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Transform the input life by reordering the features\n\n    Parameters:\n        X: The input life to be transformed\n\n    Returns:\n        A new DataFrame containing the features in the order specified in the constructor\n    \"\"\"\n    cols = list(X.columns)\n    for name, pos in self.features.items():\n        a, b = cols.index(name), pos\n        cols[b], cols[a] = cols[a], cols[b]\n        X = X[cols]\n    return X\n</code></pre>"},{"location":"transformation/features/slicing/","title":"Slicing","text":""},{"location":"transformation/features/slicing/#slicing","title":"Slicing","text":""},{"location":"transformation/features/slicing/#ceruleo.transformation.features.slicing.SliceRows","title":"<code>SliceRows</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Slice portion of the run-to-failure cycle</p> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>Union[int, RelativePosition]</code> <p>Initial position of the slice, default RelativeToStart(0)</p> <code>RelativeToStart(0)</code> <code>final</code> <code>Union[int, RelativePosition]</code> <p>Final position of the slice, default RelativeToEnd(0)</p> <code>RelativeToEnd(0)</code> Source code in <code>ceruleo/transformation/features/slicing.py</code> <pre><code>class SliceRows(TransformerStep):\n    \"\"\"\n    Slice portion of the run-to-failure cycle\n\n    Parameters:\n        initial: Initial position of the slice, default RelativeToStart(0)\n        final: Final position of the slice, default RelativeToEnd(0)\n    \"\"\"\n\n    def __init__(\n        self,\n        initial: Union[int, RelativePosition] = RelativeToStart(0),\n        final: Union[int, RelativePosition] = RelativeToEnd(0),\n        *args,\n        **kwargs\n    ):\n\n        super().__init__(*args, **kwargs)\n\n        self.initial = initial\n        self.final = final\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Slice the run-to-failure cycle\n\n        Parameters:\n            X: Input dataframe\n\n        Returns:\n            A dataframe with the sliced run-to-failure cycle\n        \"\"\"\n        if isinstance(self.initial, RelativePosition):\n            initial = self.initial.get(X.shape[0])\n        else:\n            initial = self.initial\n        if isinstance(self.final, RelativePosition):\n            final = self.final.get(X.shape[0])\n        else:\n            final = self.final\n\n        return X.iloc[initial:final, :].copy()\n</code></pre>"},{"location":"transformation/features/slicing/#ceruleo.transformation.features.slicing.SliceRows.transform","title":"<code>transform(X)</code>","text":"<p>Slice the run-to-failure cycle</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input dataframe</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the sliced run-to-failure cycle</p> Source code in <code>ceruleo/transformation/features/slicing.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Slice the run-to-failure cycle\n\n    Parameters:\n        X: Input dataframe\n\n    Returns:\n        A dataframe with the sliced run-to-failure cycle\n    \"\"\"\n    if isinstance(self.initial, RelativePosition):\n        initial = self.initial.get(X.shape[0])\n    else:\n        initial = self.initial\n    if isinstance(self.final, RelativePosition):\n        final = self.final.get(X.shape[0])\n    else:\n        final = self.final\n\n    return X.iloc[initial:final, :].copy()\n</code></pre>"},{"location":"transformation/features/split/","title":"Split","text":""},{"location":"transformation/features/split/#split","title":"Split","text":""},{"location":"transformation/features/split/#ceruleo.transformation.features.split.Filter","title":"<code>Filter</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Filter rows of a dataframe based on a query</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List[Any]</code> <p>Values to filter by</p> required <code>columns</code> <code>Union[List[str], str]</code> <p>Columns to filter by</p> required Source code in <code>ceruleo/transformation/features/split.py</code> <pre><code>class Filter(TransformerStep):\n    \"\"\"\n    Filter rows of a dataframe based on a query\n\n    Parameters:\n        values: Values to filter by\n        columns: Columns to filter by\n    \"\"\"\n    def __init__(\n        self,\n        *,\n        values: List[Any],\n        columns: Union[List[str], str],\n        name: Optional[str] = None,\n    ):\n        def prepare_value(v):\n            if isinstance(v, str):\n                return f\"'{v}'\"\n            else:\n                return v\n\n        super().__init__(name=name)\n        self.values = values\n        self.columns = columns\n        self.query = \" &amp; \".join(\n            [f\"({c} == {prepare_value(v)})\" for c, v in zip(self.columns, self.values)]\n        )\n\n    def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Filter the dataframe\n\n        Parameters:\n            X: Input dataframe\n\n        Returns:\n            A dataframe with the filtered rows\n        \"\"\"\n        if self.values == [\"__category_all__\"]:\n            return X.drop(columns=self.columns)\n        else:\n            return X.query(self.query).drop(columns=self.columns)\n</code></pre>"},{"location":"transformation/features/split/#ceruleo.transformation.features.split.Filter.transform","title":"<code>transform(X)</code>","text":"<p>Filter the dataframe</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input dataframe</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the filtered rows</p> Source code in <code>ceruleo/transformation/features/split.py</code> <pre><code>def transform(self, X:pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Filter the dataframe\n\n    Parameters:\n        X: Input dataframe\n\n    Returns:\n        A dataframe with the filtered rows\n    \"\"\"\n    if self.values == [\"__category_all__\"]:\n        return X.drop(columns=self.columns)\n    else:\n        return X.query(self.query).drop(columns=self.columns)\n</code></pre>"},{"location":"transformation/features/split/#ceruleo.transformation.features.split.Joiner","title":"<code>Joiner</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Join multiple run-to-failure cycles into a single DataFrame</p> Source code in <code>ceruleo/transformation/features/split.py</code> <pre><code>class Joiner(TransformerStep):\n    \"\"\"\n    Join multiple run-to-failure cycles into a single DataFrame\n    \"\"\"\n    def transform(self, X: List[pd.DataFrame]) -&gt; pd.DataFrame:\n        \"\"\"\n        Join the input lifes\n\n        Parameters:\n            X: List of run-to-failure cycles to join\n\n        Returns:\n            A dataframe with the joined run-to-failure cycles\n        \"\"\"\n        if isinstance(X, list):\n            X_default = X[0]\n            X_q = pd.concat(X[1:])\n            missing_indices = X_default.index.difference(X_q.index)\n            X_q = pd.concat((X_q, X_default.loc[missing_indices, :])).sort_index()\n            return X_q\n        else:\n            return X\n</code></pre>"},{"location":"transformation/features/split/#ceruleo.transformation.features.split.Joiner.transform","title":"<code>transform(X)</code>","text":"<p>Join the input lifes</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>List[DataFrame]</code> <p>List of run-to-failure cycles to join</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A dataframe with the joined run-to-failure cycles</p> Source code in <code>ceruleo/transformation/features/split.py</code> <pre><code>def transform(self, X: List[pd.DataFrame]) -&gt; pd.DataFrame:\n    \"\"\"\n    Join the input lifes\n\n    Parameters:\n        X: List of run-to-failure cycles to join\n\n    Returns:\n        A dataframe with the joined run-to-failure cycles\n    \"\"\"\n    if isinstance(X, list):\n        X_default = X[0]\n        X_q = pd.concat(X[1:])\n        missing_indices = X_default.index.difference(X_q.index)\n        X_q = pd.concat((X_q, X_default.loc[missing_indices, :])).sort_index()\n        return X_q\n    else:\n        return X\n</code></pre>"},{"location":"transformation/features/transformation/","title":"Global Transformations","text":""},{"location":"transformation/features/transformation/#global-transformations","title":"Global Transformations","text":""},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Accumulate","title":"<code>Accumulate</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the accumulated sum of each feature.</p> <p>This is useful to compute the count of binary features.</p> <p>Parameters:</p> Name Type Description Default <code>normalize</code> <code>bool</code> <p>Weather to apply the normalization or not, by default False</p> <code>False</code> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class Accumulate(TransformerStep):\n    \"\"\"\n    Compute the accumulated sum of each feature.\n\n    This is useful to compute the count of binary features.\n\n    Parameters:\n        normalize: Weather to apply the normalization or not, by default False\n    \"\"\" \n\n    def __init__(self, *, normalize: bool = False,  name: Optional[str] = None):\n        super().__init__(name=name)\n        self.normalize = normalize\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life computing the cumulated sum\n\n        Parameters:\n            X: The Input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the cumulated sum of the features\n        \"\"\"\n        X1 = X.cumsum()\n        if self.normalize:\n            return X1 / X1.abs().apply(np.sqrt)\n        else:\n            return X1\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Accumulate.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life computing the cumulated sum</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the cumulated sum of the features</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life computing the cumulated sum\n\n    Parameters:\n        X: The Input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the cumulated sum of the features\n    \"\"\"\n    X1 = X.cumsum()\n    if self.normalize:\n        return X1 / X1.abs().apply(np.sqrt)\n    else:\n        return X1\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Apply","title":"<code>Apply</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Apply the input function element-wise</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class Apply(TransformerStep):\n    \"\"\"Apply the input function element-wise\"\"\"\n\n    def __init__(self, *, fun, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.fun = fun\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life applying the input function to each element\n\n        Parameters\n            X: The Input life\n\n        Returns:\n            Return a new DataFrame with results of the function application to each element.\n        \"\"\"\n        return X.apply(self.fun)\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Apply.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life applying the input function to each element</p> <p>Parameters     X: The Input life</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with results of the function application to each element.</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life applying the input function to each element\n\n    Parameters\n        X: The Input life\n\n    Returns:\n        Return a new DataFrame with results of the function application to each element.\n    \"\"\"\n    return X.apply(self.fun)\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Clip","title":"<code>Clip</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Clip values onto a predefined range</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>float</code> <p>The lower value</p> required <code>upper</code> <code>float</code> <p>The Upper value</p> required Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class Clip(TransformerStep):\n    \"\"\"\n    Clip values onto a predefined range\n\n    Parameters:\n        lower: The lower value\n        upper: The Upper value\n    \"\"\"\n\n    def __init__(self, *, lower: float, upper: float, **kwargs):\n        super().__init__(**kwargs)\n        self.lower = lower\n        self.upper = upper\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life clipping the values onto a predefined range\n\n        Parameters:\n            X: The Input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the clipped values\n        \"\"\"\n        return X.clip(self.lower, self.upper)\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Clip.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life clipping the values onto a predefined range</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the clipped values</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life clipping the values onto a predefined range\n\n    Parameters:\n        X: The Input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the clipped values\n    \"\"\"\n    return X.clip(self.lower, self.upper)\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Diff","title":"<code>Diff</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the 1 step difference of each feature.</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class Diff(TransformerStep):\n    \"\"\"Compute the 1 step difference of each feature.\"\"\"\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life computing the 1 step difference\n\n        Parameters:\n            X: The Input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the 1 step difference of the features\n        \"\"\"\n        return X.diff()\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Diff.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life computing the 1 step difference</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the 1 step difference of the features</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life computing the 1 step difference\n\n    Parameters:\n        X: The Input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the 1 step difference of the features\n    \"\"\"\n    return X.diff()\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.ExpandingCentering","title":"<code>ExpandingCentering</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Center the life using an expanding window</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class ExpandingCentering(TransformerStep):\n    \"\"\"\n    Center the life using an expanding window\n    \"\"\"\n\n    #.. raw:: html\n    #&lt;p&gt;Formula: \\(X - X.expanding().mean())&lt;/p&gt;\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the live centering it using an expanding window\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the data centered\n        \"\"\"\n        return X - X.expanding().mean()\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.ExpandingCentering.transform","title":"<code>transform(X)</code>","text":"<p>Transform the live centering it using an expanding window</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the data centered</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the live centering it using an expanding window\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the data centered\n    \"\"\"\n    return X - X.expanding().mean()\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.ExpandingNormalization","title":"<code>ExpandingNormalization</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Normalize the life features using an expanding window</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class ExpandingNormalization(TransformerStep):\n    \"\"\"Normalize the life features using an expanding window\n    \"\"\"\n\n    #.. highlight:: python\n    #.. code-block:: python\n\n    #(X - X.expanding().mean()) / (X.expanding().std())\n\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the live normalized it using an expanding window\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the data normalized\n        \"\"\"\n        return (X - X.expanding().mean()) / (X.expanding().std())\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.ExpandingNormalization.transform","title":"<code>transform(X)</code>","text":"<p>Transform the live normalized it using an expanding window</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the data normalized</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the live normalized it using an expanding window\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the data normalized\n    \"\"\"\n    return (X - X.expanding().mean()) / (X.expanding().std())\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.MeanCentering","title":"<code>MeanCentering</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Center the data with respect to the mean</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class MeanCentering(TransformerStep):\n    \"\"\"Center the data with respect to the mean\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.N = 0\n        self.sum = None\n\n    def fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Compute the mean of the dataset\n\n        Parameters:\n            X: The input dataset\n        \"\"\"\n        self.mean = X.mean()\n        return self\n\n    def partial_fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Compute incrementally the mean of the dataset\n\n        Parameters:\n            X: The input life\n        \"\"\"\n        if self.sum is None:\n            self.sum = X.sum()\n        else:\n            self.sum += X.sum()\n\n        self.N += X.shape[0]\n        self.mean = self.sum / self.N\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Center the input life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset\n        \"\"\"\n        return X - self.mean\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.MeanCentering.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the mean of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Compute the mean of the dataset\n\n    Parameters:\n        X: The input dataset\n    \"\"\"\n    self.mean = X.mean()\n    return self\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.MeanCentering.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Compute incrementally the mean of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Compute incrementally the mean of the dataset\n\n    Parameters:\n        X: The input life\n    \"\"\"\n    if self.sum is None:\n        self.sum = X.sum()\n    else:\n        self.sum += X.sum()\n\n    self.N += X.shape[0]\n    self.mean = self.sum / self.N\n    return self\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.MeanCentering.transform","title":"<code>transform(X)</code>","text":"<p>Center the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Center the input life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset\n    \"\"\"\n    return X - self.mean\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.MedianCentering","title":"<code>MedianCentering</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Center the data with respect to the median</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class MedianCentering(TransformerStep):\n    \"\"\"Center the data with respect to the median\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.tdigest_dict = None\n        self.median = None\n\n    def fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Compute the median of the dataset\n\n        Parameters:\n            X: The input dataset\n        \"\"\"\n        self.median = X.median()\n        return self\n\n    def partial_fit(self, X: pd.DataFrame, y=None):\n        \"\"\"\n        Compute incrementally the median of the dataset\n\n        Parameters:\n            X: The input life\n        \"\"\"\n        if X.shape[0] &lt; 2:\n            return self\n\n        if self.tdigest_dict is None:\n            self.tdigest_dict = {c: TDigest(100) for c in X.columns}\n        for c in X.columns:\n            self.tdigest_dict[c] = self.tdigest_dict[c].merge_unsorted(X[c].values)\n\n        self.median = pd.Series(\n            {\n                c: self.tdigest_dict[c].estimate_quantile(0.5)\n                for c in self.tdigest_dict.keys()\n            }\n        )\n        return self\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Center the input life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset\n        \"\"\"\n        return X - self.median\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.MedianCentering.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the median of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataset</p> required Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Compute the median of the dataset\n\n    Parameters:\n        X: The input dataset\n    \"\"\"\n    self.median = X.median()\n    return self\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.MedianCentering.partial_fit","title":"<code>partial_fit(X, y=None)</code>","text":"<p>Compute incrementally the median of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def partial_fit(self, X: pd.DataFrame, y=None):\n    \"\"\"\n    Compute incrementally the median of the dataset\n\n    Parameters:\n        X: The input life\n    \"\"\"\n    if X.shape[0] &lt; 2:\n        return self\n\n    if self.tdigest_dict is None:\n        self.tdigest_dict = {c: TDigest(100) for c in X.columns}\n    for c in X.columns:\n        self.tdigest_dict[c] = self.tdigest_dict[c].merge_unsorted(X[c].values)\n\n    self.median = pd.Series(\n        {\n            c: self.tdigest_dict[c].estimate_quantile(0.5)\n            for c in self.tdigest_dict.keys()\n        }\n    )\n    return self\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.MedianCentering.transform","title":"<code>transform(X)</code>","text":"<p>Center the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Center the input life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new DataFrame with the same index as the input with the data centered with respect to the mean of the fiited dataset\n    \"\"\"\n    return X - self.median\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Peaks","title":"<code>Peaks</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Find Peaks in the input life. Return a new DataFrame with the same index as the input with 1 in the position of the peaks and 0 otherwise</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class Peaks(TransformerStep):\n    \"\"\"Find Peaks in the input life. Return a new DataFrame with the same index as the input with 1 in the position of the peaks and 0 otherwise\"\"\"\n\n    distance: float\n\n    def __init__(self, *, distance:float, name : Optional[str] = None):\n        super().__init__(name=name)\n        self.distance = distance\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Detect the peaks in the input life\n\n        Parameters:\n            X: The Input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the peaks marked as 1 and 0 otherwise. \n        \"\"\"\n        new_X = pd.DataFrame(np.zeros(X.shape), index=X.index, columns=X.columns)\n        for i, c in enumerate(X.columns):\n            peaks_positions, _ = find_peaks(X[c].values, distance=self.distance)\n            new_X.iloc[peaks_positions, i] = 1\n\n        return new_X\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Peaks.transform","title":"<code>transform(X)</code>","text":"<p>Detect the peaks in the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the peaks marked as 1 and 0 otherwise.</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Detect the peaks in the input life\n\n    Parameters:\n        X: The Input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the peaks marked as 1 and 0 otherwise. \n    \"\"\"\n    new_X = pd.DataFrame(np.zeros(X.shape), index=X.index, columns=X.columns)\n    for i, c in enumerate(X.columns):\n        peaks_positions, _ = find_peaks(X[c].values, distance=self.distance)\n        new_X.iloc[peaks_positions, i] = 1\n\n    return new_X\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.RollingCentering","title":"<code>RollingCentering</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Center the life using an rolling window</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class RollingCentering(TransformerStep):\n    \"\"\"\n    Center the life using an rolling window\n\n    \"\"\"\n\n\n\n    def __init__(self, window: int, min_points: int, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.window = window\n        self.min_points = min_points\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the live centering it using a rolling window\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the data centered\n        \"\"\"\n        return X - X.rolling(window=self.window, min_periods=self.min_points).mean()\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.RollingCentering.transform","title":"<code>transform(X)</code>","text":"<p>Transform the live centering it using a rolling window</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the data centered</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the live centering it using a rolling window\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the data centered\n    \"\"\"\n    return X - X.rolling(window=self.window, min_periods=self.min_points).mean()\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Scale","title":"<code>Scale</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Scale each feature by a given vaulue</p> <p>Parameters:</p> Name Type Description Default <code>scale_factor</code> <code>float</code> <p>Scale factor to apply to each feature</p> required <code>name</code> <code>Optional[str]</code> <p>Name of the step, by default None</p> <code>None</code> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class Scale(TransformerStep):\n    \"\"\"\n    Scale each feature by a given vaulue\n\n    Parameters:\n        scale_factor: Scale factor to apply to each feature\n        name: Name of the step, by default None\n    \"\"\"\n\n    def __init__(self, *, scale_factor: float, name: Optional[str] = None):\n        super().__init__(name=name)\n        self.scale_factor = scale_factor\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Return the scaled life\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the scaled features\n        \"\"\"\n        return X * self.scale_factor\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Scale.transform","title":"<code>transform(X)</code>","text":"<p>Return the scaled life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the scaled features</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Return the scaled life\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the scaled features\n    \"\"\"\n    return X * self.scale_factor\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Sqrt","title":"<code>Sqrt</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the sqrt of the values of each feature</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class Sqrt(TransformerStep):\n    \"\"\"Compute the sqrt of the values of each feature\"\"\"\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life with the sqrt of the values\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new dataframe with the same index as the input with the sqrt of the values\n        \"\"\"\n        return X.pow(1.0 / 2)\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Sqrt.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life with the sqrt of the values</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new dataframe with the same index as the input with the sqrt of the values</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life with the sqrt of the values\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new dataframe with the same index as the input with the sqrt of the values\n    \"\"\"\n    return X.pow(1.0 / 2)\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Square","title":"<code>Square</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Compute the square of the values of each feature</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class Square(TransformerStep):\n    \"\"\"Compute the square of the values of each feature\"\"\"\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life with the square of the values\n\n        Parameters:\n            X: The input life\n\n        Returns:\n            A new dataframe with the same index as the input with the square of the values\n        \"\"\"\n        return X.pow(2)\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.Square.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life with the square of the values</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new dataframe with the same index as the input with the square of the values</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life with the square of the values\n\n    Parameters:\n        X: The input life\n\n    Returns:\n        A new dataframe with the same index as the input with the square of the values\n    \"\"\"\n    return X.pow(2)\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.StringConcatenate","title":"<code>StringConcatenate</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Return a new DataFrame with a single column with the concatenation of the values of each row. The method works only on strings and the values are separated by -</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class StringConcatenate(TransformerStep):\n    \"\"\"\n    Return a new DataFrame with a single column with the concatenation of the values of each row. The method works only on strings and the values are separated by -\"\"\"\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform the input life computing the 1 step difference\n\n        Parameters:\n            X: The Input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the difference of the features\n        \"\"\"\n        new_X = pd.DataFrame(index=X.index)\n        new_X[\"concatenation\"] = X.agg(\"-\".join, axis=1)\n        return new_X\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.StringConcatenate.transform","title":"<code>transform(X)</code>","text":"<p>Transform the input life computing the 1 step difference</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the difference of the features</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transform the input life computing the 1 step difference\n\n    Parameters:\n        X: The Input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the difference of the features\n    \"\"\"\n    new_X = pd.DataFrame(index=X.index)\n    new_X[\"concatenation\"] = X.agg(\"-\".join, axis=1)\n    return new_X\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.SubstractLinebase","title":"<code>SubstractLinebase</code>","text":"<p>               Bases: <code>TransformerStep</code></p> <p>Subtract the values in the first row from all the rows in the input life</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>class SubstractLinebase(TransformerStep):\n    \"\"\"Subtract the values in the first row from all the rows in the input life\"\"\"\n\n    def __init__(self, *args):\n        super().__init__(*args)\n\n    def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\" \n        Apply the transformation to the input life\n\n        Parameters:\n            X: The Input life\n\n        Returns:\n            Return a new DataFrame with the same index as the input with the subtraction of the first row\n        \"\"\"\n        return X - X.iloc[0, :]\n</code></pre>"},{"location":"transformation/features/transformation/#ceruleo.transformation.features.transformation.SubstractLinebase.transform","title":"<code>transform(X)</code>","text":"<p>Apply the transformation to the input life</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The Input life</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Return a new DataFrame with the same index as the input with the subtraction of the first row</p> Source code in <code>ceruleo/transformation/features/transformation.py</code> <pre><code>def transform(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\" \n    Apply the transformation to the input life\n\n    Parameters:\n        X: The Input life\n\n    Returns:\n        Return a new DataFrame with the same index as the input with the subtraction of the first row\n    \"\"\"\n    return X - X.iloc[0, :]\n</code></pre>"}]}